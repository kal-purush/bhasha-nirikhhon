{
    "identifiers": [
        "functools",
        "partial",
        "torch",
        "torch",
        "nn",
        "nn",
        "timm",
        "models",
        "layers",
        "trunc_normal_",
        "vision_transformer",
        "VisionTransformer",
        "Attention",
        "Decoder",
        "PadIm2Video",
        "layer",
        "init_weight",
        "init_bias",
        "init_weight",
        "init_weight",
        "tensor",
        "layer",
        "weight",
        "data",
        "init_bias",
        "init_bias",
        "tensor",
        "layer",
        "bias",
        "data",
        "layer",
        "tensor",
        "torch",
        "nn",
        "init",
        "xavier_uniform_",
        "tensor",
        "view",
        "tensor",
        "shape",
        "nn",
        "Module",
        "trunk",
        "head",
        "trunk",
        "head",
        "imgOrVideo",
        "mask",
        "trunk",
        "imgOrVideo",
        "mask",
        "mask",
        "mask",
        "head",
        "outputs",
        "head",
        "outputs",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "partial",
        "Decoder",
        "attn_target",
        "partial",
        "Attention",
        "num_heads",
        "decoder_depth",
        "decoder_embed_dim",
        "embed_dim",
        "learnable_pos_embed",
        "qkv_bias",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "partial",
        "Decoder",
        "attn_target",
        "partial",
        "Attention",
        "num_heads",
        "decoder_depth",
        "decoder_embed_dim",
        "embed_dim",
        "learnable_pos_embed",
        "qkv_bias",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "partial",
        "Decoder",
        "attn_target",
        "partial",
        "Attention",
        "num_heads",
        "decoder_depth",
        "decoder_embed_dim",
        "embed_dim",
        "learnable_pos_embed",
        "qkv_bias",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_bias",
        "partial",
        "torch",
        "nn",
        "init",
        "zeros_",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model",
        "ckpt_path",
        "VisionTransformer",
        "img_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "depth",
        "mlp_ratio",
        "attn_target",
        "partial",
        "Attention",
        "attn_drop",
        "num_heads",
        "proj_drop",
        "qk_scale",
        "qkv_bias",
        "drop_rate",
        "drop_path_rate",
        "drop_path_type",
        "force_cast_ln_fp32",
        "classifier_feature",
        "use_cls_token",
        "learnable_pos_embed",
        "non_skip_wt",
        "non_skip_wt_learnable",
        "layer_scale_type",
        "layer_scale_init_value",
        "patch_embed_type",
        "patch_embed_params_list",
        "PadIm2Video",
        "ntimes",
        "pad_type",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Conv3d",
        "in_channels",
        "kernel_size",
        "out_channels",
        "stride",
        "init_weight",
        "partial",
        "reshape_and_init_as_mlp",
        "layer_norm_eps",
        "masked_image_modeling",
        "patch_drop_min_patches",
        "patch_drop_max_patches",
        "patch_drop_at_eval",
        "add_pos_same_dtype",
        "patch_dropping",
        "post_encoder_params",
        "decoder",
        "mask_token_embed_dim",
        "make_conv_or_linear",
        "layer",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "init_weight",
        "partial",
        "trunc_normal_",
        "mean",
        "std",
        "OmniMAE",
        "trunk",
        "head",
        "ckpt_path",
        "model",
        "load_state_dict",
        "torch",
        "load",
        "ckpt_path",
        "model"
    ],
    "literals": [
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\"",
        "\"progressive\"",
        "\"global_pool\"",
        "\"generic\"",
        "\"repeat\""
    ],
    "variables": [
        "trunk",
        "head",
        "outputs",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model",
        "trunk",
        "head",
        "model"
    ],
    "comments": [
        "!/usr/bin/env python3",
        "Portions Copyright (c) Meta Platforms, Inc. and affiliates.",
        "All rights reserved.",
        "This source code is licensed under the license found in the",
        "LICENSE file in the root directory of this source tree.",
        "Based on MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py",
        "imgOrVideo: A tensor of shape [N,C,H,W] for images and [N,C,T,H,W] for videos",
        "mask: A boolean tensor of the shape [N, patch_layout's shpae]",
        "NOTE: Head config for this model has funcky dropout in head",
        "ckpt loeading is different.",
        "NOTE: Head config for this model has funcky dropout in head",
        "ckpt loeading is different."
    ],
    "docstrings": [],
    "functions": [
        "make_conv_or_linear",
        "reshape_and_init_as_mlp",
        "forward",
        "vit_base_mae_pretraining",
        "vit_base_mae_finetune_ssv2",
        "vit_base_mae_finetune_in1k",
        "vit_large_mae_pretraining",
        "vit_large_mae_finetune_ssv2",
        "vit_large_mae_finetune_in1k",
        "vit_huge_mae_pretraining",
        "vit_huge_mae_finetune_ssv2",
        "vit_huge_mae_finetune_in1k"
    ],
    "classes": [
        "OmniMAE"
    ]
}