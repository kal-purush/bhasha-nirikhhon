{
    "identifiers": [
        "numpy",
        "np",
        "random",
        "gym",
        "torch",
        "rl_games",
        "common",
        "segment_tree",
        "SumSegmentTree",
        "MinSegmentTree",
        "torch",
        "rl_games",
        "algos_torch",
        "torch_ext",
        "numpy_to_torch_dtype_dict",
        "size",
        "ob_space",
        "np",
        "zeros",
        "size",
        "ob_space",
        "shape",
        "dtype",
        "ob_space",
        "dtype",
        "np",
        "zeros",
        "size",
        "ob_space",
        "shape",
        "dtype",
        "ob_space",
        "dtype",
        "np",
        "zeros",
        "size",
        "np",
        "zeros",
        "size",
        "dtype",
        "np",
        "int32",
        "np",
        "zeros",
        "size",
        "dtype",
        "np",
        "size",
        "_curr_size",
        "obs_t",
        "action",
        "reward",
        "obs_tp1",
        "done",
        "min",
        "_curr_size",
        "_maxsize",
        "_obses",
        "_next_idx",
        "obs_t",
        "_next_obses",
        "_next_idx",
        "obs_tp1",
        "_rewards",
        "_next_idx",
        "reward",
        "_actions",
        "_next_idx",
        "action",
        "_dones",
        "_next_idx",
        "done",
        "_next_idx",
        "_maxsize",
        "idx",
        "_obses",
        "idx",
        "_actions",
        "idx",
        "_rewards",
        "idx",
        "_next_obses",
        "idx",
        "_dones",
        "idx",
        "idxes",
        "len",
        "idxes",
        "batch_size",
        "batch_size",
        "batch_size",
        "batch_size",
        "batch_size",
        "i",
        "idxes",
        "_get",
        "i",
        "data",
        "np",
        "array",
        "obs_t",
        "copy",
        "np",
        "array",
        "action",
        "copy",
        "reward",
        "np",
        "array",
        "obs_tp1",
        "copy",
        "done",
        "it",
        "np",
        "array",
        "obses_t",
        "np",
        "array",
        "actions",
        "np",
        "array",
        "rewards",
        "np",
        "array",
        "obses_tp1",
        "np",
        "array",
        "dones",
        "batch_size",
        "random",
        "randint",
        "_curr_size",
        "_",
        "batch_size",
        "_encode_sample",
        "idxes",
        "ReplayBuffer",
        "size",
        "alpha",
        "ob_space",
        "PrioritizedReplayBuffer",
        "size",
        "ob_space",
        "alpha",
        "alpha",
        "it_capacity",
        "size",
        "it_capacity",
        "SumSegmentTree",
        "it_capacity",
        "MinSegmentTree",
        "it_capacity",
        "args",
        "kwargs",
        "_next_idx",
        "add",
        "args",
        "kwargs",
        "_it_sum",
        "_max_priority",
        "_alpha",
        "_it_min",
        "_max_priority",
        "_alpha",
        "batch_size",
        "_it_sum",
        "sum",
        "_curr_size",
        "p_total",
        "batch_size",
        "i",
        "batch_size",
        "random",
        "random",
        "every_range_len",
        "i",
        "every_range_len",
        "_it_sum",
        "find_prefixsum_idx",
        "mass",
        "res",
        "append",
        "idx",
        "res",
        "batch_size",
        "beta",
        "beta",
        "_sample_proportional",
        "batch_size",
        "_it_min",
        "min",
        "_it_sum",
        "sum",
        "p_min",
        "_curr_size",
        "beta",
        "idx",
        "idxes",
        "_it_sum",
        "idx",
        "_it_sum",
        "sum",
        "p_sample",
        "_curr_size",
        "beta",
        "weights",
        "append",
        "weight",
        "max_weight",
        "np",
        "array",
        "weights",
        "_encode_sample",
        "idxes",
        "encoded_sample",
        "weights",
        "idxes",
        "idxes",
        "priorities",
        "len",
        "idxes",
        "len",
        "priorities",
        "idx",
        "priority",
        "idxes",
        "priorities",
        "priority",
        "idx",
        "_curr_size",
        "_it_sum",
        "priority",
        "_alpha",
        "_it_min",
        "priority",
        "_alpha",
        "max",
        "_max_priority",
        "priority",
        "obs_shape",
        "action_shape",
        "capacity",
        "device",
        "device",
        "torch",
        "empty",
        "capacity",
        "obs_shape",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "torch",
        "empty",
        "capacity",
        "obs_shape",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "torch",
        "empty",
        "capacity",
        "action_shape",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "torch",
        "empty",
        "capacity",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "torch",
        "empty",
        "capacity",
        "dtype",
        "torch",
        "device",
        "device",
        "capacity",
        "property",
        "capacity",
        "full",
        "idx",
        "obs",
        "action",
        "reward",
        "next_obs",
        "terminated",
        "done",
        "obs",
        "shape",
        "min",
        "capacity",
        "idx",
        "num_observations",
        "num_observations",
        "remaining_capacity",
        "remaining_capacity",
        "num_observations",
        "obses",
        "overflow",
        "obs",
        "overflow",
        "actions",
        "overflow",
        "action",
        "overflow",
        "rewards",
        "overflow",
        "reward",
        "overflow",
        "next_obses",
        "overflow",
        "next_obs",
        "overflow",
        "dones",
        "overflow",
        "terminated",
        "overflow",
        "obses",
        "idx",
        "idx",
        "remaining_capacity",
        "obs",
        "remaining_capacity",
        "actions",
        "idx",
        "idx",
        "remaining_capacity",
        "action",
        "remaining_capacity",
        "rewards",
        "idx",
        "idx",
        "remaining_capacity",
        "reward",
        "remaining_capacity",
        "next_obses",
        "idx",
        "idx",
        "remaining_capacity",
        "next_obs",
        "remaining_capacity",
        "dones",
        "idx",
        "idx",
        "remaining_capacity",
        "terminated",
        "remaining_capacity",
        "idx",
        "num_observations",
        "capacity",
        "full",
        "idx",
        "batch_size",
        "torch",
        "randint",
        "capacity",
        "full",
        "idx",
        "batch_size",
        "device",
        "device",
        "obses",
        "idxs",
        "actions",
        "idxs",
        "rewards",
        "idxs",
        "next_obses",
        "idxs",
        "dones",
        "idxs",
        "obses",
        "actions",
        "rewards",
        "next_obses",
        "dones",
        "env_info",
        "algo_info",
        "device",
        "aux_tensor_dict",
        "env_info",
        "algo_info",
        "device",
        "env_info",
        "get",
        "env_info",
        "algo_info",
        "algo_info",
        "algo_info",
        "algo_info",
        "get",
        "num_actors",
        "num_agents",
        "horizon_length",
        "num_agents",
        "num_actors",
        "horizon_length",
        "num_actors",
        "action_space",
        "gym",
        "spaces",
        "Discrete",
        "action_space",
        "n",
        "action_space",
        "gym",
        "spaces",
        "Tuple",
        "len",
        "action_space",
        "action",
        "n",
        "action",
        "action_space",
        "action_space",
        "gym",
        "spaces",
        "Box",
        "action_space",
        "shape",
        "action_space",
        "shape",
        "_init_from_env_info",
        "env_info",
        "aux_tensor_dict",
        "aux_tensor_dict",
        "_init_from_aux_dict",
        "aux_tensor_dict",
        "env_info",
        "obs_base_shape",
        "state_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "env_info",
        "obs_base_shape",
        "has_central_value",
        "tensor_dict",
        "_create_tensor_from_space",
        "env_info",
        "state_base_shape",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "env_info",
        "get",
        "tensor_dict",
        "_create_tensor_from_space",
        "val_space",
        "obs_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "val_space",
        "obs_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "dtype",
        "np",
        "float32",
        "obs_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "dtype",
        "np",
        "uint8",
        "obs_base_shape",
        "is_discrete",
        "is_multi_discrete",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "actions_shape",
        "dtype",
        "obs_base_shape",
        "use_action_masks",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "actions_shape",
        "np",
        "sum",
        "actions_num",
        "dtype",
        "np",
        "obs_base_shape",
        "is_continuous",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "actions_shape",
        "dtype",
        "np",
        "float32",
        "obs_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "actions_shape",
        "dtype",
        "np",
        "float32",
        "obs_base_shape",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "actions_shape",
        "dtype",
        "np",
        "float32",
        "obs_base_shape",
        "tensor_dict",
        "obs_base_shape",
        "k",
        "v",
        "tensor_dict",
        "items",
        "tensor_dict",
        "_create_tensor_from_space",
        "gym",
        "spaces",
        "Box",
        "low",
        "high",
        "shape",
        "v",
        "dtype",
        "np",
        "float32",
        "obs_base_shape",
        "space",
        "base_shape",
        "space",
        "gym",
        "spaces",
        "Box",
        "numpy_to_torch_dtype_dict",
        "space",
        "dtype",
        "torch",
        "zeros",
        "base_shape",
        "space",
        "shape",
        "dtype",
        "dtype",
        "device",
        "device",
        "space",
        "gym",
        "spaces",
        "Discrete",
        "numpy_to_torch_dtype_dict",
        "space",
        "dtype",
        "torch",
        "zeros",
        "base_shape",
        "dtype",
        "dtype",
        "device",
        "device",
        "space",
        "gym",
        "spaces",
        "Tuple",
        "numpy_to_torch_dtype_dict",
        "space",
        "dtype",
        "len",
        "space",
        "torch",
        "zeros",
        "base_shape",
        "tuple_len",
        "dtype",
        "dtype",
        "device",
        "device",
        "space",
        "gym",
        "spaces",
        "Dict",
        "k",
        "v",
        "space",
        "spaces",
        "items",
        "_create_tensor_from_space",
        "v",
        "base_shape",
        "t_dict",
        "name",
        "index",
        "val",
        "val",
        "k",
        "v",
        "val",
        "items",
        "tensor_dict",
        "name",
        "k",
        "v",
        "tensor_dict",
        "name",
        "val",
        "name",
        "indices",
        "play_mask",
        "val",
        "val",
        "k",
        "v",
        "val",
        "tensor_dict",
        "name",
        "k",
        "v",
        "tensor_dict",
        "name",
        "val",
        "transform_op",
        "k",
        "v",
        "tensor_dict",
        "items",
        "v",
        "kd",
        "vd",
        "v",
        "items",
        "transform_op",
        "vd",
        "transformed_dict",
        "transform_op",
        "v",
        "res_dict",
        "transform_op",
        "tensor_list",
        "k",
        "tensor_list",
        "tensor_dict",
        "get",
        "k",
        "v",
        "v",
        "kd",
        "vd",
        "v",
        "items",
        "transform_op",
        "vd",
        "transformed_dict",
        "transform_op",
        "v",
        "res_dict"
    ],
    "literals": [
        "'agents'",
        "'action_space'",
        "'num_actors'",
        "'horizon_length'",
        "'has_central_value'",
        "'use_action_masks'",
        "'obses'",
        "'observation_space'",
        "'states'",
        "'state_space'",
        "'value_size'",
        "'rewards'",
        "'values'",
        "'neglogpacs'",
        "'dones'",
        "'actions'",
        "'action_masks'",
        "'actions'",
        "'mus'",
        "'sigmas'"
    ],
    "variables": [
        "_obses",
        "_next_obses",
        "_rewards",
        "_actions",
        "_dones",
        "_maxsize",
        "_next_idx",
        "_curr_size",
        "_curr_size",
        "_next_idx",
        "batch_size",
        "obses_t",
        "actions",
        "rewards",
        "obses_tp1",
        "dones",
        "it",
        "data",
        "obs_t",
        "action",
        "reward",
        "obs_tp1",
        "done",
        "obses_t",
        "it",
        "actions",
        "it",
        "rewards",
        "it",
        "obses_tp1",
        "it",
        "dones",
        "it",
        "it",
        "idxes",
        "_alpha",
        "it_capacity",
        "_it_sum",
        "_it_min",
        "_max_priority",
        "idx",
        "idx",
        "idx",
        "res",
        "p_total",
        "every_range_len",
        "mass",
        "idx",
        "idxes",
        "weights",
        "p_min",
        "max_weight",
        "p_sample",
        "weight",
        "weights",
        "encoded_sample",
        "idx",
        "idx",
        "_max_priority",
        "device",
        "obses",
        "next_obses",
        "actions",
        "rewards",
        "dones",
        "capacity",
        "idx",
        "full",
        "num_observations",
        "remaining_capacity",
        "overflow",
        "full",
        "idx",
        "full",
        "idxs",
        "obses",
        "actions",
        "rewards",
        "next_obses",
        "dones",
        "env_info",
        "algo_info",
        "device",
        "num_agents",
        "action_space",
        "num_actors",
        "horizon_length",
        "has_central_value",
        "use_action_masks",
        "batch_size",
        "is_discrete",
        "is_multi_discrete",
        "is_continuous",
        "obs_base_shape",
        "state_base_shape",
        "actions_shape",
        "actions_num",
        "is_discrete",
        "actions_shape",
        "actions_num",
        "is_multi_discrete",
        "actions_shape",
        "actions_num",
        "is_continuous",
        "tensor_dict",
        "aux_tensor_dict",
        "obs_base_shape",
        "state_base_shape",
        "val_space",
        "obs_base_shape",
        "k",
        "dtype",
        "dtype",
        "dtype",
        "tuple_len",
        "t_dict",
        "t_dict",
        "k",
        "index",
        "index",
        "indices",
        "play_mask",
        "indices",
        "play_mask",
        "res_dict",
        "transformed_dict",
        "transformed_dict",
        "kd",
        "res_dict",
        "k",
        "res_dict",
        "k",
        "res_dict",
        "v",
        "transformed_dict",
        "transformed_dict",
        "kd",
        "res_dict",
        "k",
        "res_dict",
        "k"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\"Create Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        \"\"\"",
        "\"\"\"Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"",
        "\"\"\"Create Prioritized Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n        See Also\n        --------\n        ReplayBuffer.__init__\n        \"\"\"",
        "\"\"\"See ReplayBuffer.store_effect\"\"\"",
        "\"\"\"Sample a batch of experiences.\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        weights: np.array\n            Array of shape (batch_size,) and dtype np.float32\n            denoting importance weight of each sampled transition\n        idxes: np.array\n            Array of shape (batch_size,) and dtype np.int32\n            idexes in buffer of sampled experiences\n        \"\"\"",
        "\"\"\"Update priorities of sampled transitions.\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"",
        "\"\"\"Create Vectorized Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        See Also\n        --------\n        ReplayBuffer.__init__\n        \"\"\"",
        "\"\"\"Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obses: torch tensor\n            batch of observations\n        actions: torch tensor\n            batch of actions executed given obs\n        rewards: torch tensor\n            rewards received as results of executing act_batch\n        next_obses: torch tensor\n            next set of observations seen after executing act_batch\n        not_dones: torch tensor\n            inverse of whether the episode ended at this tuple of (observation, action) or not\n        not_dones_no_max: torch tensor\n            inverse of whether the episode ended at this tuple of (observation, action) or not, specifically exlcuding maximum episode steps\n        \"\"\"",
        "'''\n    More generalized than replay buffers.\n    Implemented for on-policy algos\n    '''",
        "'''\n            assuming that tuple is only Discrete tuple\n            '''"
    ],
    "functions": [
        "__len__",
        "add",
        "_get",
        "_encode_sample",
        "sample",
        "add",
        "_sample_proportional",
        "sample",
        "update_priorities",
        "size",
        "add",
        "sample",
        "_init_from_env_info",
        "_init_from_aux_dict",
        "_create_tensor_from_space",
        "update_data",
        "update_data_rnn",
        "get_transformed",
        "get_transformed_list"
    ],
    "classes": [
        "ReplayBuffer",
        "PrioritizedReplayBuffer",
        "VectorizedReplayBuffer",
        "ExperienceBuffer"
    ]
}