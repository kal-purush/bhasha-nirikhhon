{
    "identifiers": [
        "math",
        "os",
        "json",
        "datetime",
        "argparse",
        "warnings",
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "torch",
        "utils",
        "data",
        "Dataset",
        "DataLoader",
        "numpy",
        "np",
        "datetime",
        "datetime",
        "now",
        "strftime",
        "transformers",
        "GPT2Config",
        "GPT2Tokenizer",
        "TrainingArguments",
        "Trainer",
        "set_seed",
        "TrainerCallback",
        "transformers",
        "models",
        "gpt2",
        "modeling_gpt2",
        "GPT2Block",
        "transformers",
        "modeling_outputs",
        "CausalLMOutput",
        "datasets",
        "types",
        "SimpleNamespace",
        "typing",
        "Optional",
        "matplotlib",
        "pyplot",
        "plt",
        "Dataset",
        "split",
        "tokenizer",
        "seq_len",
        "tokenizer",
        "seq_len",
        "datasets",
        "load_dataset",
        "split",
        "split",
        "join",
        "data",
        "tokenizer",
        "tokenizer",
        "encode",
        "text",
        "add_special_tokens",
        "i",
        "len",
        "token_ids",
        "seq_len",
        "seq_len",
        "samples",
        "append",
        "token_ids",
        "i",
        "i",
        "seq_len",
        "len",
        "samples",
        "idx",
        "torch",
        "tensor",
        "samples",
        "idx",
        "dtype",
        "torch",
        "sample",
        "sample",
        "batch",
        "torch",
        "stack",
        "item",
        "item",
        "batch",
        "torch",
        "stack",
        "item",
        "item",
        "batch",
        "input_ids",
        "labels",
        "TrainerCallback",
        "args",
        "state",
        "control",
        "logs",
        "kwargs",
        "state",
        "global_step",
        "logs",
        "getattr",
        "state",
        "logs",
        "logs",
        "isinstance",
        "current_loss",
        "torch",
        "Tensor",
        "current_loss",
        "item",
        "current_loss",
        "best_training_loss",
        "current_loss",
        "math",
        "exp",
        "current_loss",
        "current_loss",
        "logs",
        "logs",
        "isinstance",
        "current_eval_loss",
        "torch",
        "Tensor",
        "current_eval_loss",
        "item",
        "current_eval_loss",
        "current_eval_loss",
        "best_eval_loss",
        "current_eval_loss",
        "math",
        "exp",
        "current_eval_loss",
        "current_eval_loss",
        "last_eval_loss",
        "math",
        "exp",
        "current_eval_loss",
        "current_eval_loss",
        "current_eval_loss",
        "state",
        "global_step",
        "epoch",
        "out_str",
        "epoch",
        "current_loss",
        "out_str",
        "current_loss",
        "best_training_loss",
        "training_perplexity",
        "current_eval_loss",
        "out_str",
        "current_eval_loss",
        "best_eval_loss",
        "eval_perplexity",
        "out_str",
        "Trainer",
        "eval_dataset",
        "ignore_keys",
        "metric_key_prefix",
        "evaluate",
        "eval_dataset",
        "ignore_keys",
        "metric_key_prefix",
        "hasattr",
        "state",
        "global_step",
        "hasattr",
        "state",
        "metrics",
        "get",
        "np",
        "exp",
        "eval_loss",
        "eval_loss",
        "eval_loss",
        "eval_steps",
        "append",
        "current_step",
        "eval_ppls",
        "append",
        "eval_ppl",
        "plt",
        "figure",
        "plt",
        "plot",
        "eval_steps",
        "eval_ppls",
        "marker",
        "plt",
        "xlabel",
        "plt",
        "ylabel",
        "plt",
        "title",
        "plt",
        "ylim",
        "min",
        "max",
        "eval_ppls",
        "os",
        "makedirs",
        "exist_ok",
        "plt",
        "savefig",
        "timestamp",
        "plt",
        "close",
        "metrics",
        "nn",
        "Module",
        "config",
        "chunk_size",
        "nn",
        "Embedding",
        "config",
        "vocab_size",
        "config",
        "n_embd",
        "nn",
        "Embedding",
        "chunk_size",
        "config",
        "n_embd",
        "chunk_size",
        "input_ids",
        "wte",
        "input_ids",
        "input_ids",
        "size",
        "torch",
        "arange",
        "seq_len",
        "device",
        "input_ids",
        "device",
        "unsqueeze",
        "wpe",
        "positions",
        "token_emb",
        "pos_emb",
        "nn",
        "Module",
        "config",
        "num_layers",
        "nn",
        "ModuleList",
        "GPT2Block",
        "config",
        "_",
        "num_layers",
        "nn",
        "LayerNorm",
        "config",
        "n_embd",
        "eps",
        "config",
        "layer_norm_epsilon",
        "x",
        "block",
        "blocks",
        "block",
        "x",
        "attention_mask",
        "use_cache",
        "output_attentions",
        "ln_f",
        "x",
        "x",
        "nn",
        "Module",
        "config",
        "num_layers",
        "nn",
        "ModuleList",
        "GPT2Block",
        "config",
        "_",
        "num_layers",
        "nn",
        "LayerNorm",
        "config",
        "n_embd",
        "eps",
        "config",
        "layer_norm_epsilon",
        "x",
        "causal_mask",
        "past_key_values",
        "i",
        "block",
        "blocks",
        "past_key_values",
        "past_key_values",
        "i",
        "block",
        "x",
        "attention_mask",
        "causal_mask",
        "use_cache",
        "output_attentions",
        "layer_past",
        "past",
        "new_past",
        "append",
        "present",
        "ln_f",
        "x",
        "x",
        "new_past",
        "nn",
        "Module",
        "combine_fn",
        "identity",
        "inclusive",
        "combine_fn",
        "identity",
        "inclusive",
        "X_in",
        "X_in",
        "shape",
        "L",
        "bit_length",
        "identity",
        "torch",
        "zeros",
        "B",
        "D",
        "N",
        "device",
        "X_in",
        "device",
        "dtype",
        "X_in",
        "dtype",
        "identity",
        "to",
        "X_in",
        "device",
        "expand",
        "B",
        "D",
        "N",
        "X_in",
        "transpose",
        "contiguous",
        "P",
        "L",
        "id_val",
        "unsqueeze",
        "expand",
        "B",
        "D",
        "P",
        "L",
        "N",
        "torch",
        "cat",
        "X",
        "pad",
        "dim",
        "X",
        "clone",
        "math",
        "log2",
        "P",
        "lvl",
        "levels",
        "lvl",
        "torch",
        "arange",
        "step",
        "P",
        "step",
        "device",
        "X",
        "device",
        "idx_l",
        "step",
        "idx_l",
        "numel",
        "X",
        "idx_l",
        "X",
        "idx_r",
        "left",
        "permute",
        "reshape",
        "B",
        "K",
        "D",
        "N",
        "right",
        "permute",
        "reshape",
        "B",
        "K",
        "D",
        "N",
        "combine_fn",
        "left_blocks",
        "right_blocks",
        "merged_blocks",
        "view",
        "B",
        "K",
        "D",
        "N",
        "permute",
        "X",
        "clone",
        "merged",
        "X_next",
        "X",
        "clone",
        "id_val",
        "X_next",
        "lvl",
        "reversed",
        "levels",
        "lvl",
        "torch",
        "arange",
        "step",
        "P",
        "step",
        "device",
        "X",
        "device",
        "idx_l",
        "step",
        "idx_l",
        "numel",
        "X",
        "idx_l",
        "clone",
        "X",
        "idx_r",
        "old_l",
        "permute",
        "reshape",
        "B",
        "K",
        "D",
        "N",
        "old_r",
        "permute",
        "reshape",
        "B",
        "K",
        "D",
        "N",
        "old_r_blocks",
        "combine_fn",
        "old_l_blocks",
        "old_r_blocks",
        "new_r_blocks",
        "view",
        "B",
        "K",
        "D",
        "N",
        "permute",
        "new_l_blocks",
        "view",
        "B",
        "K",
        "D",
        "N",
        "permute",
        "X",
        "clone",
        "new_l",
        "new_r",
        "X_next",
        "inclusive",
        "combine_fn",
        "X",
        "X_orig",
        "X_incl",
        "L",
        "transpose",
        "X",
        "L",
        "transpose",
        "nn",
        "Module",
        "config",
        "chunk_size",
        "T1_num_layers",
        "T2_num_layers",
        "train_mode",
        "config",
        "chunk_size",
        "config",
        "vocab_size",
        "T0",
        "config",
        "chunk_size",
        "T1",
        "config",
        "num_layers",
        "T1_num_layers",
        "T2",
        "config",
        "num_layers",
        "T2_num_layers",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "vocab_size",
        "bias",
        "train_mode",
        "train_mode",
        "seq_length",
        "device",
        "torch",
        "tril",
        "torch",
        "ones",
        "seq_length",
        "seq_length",
        "device",
        "device",
        "unsqueeze",
        "unsqueeze",
        "mask",
        "mask",
        "input_ids",
        "labels",
        "input_ids",
        "shape",
        "seq_length",
        "chunk_size",
        "input_ids",
        "view",
        "batch_size",
        "num_chunks",
        "chunk_size",
        "T0",
        "chunks",
        "i",
        "i",
        "num_chunks",
        "torch",
        "zeros_like",
        "level0",
        "train_mode",
        "vectorized_prefix_scan",
        "level0",
        "dummy",
        "debug",
        "train_mode",
        "compute_sequential_prefix",
        "input_ids",
        "debug",
        "train_mode",
        "combine_fn",
        "BlellochScan",
        "combine_fn",
        "inclusive",
        "scan",
        "torch",
        "stack",
        "level0",
        "dim",
        "ValueError",
        "nn",
        "CrossEntropyLoss",
        "level0",
        "chunk_size",
        "get_causal_mask",
        "T2_input_0",
        "size",
        "T2_input_0",
        "device",
        "T2",
        "T2_input_0",
        "causal_mask",
        "causal_mask_0",
        "T2_head",
        "T2_out_0",
        "all_logits",
        "append",
        "logits_chunk0",
        "chunks",
        "loss_fn",
        "logits_chunk0",
        "reshape",
        "vocab_size",
        "target_chunk0",
        "reshape",
        "total_loss",
        "loss_chunk0",
        "num_chunks",
        "i",
        "num_chunks",
        "P",
        "i",
        "level0",
        "i",
        "chunk_size",
        "torch",
        "cat",
        "prefix",
        "current_emb",
        "dim",
        "t2_inputs",
        "append",
        "t2_input",
        "targets",
        "append",
        "chunks",
        "i",
        "torch",
        "stack",
        "t2_inputs",
        "dim",
        "T2_inputs",
        "view",
        "T2_inputs",
        "size",
        "T2_inputs",
        "size",
        "T2_inputs",
        "size",
        "get_causal_mask",
        "seq_len_t2",
        "T2_inputs",
        "device",
        "T2",
        "T2_inputs",
        "causal_mask",
        "causal_mask",
        "T2_out",
        "chunk_size",
        "T2_head",
        "T2_out",
        "logits_chunks",
        "view",
        "batch_size",
        "num_chunks",
        "chunk_size",
        "vocab_size",
        "all_logits",
        "append",
        "logits_chunks",
        "torch",
        "stack",
        "targets",
        "dim",
        "loss_fn",
        "logits_chunks",
        "reshape",
        "vocab_size",
        "targets",
        "reshape",
        "total_loss",
        "loss_chunks",
        "num_chunks",
        "total_loss",
        "num_chunks",
        "total_loss",
        "CausalLMOutput",
        "loss",
        "total_loss",
        "logits",
        "all_logits",
        "x",
        "y",
        "x",
        "y",
        "y",
        "x",
        "torch",
        "cat",
        "x",
        "y",
        "dim",
        "T1",
        "cat",
        "isinstance",
        "chunk_size",
        "x",
        "y",
        "torch",
        "cat",
        "x",
        "y",
        "dim",
        "T1",
        "cat",
        "isinstance",
        "chunk_size",
        "states",
        "dummy",
        "debug",
        "len",
        "states",
        "math",
        "ceil",
        "math",
        "log2",
        "n",
        "n",
        "states",
        "size",
        "states",
        "device",
        "torch",
        "stack",
        "states",
        "dim",
        "torch",
        "zeros",
        "batch",
        "M",
        "dtype",
        "torch",
        "device",
        "device",
        "detach",
        "M",
        "n",
        "dummy",
        "unsqueeze",
        "expand",
        "batch",
        "M",
        "n",
        "chunk_size",
        "torch",
        "cat",
        "state_tensor",
        "pad_tensor",
        "dim",
        "n",
        "state_tensor",
        "math",
        "log2",
        "M",
        "d",
        "L_levels",
        "d",
        "M",
        "step",
        "T",
        "view",
        "batch",
        "num_groups",
        "step",
        "chunk_size",
        "d",
        "step",
        "T",
        "left_idx",
        "T",
        "right_idx",
        "torch",
        "cat",
        "left",
        "right",
        "dim",
        "view",
        "batch",
        "num_groups",
        "chunk_size",
        "T1",
        "temp",
        "isinstance",
        "view",
        "batch",
        "num_groups",
        "chunk_size",
        "chunk_size",
        "T",
        "view",
        "batch",
        "M",
        "chunk_size",
        "torch",
        "zeros_like",
        "T",
        "torch",
        "zeros",
        "batch",
        "M",
        "dtype",
        "torch",
        "device",
        "device",
        "detach",
        "M",
        "dummy",
        "M",
        "d",
        "reversed",
        "L_levels",
        "d",
        "M",
        "step",
        "T",
        "view",
        "batch",
        "num_groups",
        "step",
        "chunk_size",
        "D",
        "view",
        "batch",
        "num_groups",
        "step",
        "chunk_size",
        "upsweep_mask",
        "view",
        "batch",
        "num_groups",
        "step",
        "detach",
        "clone",
        "downsweep_mask",
        "view",
        "batch",
        "num_groups",
        "step",
        "detach",
        "clone",
        "d",
        "step",
        "D_view",
        "right_idx",
        "downsweep_mask_view",
        "right_idx",
        "T_view",
        "left_idx",
        "upsweep_mask_view",
        "left_idx",
        "parent_val",
        "downsweep_mask_view",
        "clone",
        "parent_dummy",
        "torch",
        "cat",
        "parent_val",
        "left_old_val",
        "dim",
        "view",
        "batch",
        "num_groups",
        "chunk_size",
        "T1",
        "concat",
        "isinstance",
        "combined",
        "combined",
        "combined",
        "view",
        "batch",
        "num_groups",
        "chunk_size",
        "chunk_size",
        "torch",
        "where",
        "parent_dummy",
        "unsqueeze",
        "unsqueeze",
        "detach",
        "left_old_val",
        "combined",
        "torch",
        "where",
        "left_dummy",
        "unsqueeze",
        "unsqueeze",
        "detach",
        "parent_val",
        "result",
        "result",
        "downsweep_mask_view",
        "clone",
        "parent_dummy",
        "left_dummy",
        "D",
        "n",
        "input_ids",
        "debug",
        "input_ids",
        "shape",
        "seq_length",
        "chunk_size",
        "input_ids",
        "view",
        "batch_size",
        "n",
        "chunk_size",
        "T0",
        "chunks",
        "i",
        "i",
        "n",
        "torch",
        "zeros_like",
        "level0",
        "n",
        "bit_length",
        "i",
        "n",
        "level0",
        "i",
        "s",
        "i",
        "j",
        "combine",
        "L",
        "j",
        "current",
        "j",
        "current",
        "i",
        "s",
        "k",
        "reversed",
        "len",
        "L",
        "L",
        "k",
        "prefix",
        "L",
        "k",
        "combine",
        "prefix",
        "L",
        "k",
        "prefix_list",
        "append",
        "prefix",
        "debug",
        "i",
        "prefix",
        "torch",
        "cat",
        "dummy",
        "unsqueeze",
        "p",
        "unsqueeze",
        "p",
        "prefix_list",
        "dim",
        "P_seq",
        "P_seq",
        "L",
        "input_ids",
        "torch",
        "Tensor",
        "L",
        "Optional",
        "chunks_processed",
        "prefix_val",
        "Optional",
        "torch",
        "Tensor",
        "past_key_values",
        "input_ids",
        "shape",
        "chunk_size",
        "total_len",
        "chunk",
        "total_len",
        "chunk",
        "L",
        "num_full",
        "compute_sequential_prefix",
        "input_ids",
        "num_full",
        "chunk",
        "num_full",
        "P_seq",
        "torch",
        "zeros",
        "batch_size",
        "chunk",
        "config",
        "n_embd",
        "device",
        "input_ids",
        "device",
        "num_full",
        "chunks_processed",
        "num_full",
        "num_full",
        "chunks_processed",
        "chunks_processed",
        "chunk",
        "T0",
        "input_ids",
        "start",
        "start",
        "chunk",
        "chunks_processed",
        "j",
        "combine",
        "L",
        "j",
        "state",
        "j",
        "j",
        "len",
        "L",
        "L",
        "extend",
        "j",
        "len",
        "L",
        "state",
        "chunks_processed",
        "entry",
        "reversed",
        "L",
        "entry",
        "entry",
        "prefix",
        "combine",
        "prefix",
        "entry",
        "prefix",
        "num_full",
        "chunks_processed",
        "T2",
        "prefix_val",
        "causal_mask",
        "past_key_values",
        "prefix_val",
        "T2",
        "last_token",
        "causal_mask",
        "past_key_values",
        "past_key_values",
        "T2_head",
        "t2_out",
        "next_logits",
        "L",
        "chunks_processed",
        "prefix_val",
        "past_key_values",
        "classmethod",
        "cls",
        "checkpoint_path",
        "config",
        "chunk_size",
        "device",
        "kwargs",
        "cls",
        "config",
        "chunk_size",
        "kwargs",
        "os",
        "path",
        "isdir",
        "checkpoint_path",
        "os",
        "path",
        "join",
        "checkpoint_path",
        "os",
        "path",
        "exists",
        "potential_file",
        "potential_file",
        "os",
        "path",
        "join",
        "checkpoint_path",
        "os",
        "path",
        "exists",
        "potential_file",
        "potential_file",
        "FileNotFoundError",
        "checkpoint_path",
        "safetensors",
        "torch",
        "load_file",
        "load_file",
        "checkpoint_file",
        "state_dict",
        "model",
        "load_state_dict",
        "state_dict",
        "model",
        "load_state_dict",
        "state_dict",
        "model",
        "to",
        "device",
        "model",
        "argparse",
        "ArgumentParser",
        "description",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "choices",
        "help",
        "parser",
        "parse_args",
        "args",
        "seq_len",
        "args",
        "chunk_size",
        "ValueError",
        "set_seed",
        "args",
        "seed",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "device",
        "timestamp",
        "os",
        "makedirs",
        "output_dir",
        "exist_ok",
        "GPT2Tokenizer",
        "from_pretrained",
        "WikiTextDataset",
        "split",
        "tokenizer",
        "tokenizer",
        "seq_len",
        "args",
        "seq_len",
        "WikiTextDataset",
        "split",
        "tokenizer",
        "tokenizer",
        "seq_len",
        "args",
        "seq_len",
        "GPT2Config",
        "vocab_size",
        "tokenizer",
        "vocab_size",
        "n_positions",
        "n_embd",
        "n_layer",
        "n_head",
        "dropout",
        "TransformerScanModel",
        "config",
        "chunk_size",
        "args",
        "chunk_size",
        "T1_num_layers",
        "T2_num_layers",
        "train_mode",
        "args",
        "train_mode",
        "model",
        "to",
        "device",
        "TrainingArguments",
        "output_dir",
        "output_dir",
        "evaluation_strategy",
        "eval_steps",
        "save_steps",
        "logging_steps",
        "per_device_train_batch_size",
        "args",
        "batch_size",
        "per_device_eval_batch_size",
        "args",
        "batch_size",
        "num_train_epochs",
        "args",
        "epochs",
        "learning_rate",
        "args",
        "learning_rate",
        "warmup_steps",
        "weight_decay",
        "fp16",
        "seed",
        "args",
        "seed",
        "lr_scheduler_type",
        "report_to",
        "max_grad_norm",
        "logging_dir",
        "save_total_limit",
        "CustomTrainer",
        "model",
        "model",
        "args",
        "training_args",
        "train_dataset",
        "train_dataset",
        "eval_dataset",
        "valid_dataset",
        "data_collator",
        "collate_fn",
        "tokenizer",
        "tokenizer",
        "callbacks",
        "PrintLossCallback",
        "transformers",
        "ProgressCallback",
        "trainer",
        "remove_callback",
        "ProgressCallback",
        "trainer",
        "train",
        "main"
    ],
    "literals": [
        "\"%Y%m%d_%H%M%S\"",
        "\"wikitext\"",
        "\"wikitext-2-raw-v1\"",
        "\" \"",
        "\"text\"",
        "\"input_ids\"",
        "\"labels\"",
        "\"input_ids\"",
        "\"labels\"",
        "\"input_ids\"",
        "\"labels\"",
        "'inf'",
        "'inf'",
        "\"epoch\"",
        "\"loss\"",
        "\"loss\"",
        "'inf'",
        "\"eval_loss\"",
        "\"eval_loss\"",
        "'inf'",
        "'inf'",
        "f\"Step {state.global_step}: \"",
        "f\"Epoch {epoch} | \"",
        "f\"Training Loss: {current_loss:.4f} (Best: {self.best_training_loss:.4f}, Perp: {training_perplexity:.4f})\"",
        "f\" | Eval Loss: {current_eval_loss:.4f} (Best: {self.best_eval_loss:.4f}, Perp: {eval_perplexity:.4f})\"",
        "\"eval\"",
        "'eval_steps'",
        "'global_step'",
        "\"eval_loss\"",
        "'inf'",
        "'o'",
        "\"Global Step\"",
        "\"Evaluation Perplexity\"",
        "\"Evaluation Perplexity Over Time\"",
        "\"plots\"",
        "f\"plots/eval_ppl_{timestamp}.png\"",
        "\"parallel\"",
        "'=> Training with mode:'",
        "\"parallel\"",
        "\"sequential\"",
        "\"blelloch\"",
        "\"Invalid training mode. Choose 'parallel', 'sequential', or 'blelloch'.\"",
        "f\"Sequential: computed prefix for chunk {i}, flag: {prefix[1]}\"",
        "'update chunk: '",
        "\"Only one new chunk per token\"",
        "\"cpu\"",
        "\"model.safetensors\"",
        "\"pytorch_model.bin\"",
        "\"No valid model weights file found in the checkpoint directory.\"",
        "\"model_state_dict\"",
        "\"model_state_dict\"",
        "'Train a GPT2-based Transformer Scan LM with binary tree aggregation on WikiText-2.'",
        "'--seq_len'",
        "'Number of tokens per sample (must be a multiple of chunk_size).'",
        "'--batch_size'",
        "'Batch size.'",
        "'--epochs'",
        "'Number of training epochs.'",
        "'--learning_rate'",
        "'Learning rate.'",
        "'--seed'",
        "'Random seed.'",
        "'--chunk_size'",
        "'Chunk size (to be safe, use powers of 2).'",
        "'--train_mode'",
        "'parallel'",
        "'parallel'",
        "'blelloch'",
        "'sequential'",
        "'Training mode: parallel or sequential.'",
        "\"seq_len must be a multiple of chunk_size.\"",
        "\"cuda\"",
        "\"cpu\"",
        "\"Device:\"",
        "f\"../out/tree_model/tree_{timestamp}\"",
        "\"gpt2\"",
        "\"train\"",
        "\"validation\"",
        "\"steps\"",
        "\"cosine\"",
        "\"tensorboard\"",
        "\"./logs\"",
        "\"__main__\""
    ],
    "variables": [
        "timestamp",
        "TOL",
        "tokenizer",
        "seq_len",
        "data",
        "text",
        "model_max_length",
        "token_ids",
        "samples",
        "sample",
        "input_ids",
        "labels",
        "best_training_loss",
        "best_eval_loss",
        "last_eval_loss",
        "epoch",
        "current_loss",
        "current_loss",
        "best_training_loss",
        "training_perplexity",
        "current_loss",
        "training_perplexity",
        "current_eval_loss",
        "current_eval_loss",
        "last_eval_loss",
        "best_eval_loss",
        "eval_perplexity",
        "current_eval_loss",
        "eval_perplexity",
        "out_str",
        "metrics",
        "eval_steps",
        "eval_ppls",
        "current_step",
        "eval_loss",
        "eval_ppl",
        "wte",
        "wpe",
        "chunk_size",
        "token_emb",
        "seq_len",
        "positions",
        "pos_emb",
        "blocks",
        "ln_f",
        "x",
        "x",
        "blocks",
        "ln_f",
        "new_past",
        "past",
        "x",
        "present",
        "x",
        "combine_fn",
        "identity",
        "inclusive",
        "B",
        "L",
        "D",
        "N",
        "P",
        "id_val",
        "id_val",
        "X",
        "pad",
        "X",
        "X_orig",
        "levels",
        "step",
        "idx_l",
        "idx_r",
        "K",
        "left",
        "right",
        "left_blocks",
        "right_blocks",
        "merged_blocks",
        "merged",
        "X_next",
        "X_next",
        "idx_r",
        "X",
        "X_next",
        "X_next",
        "X",
        "step",
        "idx_l",
        "idx_r",
        "K",
        "old_l",
        "old_r",
        "old_l_blocks",
        "old_r_blocks",
        "new_l_blocks",
        "new_r_blocks",
        "new_r",
        "new_l",
        "X_next",
        "X_next",
        "idx_l",
        "X_next",
        "idx_r",
        "X",
        "X_incl",
        "config",
        "chunk_size",
        "vocab_size",
        "T0",
        "T1",
        "T2",
        "T2_head",
        "train_mode",
        "mask",
        "mask",
        "batch_size",
        "seq_length",
        "num_chunks",
        "chunks",
        "level0",
        "dummy",
        "P",
        "P",
        "L",
        "combine_fn",
        "scan",
        "P",
        "loss_fn",
        "total_loss",
        "all_logits",
        "T2_input_0",
        "causal_mask_0",
        "T2_out_0",
        "_",
        "logits_chunk0",
        "target_chunk0",
        "loss_chunk0",
        "t2_inputs",
        "targets",
        "prefix",
        "current_emb",
        "t2_input",
        "T2_inputs",
        "T2_inputs",
        "seq_len_t2",
        "causal_mask",
        "T2_out",
        "_",
        "T2_out",
        "logits_chunks",
        "logits_chunks",
        "targets",
        "loss_chunks",
        "total_loss",
        "total_loss",
        "cat",
        "cat",
        "n",
        "M",
        "batch",
        "device",
        "state_tensor",
        "upsweep_mask",
        "pad_tensor",
        "state_tensor",
        "upsweep_mask",
        "T",
        "L_levels",
        "step",
        "num_groups",
        "T",
        "left_idx",
        "right_idx",
        "left",
        "right",
        "temp",
        "T",
        "right_idx",
        "T",
        "D",
        "downsweep_mask",
        "D",
        "downsweep_mask",
        "step",
        "num_groups",
        "T_view",
        "D_view",
        "upsweep_mask_view",
        "downsweep_mask_view",
        "left_idx",
        "right_idx",
        "parent_val",
        "parent_dummy",
        "left_old_val",
        "left_dummy",
        "D_view",
        "left_idx",
        "downsweep_mask_view",
        "downsweep_mask_view",
        "left_idx",
        "concat",
        "combined",
        "combined",
        "combined",
        "result",
        "result",
        "D_view",
        "right_idx",
        "downsweep_mask_view",
        "downsweep_mask_view",
        "right_idx",
        "batch_size",
        "seq_length",
        "n",
        "chunks",
        "level0",
        "dummy",
        "prefix_list",
        "L",
        "s",
        "current",
        "j",
        "current",
        "L",
        "j",
        "L",
        "j",
        "prefix",
        "prefix",
        "prefix",
        "prefix",
        "P_seq",
        "P_seq",
        "batch_size",
        "total_len",
        "chunk",
        "num_full",
        "remainder_len",
        "P_seq",
        "L",
        "chunks_processed",
        "prefix_val",
        "L",
        "chunks_processed",
        "prefix_val",
        "start",
        "state",
        "j",
        "state",
        "L",
        "j",
        "L",
        "j",
        "prefix",
        "prefix",
        "prefix_val",
        "t2_out",
        "past_key_values",
        "last_token",
        "t2_out",
        "past_key_values",
        "next_logits",
        "model",
        "potential_file",
        "checkpoint_file",
        "potential_file",
        "checkpoint_file",
        "checkpoint_file",
        "state_dict",
        "parser",
        "args",
        "device",
        "output_dir",
        "tokenizer",
        "tokenizer",
        "model_max_length",
        "train_dataset",
        "valid_dataset",
        "config",
        "model",
        "training_args",
        "trainer"
    ],
    "comments": [
        "Tolerance to determine if a tensor is the dummy",
        "-----------------------------------------------------------------------------",
        "Dataset and Data Collation (same as before)",
        "-----------------------------------------------------------------------------",
        "self.data = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split)",
        "-----------------------------------------------------------------------------",
        "Trainer Callbacks (unchanged)",
        "-----------------------------------------------------------------------------",
        "Retrieve epoch from the state, if available",
        "print('evaluate')",
        "-----------------------------------------------------------------------------",
        "Model Definition: Transformer Scan with Binary Tree Aggregation using GPT-2 Blocks",
        "-----------------------------------------------------------------------------",
        "(batch, seq_len, hidden_dim)",
        "X_in: (B, L, D, N)",
        "padded length P = next power of two ≥ L",
        "prepare identity (e.g. zero for additive scan)",
        "requires_grad_(False)",
        "reformat to (B, D, L, N)",
        "pad to next power of two",
        "→ (B, D, P, N)",
        "--- UPSWEEP (reduction) ---",
        "number of pairs at this level",
        "→ (B, D, K, N) where K = len(idx_l)",
        "→ (B, D, K, N) where K = len(idx_r)",
        "un-fold back to (B, D, K, N)",
        "in-place on Xn but Xn is non-leaf, so PyTorch records gradients",
        "--- DOWNSWEEP (distribution) ---",
        "number of pairs at this level",
        "(B*K, D, N)",
        "new_l = old_r                                   # ← set left to old right",
        "new_r = self.combine_fn(old_l, old_r)           # ← combine old left, old right of shapes (B, D, N)",
        "X is now the *exclusive* scan in (B, D, L, N).",
        "To get *inclusive*, do combine(prefix_excl, original):",
        "→ (B, L, D, N)",
        "→ (B, L, D, N)",
        "Split input into chunks: shape (batch, num_chunks, chunk_size)",
        "Compute T0 embeddings for each chunk.",
        "Each element in level0: (batch, chunk_size, hidden_dim)",
        "Define dummy state.",
        "Compute prefix states using the vectorized scan.",
        "P: (batch, num_chunks+1, chunk_size, hidden_dim)",
        "--- Process chunk 0: standard autoregressive prediction ---",
        "(batch, chunk_size-1, hidden_dim)",
        "(batch, chunk_size-1, hidden_dim)",
        "(batch, chunk_size-1, vocab_size)",
        "(batch, chunk_size-1)",
        "--- Process chunks 1 to num_chunks-1 in parallel ---",
        "to store T2 inputs for each chunk (i from 1 to num_chunks-1)",
        "to store corresponding target tokens",
        "For chunk i, the T2 input is:",
        "Concatenate the prefix state P[:, i, :, :] (aggregation of chunks 0...i-1)",
        "with the T0 embedding of chunk i (excluding its last token).",
        "(batch, chunk_size, hidden_dim)",
        "(batch, chunk_size-1, hidden_dim)",
        "(batch, 2*chunk_size-1, hidden_dim)",
        "(batch, chunk_size-1)",
        "Stack T2 inputs along a new dimension: shape becomes (batch, num_chunks-1, 2*chunk_size-1, hidden_dim)",
        "Reshape to merge the batch and chunk dimensions: (batch*(num_chunks-1), 2*chunk_size-1, hidden_dim)",
        "this equals 2*chunk_size-1",
        "(batch*(num_chunks-1), 2*chunk_size-1, hidden_dim)",
        "For each T2 input, we only want to predict the tokens corresponding to the current chunk.",
        "That is, we take only the last (chunk_size-1) tokens.",
        "(batch*(num_chunks-1), chunk_size-1, hidden_dim)",
        "(batch*(num_chunks-1), chunk_size-1, vocab_size)",
        "Reshape back to (batch, num_chunks-1, chunk_size-1, vocab_size)",
        "Similarly, stack targets for chunks 1 to num_chunks-1: shape (batch, num_chunks-1, chunk_size-1)",
        "print(\"Chunk0 logits shape:\", logits_chunk0.shape, \"target shape:\", target_chunk0.shape, \"loss_chunk0:\", loss_chunk0.item())",
        "print(\"Parallel T2 inputs shape:\", T2_inputs.shape)",
        "print(\"T2_out shape:\", T2_out.shape)",
        "print(\"Logits_chunks shape:\", logits_chunks.shape, \"Targets shape:\", targets.shape, \"loss_chunks:\", loss_chunks.item())",
        "If left is dummy, return right.",
        "If right is dummy, return left.",
        "Both are real: combine via T1.",
        "shape: (batch, 2*chunk_size, hidden_dim)",
        "Shape of x and y is (B*K, D, N) where D is chunk size and N is dimension",
        "Stack real states and build upsweep mask for padded positions",
        "Upsweep (unchanged)",
        "Downsweep: allocate new tree D and dummy-mask",
        "Initialize root to dummy",
        "Parallel downsweep levels",
        "Propagate parent down to left child",
        "Compute combined only for real-real pairs",
        "Merge according to dummy flags",
        "If parent is dummy -> result = left_old_val",
        "If left is dummy -> result = parent_val",
        "Else -> result = combined",
        "Return leaves: exclusive prefixes (including dummy at index 0)",
        "Reshape into chunks.",
        "Apply initial embedding T0; wrap each state as (value, False)",
        "Create dummy as (tensor, True)",
        "L will be our binary counter; its length is O(log n)",
        "(A[i], False)",
        "Carry update: while the j-th bit is 1, combine.",
        "Now, fold the non-None entries of L in descending order (MSB-to-LSB).",
        "Concatenate dummy and all prefix values (ignoring flags for output)",
        "INITIAL PROMPT: fold all full chunks at once",
        "last prefix of the prompt",
        "INCREMENTAL STEP: update only on chunk boundary",
        "Recompute prefix_val by folding non‑None L in MSB→LSB",
        "Build T2 input using cached prefix_val",
        "If we just crossed into a new full chunk, send the entire prefix chunk into T2 (cold start)",
        "i.e. we just incremented chunks_processed above",
        "prefix_val shape = (batch, chunk, hidden_dim)",
        "Otherwise, only feed the newest single token into T2 using the cached KV",
        "Take the last timestep of prefix_val as the new input token",
        "shape = (batch, 1, hidden_dim)",
        "Instantiate the model on CPU first.",
        "If checkpoint_path is a directory, locate the weight file.",
        "Try \"model.safetensors\" first.",
        "Fallback to \"pytorch_model.bin\".",
        "Import and use the safetensors loader without a device parameter.",
        "Loads on CPU by default.",
        "If the checkpoint is a dict with \"model_state_dict\", extract it.",
        "Finally, move the model to the desired device.",
        "-----------------------------------------------------------------------------",
        "Main Training Code (unchanged)",
        "-----------------------------------------------------------------------------"
    ],
    "docstrings": [
        "\"\"\"\n    T0: Initial embedding module.\n    \"\"\"",
        "\"\"\"\n    T1: Aggregation module.\n    Uses GPT-2 blocks (without a causal mask) to aggregate two sequences.\n    \"\"\"",
        "\"\"\"\n    T2: Autoregressive prediction module.\n    Uses GPT-2 blocks with a causal mask.\n    \"\"\"",
        "\"\"\"\n    A parallel Blelloch scan calls `combine_fn(left, right)` on every pair.\n    Works on inputs X_in of shape (B, L, D, N), with L a power of two.\n    \"\"\"",
        "\"\"\"\n    TransformerScanModel using binary tree forward/backward aggregation.\n    \n    This implementation assumes 8 chunks (for demonstration). The forward pass:\n      - Computes T0 embeddings for each chunk.\n      - Computes a binary tree using T1 in two passes:\n          * Forward pass: compute only the necessary internal nodes.\n          * Backward pass: compute missing prefix states.\n      - Constructs final prefix states:\n            [dummy, s[0:0], s[0:1], s[0:2], s[0:3], s[0:4], s[0:5], s[0:6]]\n      - Runs T2 for autoregressive next-token prediction.\n    \"\"\"",
        "\"\"\"\n        input_ids: (batch, seq_length), where seq_length is a multiple of chunk_size.\n        Computes prefix states via a vectorized Blelloch scan and uses them for autoregressive prediction.\n        \"\"\"",
        "\"\"\"\n        Combine two states x and y, where each is a tuple (value, is_dummy).\n        If one operand is dummy, return the other. Otherwise, combine the values.\n        \"\"\"",
        "\"\"\"\n        Computes prefix states sequentially using the binary-counter method.\n        For an input with n chunks, returns a tensor of shape \n            (batch, n+1, chunk_size, hidden_dim)\n        corresponding to:\n            [ dummy, A0, T1(A0, A1), T1(T1(A0, A1), A2), T1(T1(A0, A1), T1(A2, A3)), ... ]\n        Dummy states have an explicit boolean flag True.\n        \"\"\""
    ],
    "functions": [
        "__len__",
        "__getitem__",
        "collate_fn",
        "on_log",
        "evaluate",
        "forward",
        "forward",
        "forward",
        "forward",
        "get_causal_mask",
        "forward",
        "combine",
        "combine_fn",
        "vectorized_prefix_scan",
        "compute_sequential_prefix",
        "forward_inference",
        "from_pretrained",
        "main"
    ],
    "classes": [
        "WikiTextDataset",
        "PrintLossCallback",
        "CustomTrainer",
        "T0",
        "T1",
        "T2",
        "BlellochScan",
        "TransformerScanModel"
    ]
}