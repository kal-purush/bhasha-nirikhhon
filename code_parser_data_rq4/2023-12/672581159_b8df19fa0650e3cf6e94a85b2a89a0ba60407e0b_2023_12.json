{
    "identifiers": [
        "typing",
        "Optional",
        "matplotlib",
        "pyplot",
        "plt",
        "numpy",
        "np",
        "pandas",
        "pd",
        "seaborn",
        "sns",
        "numpy",
        "mean",
        "quantile",
        "tabulate",
        "tabulate",
        "tqdm",
        "auto",
        "trange",
        "y0",
        "algorithm",
        "conditional_independencies",
        "get_conditional_independencies",
        "y0",
        "algorithm",
        "falsification",
        "get_graph_falsifications",
        "y0",
        "graph",
        "NxMixedGraph",
        "y0",
        "CITest",
        "_ensure_method",
        "get_conditional_independence_tests",
        "get_conditional_independence_tests",
        "graph",
        "NxMixedGraph",
        "data",
        "pd",
        "DataFrame",
        "method",
        "Optional",
        "CITest",
        "significance_level",
        "Optional",
        "NxMixedGraph",
        "NxMixedGraph",
        "directed",
        "graph",
        "directed",
        "copy",
        "undirected",
        "graph",
        "undirected",
        "copy",
        "significance_level",
        "DEFAULT_SIGNIFICANCE",
        "judgement",
        "get_conditional_independencies",
        "rv",
        "judgement",
        "test",
        "data",
        "method",
        "method",
        "significance_level",
        "significance_level",
        "rv",
        "add_undirected_edge",
        "judgement",
        "left",
        "judgement",
        "right",
        "rv",
        "graph",
        "NxMixedGraph",
        "data",
        "pd",
        "DataFrame",
        "method",
        "Optional",
        "CITest",
        "max_given",
        "Optional",
        "significance_level",
        "Optional",
        "verbose",
        "Optional",
        "tablefmt",
        "significance_level",
        "DEFAULT_SIGNIFICANCE",
        "get_graph_falsifications",
        "graph",
        "graph",
        "df",
        "data",
        "method",
        "method",
        "significance_level",
        "significance_level",
        "max_given",
        "max_given",
        "evidence",
        "len",
        "evidence_df",
        "evidence_df",
        "sum",
        "n_failed",
        "n_total",
        "n_failed",
        "n_total",
        "significance_level",
        "verbose",
        "evidence_df",
        "evidence_df",
        "evidence_df",
        "tabulate",
        "dd",
        "headers",
        "dd",
        "columns",
        "tablefmt",
        "tablefmt",
        "showindex",
        "df",
        "pd",
        "DataFrame",
        "sample_size",
        "left",
        "right",
        "conditions",
        "test",
        "Optional",
        "CITest",
        "significance_level",
        "Optional",
        "significance_level",
        "_ensure_method",
        "method",
        "test",
        "df",
        "df",
        "df",
        "sample",
        "n",
        "sample_size",
        "replace",
        "TESTS",
        "test",
        "X",
        "left",
        "Y",
        "right",
        "Z",
        "conditions",
        "data",
        "bootstrap_data",
        "significance_level",
        "significance_level",
        "result",
        "p_val",
        "df",
        "pd",
        "DataFrame",
        "sample_size",
        "left",
        "right",
        "conditions",
        "test",
        "Optional",
        "CITest",
        "significance_level",
        "Optional",
        "boot_size",
        "Optional",
        "boot_size",
        "_ensure_method",
        "method",
        "test",
        "df",
        "df",
        "p_value_of_bootstrap_data",
        "df",
        "sample_size",
        "left",
        "right",
        "conditions",
        "test",
        "test",
        "significance_level",
        "significance_level",
        "_",
        "trange",
        "boot_size",
        "desc",
        "leave",
        "unit_scale",
        "unit",
        "mean",
        "samples",
        "quantile",
        "samples",
        "q",
        "np",
        "absolute",
        "p_val",
        "quantile_05",
        "np",
        "absolute",
        "quantile_95",
        "p_val",
        "p_val",
        "lower_error",
        "higher_error",
        "df",
        "pd",
        "DataFrame",
        "start",
        "stop",
        "step",
        "left",
        "right",
        "conditions",
        "test",
        "Optional",
        "CITest",
        "significance_level",
        "Optional",
        "boot_size",
        "Optional",
        "significance_level",
        "p_value_statistics",
        "df",
        "df",
        "sample_size",
        "size",
        "left",
        "left",
        "right",
        "right",
        "conditions",
        "conditions",
        "test",
        "test",
        "significance_level",
        "significance_level",
        "boot_size",
        "boot_size",
        "size",
        "trange",
        "start",
        "stop",
        "step",
        "desc",
        "len",
        "conditions",
        "plt",
        "title",
        "left",
        "right",
        "join",
        "conditions",
        "plt",
        "title",
        "left",
        "right",
        "conditions_string",
        "plt",
        "xlabel",
        "plt",
        "ylabel",
        "sns",
        "lineplot",
        "x",
        "start",
        "stop",
        "step",
        "y",
        "p_vals",
        "plt",
        "errorbar",
        "start",
        "stop",
        "step",
        "p_vals",
        "yerr",
        "np",
        "array",
        "lower_errors",
        "higher_errors",
        "ecolor",
        "elinewidth",
        "fmt",
        "plt",
        "hlines",
        "significance_level",
        "stop",
        "linestyles"
    ],
    "literals": [
        "\"add_ci_undirected_edges\"",
        "\"print_graph_falsifications\"",
        "\"p_value_of_bootstrap_data\"",
        "\"p_value_statistics\"",
        "\"plot_ci_size_dependence\"",
        "\"rst\"",
        "\"p_adj_significant\"",
        "f\"Failed tests: {n_failed}/{n_total} ({n_failed / n_total:.2%})\"",
        "f\"Reject null hypothesis when p<{significance_level}\"",
        "\"p_adj_significant\"",
        "\"Bootstrapping\"",
        "\"bootstrap\"",
        "\"Sampling\"",
        "f\"Independence of {left} and {right}\"",
        "\", \"",
        "f\"Independence of {left} and {right} given {conditions_string}\"",
        "\"Data Points\"",
        "\"Expected p-Value\"",
        "\"grey\"",
        "\"none\"",
        "\"dashed\""
    ],
    "variables": [
        "__all__",
        "TESTS",
        "DEFAULT_SIGNIFICANCE",
        "rv",
        "significance_level",
        "significance_level",
        "evidence_df",
        "n_total",
        "n_failed",
        "dd",
        "dd",
        "significance_level",
        "test",
        "bootstrap_data",
        "result",
        "p_val",
        "boot_size",
        "test",
        "samples",
        "p_val",
        "quantile_05",
        "quantile_95",
        "lower_error",
        "higher_error",
        "significance_level",
        "p_vals",
        "lower_errors",
        "higher_errors",
        "conditions_string"
    ],
    "comments": [
        "noqa:T201",
        "noqa:T201",
        "noqa:T201",
        "Calculate the mean of the p-values to get the bootstrap mean.",
        "Calculate the 5th percentile",
        "Calculate the 95th percentile"
    ],
    "docstrings": [
        "\"\"\"This module checks the validity of network structure against observational data.\n\nGiven an acyclic directed mixed graph (ADMG) and corresponding observational data,\none can assess whether the conditional independences implied by the structure of the\nADMG are supported by the data with a statistical test for conditional independence.\nBy default, this workflow uses a chi-square test for discrete data and a Pearson test\nfor continuous data from :mod:`pgmpy.estimators.CITests`.\n\nThis module provides a summary statistics for the total number of tests, percentage of failed\ntests, and a list of all (or the failed tests) with their corresponding p-value.\n\nThis process allows for checking if the network structure is consistent with the data by checking\nthe percentage of failed tests. If the percentage of failed tests is lower than 30 percent, the effect\nthat the inconsistency between the structure and the data may have on causal query estimation is minor.\nHowever, if the percentage of failed tests is larger than 30 percent, we recommend the user to revise\nthe network structure or the corresponding data.\n\n.. warning:: This functionality is not unit tested! Use at your own risk.\n\nExample\n-------\nWe'll work with :data:`eliater.examples.example_2` and take $X$ as the treatment, $Y$ as the outcome.\nThe example includes a function for simulating observational data.\n\n.. image:: img/multiple_mediators_with_multiple_confounders.png\n  :width: 200px\n\n.. code-block:: python\n\n    from eliater.examples import example_2\n    from eliater.network_validation import print_graph_falsifications\n\n    graph = example_2.graph\n    observational_df = example_2.generate_data(500, seed=1)\n\n    print_graph_falsifications(graph, data=observational_df, verbose=True)\n\n======  =======  =======  ===========  ===========  =====  ===========  ===================\nleft    right    given          stats            p  dof          p_adj  p_adj_significant\n======  =======  =======  ===========  ===========  =====  ===========  ===================\nY       Z2       M2|Z3     0.392472    7.33647e-20         1.02711e-18  True\nM2      Z3       X         0.0887728   0.047259            0.614367     False\nM2      Z2       X         0.0874659   0.0506246           0.614367     False\nX       Z3       Z1        0.0097293   0.828197            1            False\nZ1      Z3       Z2        0.0700012   0.117988            1            False\nX       Y        M2|Z3    -0.00485697  0.913731            1            False\nX       Z2       Z1       -0.0109544   0.806966            1            False\nM1      Y        M2|Z3     0.0124796   0.780733            1            False\nM2      Z1       X         0.0697175   0.119489            1            False\nY       Z1       M2|Z3     0.0169804   0.704858            1            False\nM2      X        M1        0.0435465   0.331173            1            False\nM1      Z3       X         0.0664571   0.137823            1            False\nM1      Z1       X        -0.0108138   0.809395            1            False\nM1      Z2       X         0.0372645   0.405713            1            False\n======  =======  =======  ===========  ===========  =====  ===========  ===================\n\nThe results show that out of 14 cases, 1 failed. The failed test is\nthe conditional independence between $Y$ and $Z_2$, given $M_2$ and $Z_3$.\n\nFinding False Negatives\n-----------------------\nThis module relies on statistical tests, and statistical tests have chances\nof producing false negatives, i.e., a pair of variables that are conditionally\nindependent, be concluded as conditional dependent by the test, or producing false\npositives, i.e., a pair of variables that are conditionally dependent be concluded\nas conditionally independent by the test. The main reason that the result of the test\nmay be false negative or false positive is that statistical tests rely on *p*-values.\nThe p-values are subject to known limitations in statistical analysis [halsey2015fickle]_\nand [wang2022addressing]_. In particular, the p-value of a data-driven conditional\nindependency test decrease as the number of data points increases [lucas2013too]_.\n\nThis is described below using the following example graph, and observational data\n(simulated specifically for this graph using\n:func:`eliater.frontdoor_backdoor.example2.generate`),\nand the application of subsampling.\n\n.. image:: img/multiple_mediators_with_multiple_confounders.png\n  :width: 200px\n\n.. warning::\n\n   This part is implemented based on Chapter 4 from\n   https://livebook.manning.com/book/causal-ai/welcome/v-4/, however\n   this resource is paywalled.\n\nIn this graph, $Y$ is conditionally independent (i.e., D-separated) of $M_1$ given $M_2$, $Z_2$.\nThe data has been generated based on this assumption. Hence, we expect the $p$-value to be above\n0.05, i.e., not rejecting the null hypothesis of conditional independence. We use the following\nworkflow to graphically assess how this compares to a data-driven approach.\n\n.. code-block:: python\n\n   import matplotlib.pyplot as plt\n   from matplotlib_inline.backend_inline import set_matplotlib_formats\n\n   from y0.graph import NxMixedGraph\n   from eliater.examples import example_2\n   from eliater import plot_ci_size_dependence\n\n   graph = example_2.graph\n\n   stop = 2_000\n   # Generate observational data for this graph (this is a special example)\n   observational_df = example_2.generate_data(stop, seed=1)\n\n   plot_ci_size_dependence(\n       observational_df,\n       start=50,\n       stop=stop,\n       step=100,\n       left=\"Y\",\n       right=\"M1\",\n       conditions=[\"M2\", \"Z2\"],\n   )\n   plt.savefig(\"pvalue_vs_sample_size.svg\")\n\n\n.. image:: img/pvalue_vs_sample_size.svg\n  :width: 350px\n  :height: 250px\n  :scale: 200 %\n  :alt: alternate text\n  :align: right\n\nThis plot shows that the expected $p$-value will decrease as number of data points increases. The error bars are 90%\nbootstrap confidence intervals. The horizontal dashed line is a 0.5 significance level. The p-values above this\nthreshold show that the test favors the null hypothesis of conditional independence. For number of data points\ngreater than 1,000, the test is more likely to reject the null hypothesis, and for number of data points greater\nthan 1,250, the test always rejects the null hypothesis, i.e., the data will no longer support that $Y$ is\nindependent of $M_1$ given $M_2$, and $Z_2$ where it should be. Hence, the result of network validation depends\non the size of the data. This result may seem disappointing because more data can lead to inaccurate results,\nhowever, regardless of the data size and the significance thresholds, the relative differences between $p$-values\nwhen there is no conditional independence and whe there is will be large and easy to detect.\n\nAs a result, the results obtained from this module should be regarded more as heuristics approach\nand as an indication of patterns in data as opposed to statement of ground truth and should be interpreted with caution.\nHowever, we recommend that if the percentage of failed tests is small (e.g., smaller than 30 percent), then that impact\nof inconsistency between network structure and data is minor in that causal query estimation. Hence, it is safe to\nproceed with the estimation procedure.\nIf the percentage of failed tests is large (greater than 30-40 percent), it indicates that the input network does not\nreflect the underlying data generation process, and the network or the data should be revised. Causal structure learning\nalgorithms, for examples the ones implemented in :mod:`pgmpy`\n(see `here <https://pgmpy.org/examples/Structure%20Learning%20in%20Bayesian%20Networks.html>`_)\ncan be used to revise the network structure and align it with data. This module currently does not repair the\nstructure of the network if the network is not aligned with data according to conditional independence tests.\n\nFor more reference on this topic, please see\nchapter 4 of https://livebook.manning.com/book/causal-ai/welcome/v-4/.\n\n.. [halsey2015fickle] Halsey, Lewis G., et al. \"The fickle P value generates irreproducible results.\n   \" Nature methods 12.3 (2015): 179-185.\n\n.. [wang2022addressing] Wang, Ming, and Qi Long. \"Addressing Common Misuses and Pitfalls of p values\n   in Biomedical Research.\" Cancer research 82.15 (2022): 2674-2677.\n\n.. [lucas2013too] Lucas, H., and G. Shmueli. \"Too big to fail: large samples and the p-value problem.\"\n   Inf. Syst. Res. 24.4 (2013): 906-917.\n\"\"\"",
        "\"\"\"Add undirected edges between d-separated nodes that fail a data-driven conditional independency test.\n\n    :param graph: An acyclic directed mixed graph\n    :param data: observational data corresponding to the graph\n    :param method:\n        The conditional independency test to use. If None, defaults to\n        :data:`y0.struct.DEFAULT_CONTINUOUS_CI_TEST` for continuous data\n        or :data:`y0.struct.DEFAULT_DISCRETE_CI_TEST` for discrete data.\n    :param significance_level: The statistical tests employ this value for\n        comparison with the p-value of the test to determine the independence of\n        the tested variables. If none, defaults to 0.05.\n    :returns: A copy of the input graph potentially with new undirected edges added\n    \"\"\"",
        "\"\"\"Print the summary of conditional independency test results.\n\n    Prints the summary to the console, which includes the total number of conditional independence tests,\n    the number and percentage of failed tests, and statistical information about each test such as p-values,\n    and test results.\n\n    :param graph: an NxMixedGraph\n    :param data: observational data corresponding to the graph\n    :param method: the conditional independency test to use. If None, defaults to ``pearson`` for continuous data\n        and ``chi-square`` for discrete data.\n    :param max_given: The maximum set size in the power set of the vertices minus the d-separable pairs\n    :param significance_level: The statistical tests employ this value for\n        comparison with the p-value of the test to determine the independence of\n        the tested variables. If none, defaults to 0.01.\n    :param verbose: If `False`, only print the details of failed tests.\n        If 'True', print the details of all the conditional independency results. Defaults to `False`\n    :param tablefmt: The format for the table that gets printed. By default, uses RST so it can be\n        directly copy/pasted into Python documentation\n    \"\"\"",
        "\"\"\"Calculate the p-value for a bootstrap data.\n\n    :param df: observational data\n    :param sample_size: number of data points to sample a bootstrap data from full_data\n    :param left: first variable name positioned on the left side of a conditional independence test\n    :param right: second variable name positioned on the right side of a conditional independence test\n    :param conditions: variables names to condition on in the conditional independence test\n    :param test: the conditional independency test to use. If None, defaults to ``pearson`` for continuous data\n        and ``chi-square`` for discrete data.\n    :param significance_level: The statistical tests employ this value for\n        comparison with the p-value of the test to determine the independence of\n        the tested variables. If none, defaults to 0.05.\n    :return: the $p$-value for the given bootstrap data\n    \"\"\"",
        "\"\"\"Calculate mean of p-value, the 5th percentile and 95th percentile error, for several bootstrap data.\n\n    :param df: A dataframe containing observational data\n    :param sample_size: number of data points to sample a bootstrap data from the dataframe\n    :param left: first variable name positioned on the left side of a conditional independence test\n    :param right: second variable name positioned on the right side of a conditional independence test\n    :param conditions: variables names to condition on in the conditional independence test\n    :param test: the conditional independency test to use. If None, defaults to ``pearson`` for continuous data\n        and ``chi-square`` for discrete data.\n    :param significance_level: The statistical tests employ this value for\n        comparison with the p-value of the test to determine the independence of\n        the tested variables. If none, defaults to 0.05.\n    :param boot_size: total number of times a bootstrap data is sampled\n    :return: the mean of p-value, the 5th percentile and 95 percentile error, for several bootstrap data\n    \"\"\"",
        "\"\"\"Generate the plot of expected p-value versus number of data points.\n\n    :param df: observational data\n    :param start: minimum number of data points to sample from df\n    :param stop: maximum number of data points to sample from df\n    :param step: minimum number of sampled data points increments by step number, and stops\n        before maximum number of sampled data points\n    :param left: first variable name positioned on the left side of a conditional independence test\n    :param right: second variable name positioned on the right side of a conditional independence test\n    :param conditions: variables names to condition on in the conditional independence test\n    :param test: the conditional independency test to use. If None, defaults to ``pearson`` for continuous data\n        and ``chi-square`` for discrete data.\n    :param significance_level: The statistical tests employ this value for comparison with the p-value of the test\n        to determine the independence of the tested variables. If none, defaults to 0.05.\n    :param boot_size: total number of times a bootstrap data is sampled\n    \"\"\""
    ],
    "functions": [
        "add_ci_undirected_edges",
        "print_graph_falsifications",
        "p_value_of_bootstrap_data",
        "p_value_statistics",
        "plot_ci_size_dependence"
    ],
    "classes": []
}