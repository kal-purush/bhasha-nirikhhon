{
    "identifiers": [
        "os",
        "pathlib",
        "Path",
        "torch",
        "transformers",
        "CLIPModel",
        "CLIPProcessor",
        "clarifai",
        "models",
        "model_serving",
        "model_config",
        "ModelTypes",
        "get_model_config",
        "get_model_config",
        "ModelTypes",
        "multimodal_embedder",
        "Path",
        "os",
        "path",
        "dirname",
        "CLIPModel",
        "from_pretrained",
        "os",
        "path",
        "join",
        "base_path",
        "model",
        "eval",
        "CLIPProcessor",
        "from_pretrained",
        "os",
        "path",
        "join",
        "base_path",
        "config",
        "inference",
        "wrap_func",
        "input_data",
        "kwargs",
        "inp",
        "input_data",
        "inp",
        "inp",
        "torch",
        "no_grad",
        "processor",
        "text",
        "text",
        "images",
        "image",
        "return_tensors",
        "padding",
        "text",
        "processor",
        "text",
        "text",
        "return_tensors",
        "padding",
        "model",
        "get_text_features",
        "inputs",
        "processor",
        "images",
        "image",
        "return_tensors",
        "padding",
        "model",
        "get_image_features",
        "inputs",
        "embeddings",
        "squeeze",
        "cpu",
        "numpy",
        "outputs",
        "append",
        "config",
        "inference",
        "return_type",
        "embedding_vector",
        "embeddings",
        "outputs"
    ],
    "literals": [
        "\"checkpoint\"",
        "\"checkpoint\"",
        "\"image\"",
        "\"text\"",
        "\"pt\"",
        "\"pt\"",
        "\"pt\""
    ],
    "variables": [
        "config",
        "base_path",
        "model",
        "processor",
        "outputs",
        "image",
        "text",
        "inputs",
        "inputs",
        "embeddings",
        "inputs",
        "embeddings",
        "embeddings"
    ],
    "comments": [
        "This file contains boilerplate code to allow users write their model",
        "inference code that will then interact with the Triton Inference Server",
        "Python backend to serve end user requests.",
        "The module name, module path, class name & get_predictions() method names MUST be maintained as is",
        "but other methods may be added within the class as deemed fit provided",
        "they are invoked within the main get_predictions() inference method",
        "if they play a role in any step of model inference",
        "sample model loading code:",
        "self.checkpoint_path: Path = os.path.join(self.base_path, \"your checkpoint filename/path\")",
        "self.model: Callable = <load_your_model_here from checkpoint or folder>",
        "self.text_model = CLIPTextModel.from_pretrained(os.path.join(self.base_path, \"openai/clip-vit-base-patch32\"))",
        "Add relevant model type decorator to the method below (see docs/model_types for ref.)"
    ],
    "docstrings": [
        "\"\"\"User model inference script.\"\"\"",
        "\"\"\"User model inference class.\"\"\"",
        "\"\"\"\n    Load inference time artifacts that are called frequently .e.g. models, tokenizers, etc.\n    in this method so they are loaded only once for faster inference.\n    \"\"\"",
        "\"\"\"\n    Main model inference method.\n\n    Args:\n    -----\n      input_data: A single input data item to predict on.\n        Input data can be an image or text, etc depending on the model type.\n\n    Returns:\n    --------\n      One of the clarifai.models.model_serving.models.output types. Refer to the README/docs\n    \"\"\""
    ],
    "functions": [
        "get_predictions"
    ],
    "classes": [
        "InferenceModel"
    ]
}