{
    "identifiers": [
        "os",
        "warnings",
        "typing",
        "Any",
        "Dict",
        "gymnasium",
        "gym",
        "hydra",
        "numpy",
        "np",
        "torch",
        "lightning",
        "fabric",
        "Fabric",
        "torch",
        "utils",
        "data",
        "BatchSampler",
        "DistributedSampler",
        "RandomSampler",
        "torchmetrics",
        "SumMetric",
        "sheeprl",
        "algos",
        "a2c",
        "agent",
        "build_agent",
        "sheeprl",
        "algos",
        "a2c",
        "loss",
        "policy_loss",
        "value_loss",
        "sheeprl",
        "algos",
        "a2c",
        "utils",
        "test",
        "sheeprl",
        "data",
        "ReplayBuffer",
        "sheeprl",
        "utils",
        "env",
        "make_env",
        "sheeprl",
        "utils",
        "logger",
        "get_log_dir",
        "get_logger",
        "sheeprl",
        "utils",
        "metric",
        "MetricAggregator",
        "sheeprl",
        "utils",
        "registry",
        "register_algorithm",
        "sheeprl",
        "utils",
        "timer",
        "timer",
        "sheeprl",
        "utils",
        "utils",
        "gae",
        "fabric",
        "Fabric",
        "agent",
        "torch",
        "nn",
        "Module",
        "optimizer",
        "torch",
        "optim",
        "Optimizer",
        "data",
        "Dict",
        "torch",
        "Tensor",
        "aggregator",
        "MetricAggregator",
        "cfg",
        "Dict",
        "Any",
        "next",
        "iter",
        "data",
        "values",
        "shape",
        "cfg",
        "buffer",
        "share_data",
        "DistributedSampler",
        "indexes",
        "num_replicas",
        "fabric",
        "world_size",
        "rank",
        "fabric",
        "global_rank",
        "shuffle",
        "seed",
        "cfg",
        "seed",
        "RandomSampler",
        "indexes",
        "BatchSampler",
        "sampler",
        "batch_size",
        "cfg",
        "algo",
        "per_rank_batch_size",
        "drop_last",
        "optimizer",
        "zero_grad",
        "set_to_none",
        "cfg",
        "buffer",
        "share_data",
        "sampler",
        "sampler",
        "set_epoch",
        "i",
        "batch_idxes",
        "sampler",
        "k",
        "v",
        "batch_idxes",
        "k",
        "v",
        "data",
        "items",
        "k",
        "v",
        "k",
        "v",
        "batch",
        "items",
        "k",
        "cfg",
        "algo",
        "mlp_keys",
        "encoder",
        "i",
        "len",
        "sampler",
        "fabric",
        "no_backward_sync",
        "agent",
        "enabled",
        "is_accumulating",
        "agent",
        "obs",
        "torch",
        "split",
        "batch",
        "agent",
        "actions_dim",
        "dim",
        "policy_loss",
        "logprobs",
        "batch",
        "cfg",
        "algo",
        "loss_reduction",
        "value_loss",
        "values",
        "batch",
        "cfg",
        "algo",
        "loss_reduction",
        "pg_loss",
        "v_loss",
        "fabric",
        "backward",
        "loss",
        "is_accumulating",
        "cfg",
        "algo",
        "max_grad_norm",
        "fabric",
        "clip_gradients",
        "agent",
        "optimizer",
        "max_norm",
        "cfg",
        "algo",
        "max_grad_norm",
        "optimizer",
        "step",
        "aggregator",
        "aggregator",
        "disabled",
        "aggregator",
        "update",
        "pg_loss",
        "detach",
        "aggregator",
        "update",
        "v_loss",
        "detach",
        "register_algorithm",
        "decoupled",
        "fabric",
        "Fabric",
        "cfg",
        "Dict",
        "Any",
        "fabric",
        "global_rank",
        "fabric",
        "world_size",
        "fabric",
        "device",
        "fabric",
        "seed_everything",
        "cfg",
        "seed",
        "cfg",
        "checkpoint",
        "resume_from",
        "fabric",
        "load",
        "cfg",
        "checkpoint",
        "resume_from",
        "get_logger",
        "fabric",
        "cfg",
        "logger",
        "fabric",
        "is_global_zero",
        "logger",
        "fabric",
        "logger",
        "log_hyperparams",
        "cfg",
        "get_log_dir",
        "fabric",
        "cfg",
        "root_dir",
        "cfg",
        "run_name",
        "fabric",
        "log_dir",
        "gym",
        "vector",
        "SyncVectorEnv",
        "cfg",
        "env",
        "sync_env",
        "gym",
        "vector",
        "AsyncVectorEnv",
        "vectorized_env",
        "make_env",
        "cfg",
        "cfg",
        "seed",
        "rank",
        "cfg",
        "env",
        "num_envs",
        "i",
        "rank",
        "cfg",
        "env",
        "num_envs",
        "log_dir",
        "rank",
        "vector_env_idx",
        "i",
        "i",
        "cfg",
        "env",
        "num_envs",
        "envs",
        "single_observation_space",
        "isinstance",
        "observation_space",
        "gym",
        "spaces",
        "Dict",
        "RuntimeError",
        "observation_space",
        "len",
        "cfg",
        "algo",
        "mlp_keys",
        "encoder",
        "RuntimeError",
        "k",
        "cfg",
        "algo",
        "mlp_keys",
        "encoder",
        "cfg",
        "algo",
        "cnn_keys",
        "encoder",
        "k",
        "observation_space",
        "keys",
        "len",
        "observation_space",
        "k",
        "shape",
        "ValueError",
        "k",
        "observation_space",
        "k",
        "shape",
        "cfg",
        "env",
        "id",
        "cfg",
        "metric",
        "log_level",
        "fabric",
        "cfg",
        "algo",
        "cnn_keys",
        "encoder",
        "fabric",
        "cfg",
        "algo",
        "mlp_keys",
        "encoder",
        "cfg",
        "algo",
        "mlp_keys",
        "encoder",
        "isinstance",
        "envs",
        "single_action_space",
        "gym",
        "spaces",
        "Box",
        "isinstance",
        "envs",
        "single_action_space",
        "gym",
        "spaces",
        "MultiDiscrete",
        "envs",
        "single_action_space",
        "shape",
        "is_continuous",
        "envs",
        "single_action_space",
        "nvec",
        "tolist",
        "is_multidiscrete",
        "envs",
        "single_action_space",
        "n",
        "build_agent",
        "fabric",
        "actions_dim",
        "is_continuous",
        "cfg",
        "observation_space",
        "state",
        "cfg",
        "checkpoint",
        "resume_from",
        "hydra",
        "utils",
        "instantiate",
        "cfg",
        "algo",
        "optimizer",
        "agent",
        "parameters",
        "_convert_",
        "MetricAggregator",
        "disabled",
        "MetricAggregator",
        "hydra",
        "utils",
        "instantiate",
        "cfg",
        "metric",
        "aggregator",
        "_convert_",
        "to",
        "device",
        "ReplayBuffer",
        "cfg",
        "buffer",
        "size",
        "cfg",
        "env",
        "num_envs",
        "memmap",
        "cfg",
        "buffer",
        "memmap",
        "memmap_dir",
        "os",
        "path",
        "join",
        "log_dir",
        "fabric",
        "global_rank",
        "obs_keys",
        "obs_keys",
        "cfg",
        "env",
        "num_envs",
        "cfg",
        "algo",
        "rollout_steps",
        "world_size",
        "cfg",
        "algo",
        "total_steps",
        "policy_steps_per_update",
        "cfg",
        "dry_run",
        "cfg",
        "metric",
        "log_every",
        "policy_steps_per_update",
        "warnings",
        "warn",
        "cfg",
        "metric",
        "log_every",
        "policy_steps_per_update",
        "cfg",
        "checkpoint",
        "every",
        "policy_steps_per_update",
        "warnings",
        "warn",
        "cfg",
        "checkpoint",
        "every",
        "policy_steps_per_update",
        "envs",
        "reset",
        "seed",
        "cfg",
        "seed",
        "k",
        "obs_keys",
        "next_obs",
        "k",
        "np",
        "newaxis",
        "update",
        "num_updates",
        "_",
        "cfg",
        "algo",
        "rollout_steps",
        "policy_step",
        "cfg",
        "env",
        "num_envs",
        "world_size",
        "timer",
        "SumMetric",
        "sync_on_compute",
        "torch",
        "no_grad",
        "k",
        "torch",
        "as_tensor",
        "next_obs",
        "k",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "k",
        "obs_keys",
        "agent",
        "torch_obs",
        "is_continuous",
        "torch",
        "cat",
        "actions",
        "cpu",
        "numpy",
        "torch",
        "cat",
        "act",
        "argmax",
        "dim",
        "act",
        "actions",
        "axis",
        "cpu",
        "numpy",
        "torch",
        "cat",
        "actions",
        "cpu",
        "numpy",
        "envs",
        "step",
        "real_actions",
        "reshape",
        "envs",
        "action_space",
        "shape",
        "np",
        "logical_or",
        "done",
        "truncated",
        "dones",
        "reshape",
        "cfg",
        "env",
        "num_envs",
        "rewards",
        "reshape",
        "cfg",
        "env",
        "num_envs",
        "dones",
        "np",
        "newaxis",
        "values",
        "cpu",
        "numpy",
        "np",
        "newaxis",
        "actions",
        "np",
        "newaxis",
        "rewards",
        "np",
        "newaxis",
        "cfg",
        "buffer",
        "memmap",
        "np",
        "zeros_like",
        "rewards",
        "shape",
        "rewards",
        "shape",
        "np",
        "zeros_like",
        "rewards",
        "shape",
        "rewards",
        "shape",
        "rb",
        "add",
        "step_data",
        "validate_args",
        "cfg",
        "buffer",
        "validate_args",
        "k",
        "obs_keys",
        "obs",
        "k",
        "_obs",
        "np",
        "newaxis",
        "_obs",
        "cfg",
        "metric",
        "log_level",
        "info",
        "i",
        "agent_ep_info",
        "info",
        "agent_ep_info",
        "agent_ep_info",
        "agent_ep_info",
        "aggregator",
        "aggregator",
        "aggregator",
        "update",
        "ep_rew",
        "aggregator",
        "aggregator",
        "aggregator",
        "update",
        "ep_len",
        "fabric",
        "policy_step",
        "i",
        "ep_rew",
        "rb",
        "to_tensor",
        "dtype",
        "device",
        "device",
        "from_numpy",
        "cfg",
        "buffer",
        "from_numpy",
        "torch",
        "no_grad",
        "k",
        "torch",
        "as_tensor",
        "next_obs",
        "k",
        "dtype",
        "torch",
        "float32",
        "device",
        "device",
        "k",
        "obs_keys",
        "agent",
        "get_value",
        "torch_obs",
        "gae",
        "local_data",
        "to",
        "torch",
        "float64",
        "local_data",
        "local_data",
        "next_values",
        "cfg",
        "algo",
        "rollout_steps",
        "cfg",
        "algo",
        "gamma",
        "cfg",
        "algo",
        "gae_lambda",
        "returns",
        "advantages",
        "timer",
        "SumMetric",
        "sync_on_compute",
        "cfg",
        "metric",
        "sync_on_compute",
        "train",
        "fabric",
        "agent",
        "optimizer",
        "local_data",
        "aggregator",
        "cfg",
        "policy_step",
        "last_log",
        "cfg",
        "metric",
        "log_every",
        "update",
        "num_updates",
        "cfg",
        "dry_run",
        "aggregator",
        "aggregator",
        "disabled",
        "aggregator",
        "compute",
        "fabric",
        "log_dict",
        "metrics_dict",
        "policy_step",
        "aggregator",
        "reset",
        "timer",
        "disabled",
        "timer",
        "compute",
        "timer_metrics",
        "fabric",
        "log",
        "train_step",
        "last_train",
        "timer_metrics",
        "policy_step",
        "timer_metrics",
        "fabric",
        "log",
        "policy_step",
        "last_log",
        "world_size",
        "cfg",
        "env",
        "action_repeat",
        "timer_metrics",
        "policy_step",
        "timer",
        "reset",
        "policy_step",
        "train_step",
        "cfg",
        "checkpoint",
        "every",
        "policy_step",
        "last_checkpoint",
        "cfg",
        "checkpoint",
        "every",
        "cfg",
        "dry_run",
        "update",
        "num_updates",
        "policy_step",
        "agent",
        "state_dict",
        "optimizer",
        "state_dict",
        "update",
        "os",
        "path",
        "join",
        "log_dir",
        "policy_step",
        "fabric",
        "global_rank",
        "fabric",
        "call",
        "fabric",
        "fabric",
        "ckpt_path",
        "ckpt_path",
        "state",
        "state",
        "envs",
        "close",
        "fabric",
        "is_global_zero",
        "test",
        "agent",
        "fabric",
        "cfg",
        "log_dir",
        "cfg",
        "model_manager",
        "disabled",
        "fabric",
        "is_global_zero",
        "sheeprl",
        "algos",
        "ppo",
        "utils",
        "log_models",
        "sheeprl",
        "utils",
        "mlflow",
        "register_model",
        "agent",
        "register_model",
        "fabric",
        "log_models",
        "cfg",
        "models_to_log"
    ],
    "literals": [
        "\"actions\"",
        "\"advantages\"",
        "\"returns\"",
        "\"Loss/policy_loss\"",
        "\"Loss/value_loss\"",
        "f\"Log dir: {log_dir}\"",
        "\"train\"",
        "f\"Unexpected observation type, should be of type Dict, got: {observation_space}\"",
        "\"You should specify at least one MLP key for the encoder: `algo.mlp_keys.encoder=[state]`\"",
        "\"Only environments with vector-only observations are supported by the A2C agent. \"",
        "f\"The observation with key '{k}' has shape {observation_space[k].shape}. \"",
        "f\"Provided environment: {cfg.env.id}\"",
        "\"Encoder CNN keys:\"",
        "\"Encoder MLP keys:\"",
        "\"agent\"",
        "\"all\"",
        "\"all\"",
        "\"memmap_buffer\"",
        "f\"rank_{fabric.global_rank}\"",
        "f\"The metric.log_every parameter ({cfg.metric.log_every}) is not a multiple of the \"",
        "f\"policy_steps_per_update value ({policy_steps_per_update}), so \"",
        "\"the metrics will be logged at the nearest greater multiple of the \"",
        "\"policy_steps_per_update value.\"",
        "f\"The checkpoint.every parameter ({cfg.checkpoint.every}) is not a multiple of the \"",
        "f\"policy_steps_per_update value ({policy_steps_per_update}), so \"",
        "\"the checkpoint will be saved at the nearest greater multiple of the \"",
        "\"policy_steps_per_update value.\"",
        "\"Time/env_interaction_time\"",
        "\"dones\"",
        "\"values\"",
        "\"actions\"",
        "\"rewards\"",
        "\"returns\"",
        "\"advantages\"",
        "\"final_info\"",
        "\"final_info\"",
        "\"episode\"",
        "\"r\"",
        "\"episode\"",
        "\"l\"",
        "\"Rewards/rew_avg\"",
        "\"Rewards/rew_avg\"",
        "\"Game/ep_len_avg\"",
        "\"Game/ep_len_avg\"",
        "f\"Rank-0: policy_step={policy_step}, reward_env_{i}={ep_rew[-1]}\"",
        "\"rewards\"",
        "\"values\"",
        "\"dones\"",
        "\"returns\"",
        "\"advantages\"",
        "\"Time/train_time\"",
        "\"Time/train_time\"",
        "\"Time/sps_train\"",
        "\"Time/train_time\"",
        "\"Time/env_interaction_time\"",
        "\"Time/sps_env_interaction\"",
        "\"Time/env_interaction_time\"",
        "\"agent\"",
        "\"optimizer\"",
        "\"update_step\"",
        "f\"checkpoint/ckpt_{policy_step}_{fabric.global_rank}.ckpt\"",
        "\"on_checkpoint_coupled\"",
        "\"agent\""
    ],
    "variables": [
        "indexes",
        "sampler",
        "sampler",
        "sampler",
        "batch",
        "obs",
        "is_accumulating",
        "_",
        "logprobs",
        "values",
        "pg_loss",
        "v_loss",
        "loss",
        "rank",
        "world_size",
        "device",
        "state",
        "logger",
        "fabric",
        "_loggers",
        "log_dir",
        "vectorized_env",
        "envs",
        "observation_space",
        "obs_keys",
        "is_continuous",
        "is_multidiscrete",
        "actions_dim",
        "agent",
        "optimizer",
        "aggregator",
        "aggregator",
        "rb",
        "last_log",
        "last_train",
        "train_step",
        "policy_step",
        "last_checkpoint",
        "policy_steps_per_update",
        "num_updates",
        "step_data",
        "next_obs",
        "step_data",
        "k",
        "torch_obs",
        "actions",
        "_",
        "values",
        "real_actions",
        "real_actions",
        "actions",
        "obs",
        "rewards",
        "done",
        "truncated",
        "info",
        "dones",
        "dones",
        "rewards",
        "step_data",
        "step_data",
        "step_data",
        "step_data",
        "step_data",
        "step_data",
        "next_obs",
        "_obs",
        "step_data",
        "k",
        "next_obs",
        "k",
        "ep_rew",
        "ep_len",
        "local_data",
        "torch_obs",
        "next_values",
        "returns",
        "advantages",
        "local_data",
        "local_data",
        "metrics_dict",
        "timer_metrics",
        "last_log",
        "last_train",
        "last_checkpoint",
        "state",
        "ckpt_path",
        "models_to_log"
    ],
    "comments": [
        "Prepare the sampler",
        "If we are in the distributed setting, we need to use a DistributedSampler, which",
        "will shuffle the data at each epoch and will ensure that each process will get",
        "a different part of the data",
        "Train the agent",
        "Even though in the Spinning-Up A2C algorithm implementation",
        "(https://spinningup.openai.com/en/latest/algorithms/vpg.html) the policy gradient is estimated",
        "by taking the mean over all the sequences collected",
        "of the sum of the actions log-probabilities gradients' multiplied by the advantages,",
        "we do not do that, instead we take the overall sum (or mean, depending on the loss reduction).",
        "This is achieved by accumulating the gradients and calling the backward method only at the end.",
        "is_accumulating is True for every i except for the last one",
        "Policy loss",
        "Value loss",
        "Update metrics",
        "Resume from checkpoint",
        "Create Logger. This will create the logger only on the",
        "rank-0 process",
        "Environment setup",
        "Create the agent model: this should be a torch.nn.Module to be accelerated with Fabric",
        "Given that the environment has been created with the `make_env` method, the agent",
        "forward method must accept as input a dictionary like {\"obs1_name\": obs1, \"obs2_name\": obs2, ...}.",
        "The agent should be able to process both image and vector-like observations.",
        "the optimizer and set up it with Fabric",
        "Create a metric aggregator to log the metrics",
        "Local data",
        "Global variables",
        "Warning for log and checkpoint every",
        "Get the first environment observation and start the optimization",
        "[N_envs, N_obs]",
        "Measure environment interaction time: this considers both the model forward",
        "to get the action given the observation and the time taken into the environment",
        "Sample an action given the observation received by the environment",
        "This calls the `forward` method of the PyTorch module, escaping from Fabric",
        "because we don't want this to be a synchronization point",
        "Single environment step",
        "Update the step data",
        "Append data to buffer",
        "Update the observation and dones",
        "Transform the data into PyTorch Tensors",
        "Estimate returns with GAE (https://arxiv.org/abs/1506.02438)",
        "Add returns and advantages to the buffer",
        "Train the agent",
        "Log metrics",
        "Sync distributed metrics",
        "Sync distributed timers",
        "Reset counters",
        "Checkpoint model"
    ],
    "docstrings": [
        "\"\"\"Train the agent on the data collected from the environment.\"\"\""
    ],
    "functions": [
        "train",
        "main"
    ],
    "classes": []
}