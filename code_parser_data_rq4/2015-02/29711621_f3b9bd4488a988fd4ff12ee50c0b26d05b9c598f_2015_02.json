{
    "identifiers": [
        "print_function",
        "unicode_literals",
        "nltk",
        "nltk",
        "corpus",
        "CrubadanCorpusReader",
        "nltk",
        "tokenize",
        "word_tokenize",
        "nltk",
        "probability",
        "FreqDist",
        "sys",
        "maxint",
        "regex",
        "re",
        "encode",
        "encode",
        "CrubadanCorpusReader",
        "nltk",
        "data",
        "find",
        "text",
        "_START_CHAR",
        "text",
        "_END_CHAR",
        "i",
        "len",
        "padded_text",
        "padded_text",
        "i",
        "i",
        "len",
        "cur_trigram",
        "cur_trigram",
        "_END_CHAR",
        "trigrams",
        "append",
        "cur_trigram",
        "trigrams",
        "trigrams",
        "t",
        "trigrams",
        "t",
        "text",
        "re",
        "sub",
        "text",
        "decode",
        "text",
        "remove_punctuation",
        "text",
        "word_tokenize",
        "clean_text",
        "FreqDist",
        "t",
        "tokens",
        "trigrams",
        "t",
        "cur_trigram",
        "token_trigrams",
        "cur_trigram",
        "fingerprint",
        "fingerprint",
        "cur_trigram",
        "fingerprint",
        "lang",
        "trigram",
        "text_profile",
        "_corpus",
        "all_lang_freq",
        "lang",
        "trigram",
        "lang_fd",
        "lang_fd",
        "keys",
        "index",
        "trigram",
        "text_profile",
        "keys",
        "index",
        "trigram",
        "abs",
        "idx_lang_profile",
        "idx_text",
        "maxint",
        "dist",
        "text",
        "profile",
        "text",
        "lang",
        "_corpus",
        "all_lang_freq",
        "keys",
        "trigram",
        "profile",
        "lang_dist",
        "calc_dist",
        "lang",
        "trigram",
        "profile",
        "lang_dist",
        "distances",
        "text",
        "lang_dists",
        "text",
        "min",
        "r",
        "key",
        "r",
        "get",
        "os",
        "listdir",
        "os",
        "path",
        "isfile",
        "f",
        "listdir",
        "path",
        "isfile",
        "f",
        "re",
        "match",
        "f",
        "m",
        "lang_samples",
        "append",
        "f",
        "lang_samples",
        "f",
        "lang_samples",
        "open",
        "f",
        "cur_sample",
        "read",
        "f",
        "cur_data",
        "decode",
        "guess_language",
        "cur_data"
    ],
    "literals": [
        "\"<\"",
        "'utf8'",
        "\">\"",
        "'utf8'",
        "'corpora/crubadan'",
        "'.*\\.txt'",
        "ur\"[^\\P{P}\\']+\"",
        "\"\"",
        "'utf8'",
        "'.'",
        "'sample_\\w+\\.txt'",
        "'rU'",
        "'Language sample file: '",
        "'Contents snippet:  '",
        "'utf8'",
        "'#################################################'",
        "'Language detection: '",
        "'#################################################'"
    ],
    "variables": [
        "_corpus",
        "fingerprints",
        "_START_CHAR",
        "_END_CHAR",
        "_corpus",
        "padded_text",
        "trigrams",
        "cur_trigram",
        "cur_trigram",
        "clean_text",
        "tokens",
        "fingerprint",
        "token_trigrams",
        "fingerprint",
        "cur_trigram",
        "lang_fd",
        "dist",
        "idx_lang_profile",
        "idx_text",
        "dist",
        "dist",
        "distances",
        "profile",
        "lang_dist",
        "distances",
        "lang",
        "r",
        "path",
        "lang_samples",
        "m",
        "cur_sample",
        "cur_data"
    ],
    "comments": [
        "-*- coding: utf-8 -*-",
        "Natural Language Toolkit: Language ID module using TextCat algorithm",
        "",
        "Copyright (C) 2001-2015 NLTK Project",
        "Author: Avital Pekker <avital.pekker@utoronto.ca>",
        "",
        "URL: <http://nltk.org/>",
        "For license information, see LICENSE.TXT",
        "Ensure that your own literal strings default to unicode rather than str.",
        "Note: this is NOT \"re\" you're likely used to. The regex module",
        "is an alternative to the standard re module that supports",
        "Unicode codepoint properties with the \\p{} syntax.",
        "You may have to \"pip install regx\"",
        "",
        "Language identification using TextCat",
        "",
        "Generate 3-grams for given text",
        "Arbitrary but should be larger than",
        "any possible trigram file length",
        "in terms of total lines",
        "For all the languages",
        "Calculate distance metric for every trigram in",
        "input text to be identified",
        "Current dir"
    ],
    "docstrings": [
        "\"\"\"\nA module for language identification using the TextCat algorithm.\nAn implementation of the text categorization algorithm\npresented in Cavnar, W. B. and J. M. Trenkle, \n\"N-Gram-Based Text Categorization\".\n\nThe algorithm takes advantage of Zipf's law and uses \nn-gram frequencies to profile languages and text-yet to\nbe identified-then compares using a distance measure.\n\nLanguage n-grams are provided by the \"An Crubadan\"\nproject. A corpus reader was created seperately to read\nthose files.\n\nFor details regarding the algorithm, see:\nhttp://www.let.rug.nl/~vannoord/TextCat/textcat.pdf\n\nFor details about An Crubadan, see:\nhttp://borel.slu.edu/crubadan/index.html\n\"\"\"",
        "''' Get rid of punctuation except apostrophes '''",
        "''' Create FreqDist of trigrams within text '''",
        "''' Calculate the \"out-of-place\" measure between the\n            text and language profile for a single trigram '''",
        "''' Calculate the \"out-of-place\" measure between\n            the text and all languages '''",
        "''' Find the language with the min distance\n            to the text and return its ISO 639-3 code '''",
        "''' Demo of language guessing using a bunch of UTF-8 encoded\n            text files with snippets of text copied from news websites\n            around the web in different languages '''"
    ],
    "functions": [
        "trigrams",
        "_print_trigrams",
        "remove_punctuation",
        "profile",
        "calc_dist",
        "lang_dists",
        "guess_language",
        "demo"
    ],
    "classes": [
        "TextCat"
    ]
}