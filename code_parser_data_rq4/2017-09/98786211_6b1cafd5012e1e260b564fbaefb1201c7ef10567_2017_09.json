{
    "identifiers": [
        "os",
        "csv",
        "nltk",
        "tokenize",
        "sent_tokenize",
        "nltk",
        "tokenize",
        "word_tokenize",
        "argparse",
        "ntpath",
        "sys",
        "tensorflow",
        "tf",
        "numpy",
        "np",
        "embed_dir",
        "embed_dim",
        "keep_embed",
        "open",
        "os",
        "path",
        "join",
        "embed_dir",
        "format",
        "embed_dim",
        "f",
        "name",
        "avg_num_embedd",
        "np",
        "zeros",
        "line",
        "f",
        "line",
        "split",
        "row",
        "glove_vocab",
        "append",
        "word",
        "keep_embed",
        "is_number",
        "word",
        "num_freq",
        "avg_num_embedd",
        "val",
        "val",
        "row",
        "glove_embeddings",
        "append",
        "val",
        "val",
        "row",
        "f",
        "close",
        "avg_num_embedd",
        "num_freq",
        "glove_word_to_id",
        "word",
        "i",
        "i",
        "word",
        "glove_vocab",
        "glove_vocab",
        "glove_word_to_id",
        "glove_embeddings",
        "word",
        "word",
        "unknown_words",
        "unknown_words",
        "word",
        "word",
        "word",
        "glove_word_to_id",
        "add_unkonwn_word",
        "word",
        "word",
        "vocab",
        "word",
        "vocab",
        "vocab",
        "word",
        "word",
        "s",
        "s",
        "numbers_freq",
        "numbers_freq",
        "ValueError",
        "word",
        "is_number",
        "word",
        "add_word",
        "filtered_vocabulary",
        "in_glove_vocab",
        "word",
        "add_word",
        "word",
        "filtered_vocabulary",
        "word",
        "add_word",
        "filtered_vocabulary",
        "word",
        "threshold",
        "is_number",
        "word",
        "word",
        "vocab",
        "word",
        "threshold",
        "path",
        "dataset",
        "dataset",
        "open",
        "path",
        "encoding",
        "errors",
        "f",
        "csv",
        "reader",
        "f",
        "delimiter",
        "delimiter",
        "line",
        "reader",
        "first_line",
        "line",
        "len",
        "line",
        "line",
        "dataset",
        "line",
        "row_length",
        "line",
        "line",
        "sent_tokenize",
        "paper",
        "word_tokenize",
        "x",
        "x",
        "sentences",
        "add_word",
        "word",
        "vocab",
        "sentence",
        "sentences",
        "word",
        "sentence",
        "doc_id",
        "documents",
        "word",
        "documents",
        "doc_id",
        "is_less_frequent",
        "word",
        "MIN_WORD_FREQUENCY",
        "filtered_documents",
        "doc_id",
        "append",
        "process_word",
        "word",
        "ntpath",
        "split",
        "path",
        "os",
        "path",
        "join",
        "parent_dir",
        "filename",
        "format",
        "out_path",
        "open",
        "out_path",
        "encoding",
        "errors",
        "f",
        "csv",
        "writer",
        "f",
        "delimiter",
        "delimiter",
        "quoting",
        "csv",
        "QUOTE_MINIMAL",
        "writer",
        "writerow",
        "doc_id",
        "filtered_documents",
        "writer",
        "writerow",
        "doc_id",
        "join",
        "filtered_documents",
        "doc_id",
        "format",
        "out_path",
        "value",
        "tf",
        "train",
        "Feature",
        "int64_list",
        "tf",
        "train",
        "Int64List",
        "value",
        "value",
        "value",
        "tf",
        "train",
        "Feature",
        "bytes_list",
        "tf",
        "train",
        "BytesList",
        "value",
        "value",
        "filename",
        "filtered_vocabulary",
        "format",
        "filename",
        "tf",
        "python_io",
        "TFRecordWriter",
        "filename",
        "word",
        "filtered_vocabulary",
        "word",
        "avg_num_embedd",
        "glove_word_to_id",
        "word",
        "glove_embeddings",
        "word_id",
        "_bytes_feature",
        "tf",
        "compat",
        "as_bytes",
        "word",
        "_int64_feature",
        "word_id",
        "tf",
        "train",
        "Feature",
        "float_list",
        "tf",
        "train",
        "FloatList",
        "value",
        "word_embed",
        "tf",
        "train",
        "Example",
        "features",
        "tf",
        "train",
        "Features",
        "feature",
        "feature",
        "writer",
        "write",
        "example",
        "SerializeToString",
        "writer",
        "close",
        "sys",
        "stdout",
        "flush",
        "argparse",
        "ArgumentParser",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "choices",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "choices",
        "parser",
        "parse_args",
        "args",
        "dataset",
        "args",
        "data_dir",
        "args",
        "dataset",
        "args",
        "data_dir",
        "args",
        "dataset",
        "args",
        "data_dir",
        "args",
        "data_dir",
        "os",
        "path",
        "join",
        "dataset_folder",
        "os",
        "path",
        "exists",
        "raw_data_path",
        "format",
        "raw_data_path",
        "load_glove_embeddings",
        "args",
        "embedding_dir",
        "args",
        "embedding_dim",
        "keep_embed",
        "process_documents",
        "raw_data_path",
        "args",
        "dataset",
        "os",
        "path",
        "join",
        "dataset_folder",
        "format",
        "args",
        "dataset",
        "args",
        "embedding_dim",
        "save_embeddings",
        "embeddings_path",
        "format",
        "len",
        "vocab",
        "format",
        "len",
        "vocab",
        "len",
        "unknown_words",
        "numbers_freq",
        "format",
        "len",
        "unknown_words",
        "sum",
        "unknown_words",
        "values",
        "format",
        "numbers_freq",
        "main"
    ],
    "literals": [
        "'glove.6B.{0}d.txt'",
        "\"Loading GloVe embedding data  \"",
        "'<NUM>'",
        "'<NUM>'",
        "'unk'",
        "'unk'",
        "'unk'",
        "','",
        "'citeulike-t'",
        "'\\t'",
        "'Reading documents ...'",
        "\"r\"",
        "'utf-8'",
        "'ignore'",
        "'citeulike-t'",
        "' '",
        "'Processing documents ...'",
        "'processed-'",
        "'Writing processed file {0} ...'",
        "\"w\"",
        "'utf-8'",
        "'ignore'",
        "'doc_id'",
        "'title+abstract'",
        "' '",
        "'Finished Writing.'",
        "'Writing embeddings data {0} ...'",
        "'<NUM>'",
        "'word'",
        "'glove_id'",
        "'embed'",
        "'--data_dir'",
        "'/home/wanli/data/Extended_ctr'",
        "'data directory containing input.txt'",
        "\"--dataset\"",
        "\"-d\"",
        "'dummy'",
        "\"Which dataset to use\"",
        "'dummy'",
        "'citeulike-a'",
        "'citeulike-t'",
        "'--embedding_dir'",
        "'/home/wanli/data/glove.6B/'",
        "'GloVe embedding directory containing embeddings file'",
        "'--embedding_dim'",
        "'dimension of the embeddings'",
        "'50'",
        "'100'",
        "'200'",
        "'300'",
        "'citeulike-a'",
        "'/citeulike_a_extended'",
        "'citeulike-t'",
        "'/citeulike_t_extended'",
        "'dummy'",
        "'/dummy'",
        "\"Warning: Given dataset not known, setting to dummy\"",
        "'/citeulike_a_extended'",
        "'raw-data.csv'",
        "\"File {0} doesn't exist\"",
        "'{0}-embeddings-{1}.tfrecord'",
        "'Raw data vocabulary size {0}'",
        "'Processed data vocabulary size {0}'",
        "'# of unique unknown words {0}, frequency of unknown words {1}'",
        "'Numbers frequency {0}'",
        "'__main__'"
    ],
    "variables": [
        "vocab",
        "unknown_words",
        "filtered_vocabulary",
        "glove_vocab",
        "glove_word_to_id",
        "glove_embeddings",
        "avg_num_embedd",
        "MIN_WORD_FREQUENCY",
        "numbers_freq",
        "f",
        "num_freq",
        "avg_num_embedd",
        "row",
        "word",
        "avg_num_embedd",
        "glove_word_to_id",
        "unknown_words",
        "word",
        "vocab",
        "word",
        "delimiter",
        "delimiter",
        "reader",
        "first_line",
        "documents",
        "row_length",
        "labels",
        "row_length",
        "first_line",
        "doc_id",
        "paper",
        "paper",
        "sentences",
        "sentences",
        "documents",
        "doc_id",
        "filtered_documents",
        "filtered_documents",
        "doc_id",
        "parent_dir",
        "filename",
        "out_path",
        "writer",
        "writer",
        "word_id",
        "word_embed",
        "word_id",
        "word_embed",
        "feature",
        "example",
        "parser",
        "args",
        "dataset_folder",
        "dataset_folder",
        "dataset_folder",
        "dataset_folder",
        "raw_data_path",
        "embeddings_path"
    ],
    "comments": [
        "Load pre-trained embeddings",
        "Count the number of the occurance of numbers in glove embeddings",
        "Add the word to the vocabulary list of the pre-trained word embeddings dataset",
        "Add the embedding of the word",
        "calculate the number embedding as the avg of the numbers that are in glove embeddings",
        "It's a naive way to do it!!",
        "Create a word to id index",
        "read raw data",
        "process raw data,",
        "- remove less frequent words",
        "- replace unknown words with 'unk' token, replace numbers with '<NUM>'",
        "write the processd documents",
        "Create a feature",
        "Create an example protocol buffer",
        "Serialize to string and write on the file"
    ],
    "docstrings": [
        "\"\"\"\n    Check if the word id exists in the vocabulary of the pre-trained embeddings .\n        :returns: Word id\n        :rtype int\n    \"\"\"",
        "'''\n    replace unknown words with 'unk' token, replace numbers with '<NUM>'. Otherwise, return word\n    :param word:\n    :return:\n    '''",
        "\"\"\"\n     Parses paper raw data\n     :return: A tuple of Papers' labels and abstracts, where abstracts are returned as strings\n     \"\"\""
    ],
    "functions": [
        "load_glove_embeddings",
        "add_unkonwn_word",
        "in_glove_vocab",
        "add_word",
        "is_number",
        "process_word",
        "is_less_frequent",
        "process_documents",
        "_int64_feature",
        "_bytes_feature",
        "save_embeddings",
        "main"
    ],
    "classes": []
}