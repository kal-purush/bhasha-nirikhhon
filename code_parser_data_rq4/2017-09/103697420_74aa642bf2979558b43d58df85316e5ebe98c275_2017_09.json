{
    "identifiers": [
        "keras",
        "keras",
        "preprocessing",
        "text",
        "Tokenizer",
        "Tokenizer",
        "num_words",
        "tokenizer",
        "fit_on_texts",
        "samples",
        "tokenizer",
        "texts_to_sequences",
        "samples",
        "tokenizer",
        "texts_to_matrix",
        "samples",
        "mode",
        "tokenizer",
        "word_index",
        "len",
        "word_index",
        "word",
        "word_index",
        "word",
        "one_hot_results",
        "numpy",
        "np",
        "np",
        "zeros",
        "len",
        "samples",
        "max_length",
        "dimensionality",
        "i",
        "sample",
        "samples",
        "j",
        "word",
        "sample",
        "split",
        "max_length",
        "abs",
        "hash",
        "word",
        "dimensionality",
        "results"
    ],
    "literals": [
        "'The cat sat on the mat.'",
        "'The dog ate my homework.'",
        "'binary'",
        "'Found %s unique tokens.'",
        "'The cat sat on the mat.'",
        "'The dog ate my homework.'"
    ],
    "variables": [
        "samples",
        "tokenizer",
        "sequences",
        "one_hot_results",
        "word_index",
        "samples",
        "dimensionality",
        "max_length",
        "results",
        "index",
        "results",
        "i",
        "j",
        "index"
    ],
    "comments": [
        "TextGen",
        "",
        "Created by: Guy Feldman <gfeldman8@gmail.com>",
        "Created: 9/14/17",
        "",
        "ID: ex601.py",
        "We create a tokenizer, configured to only take",
        "into account the top-1000 most common on words",
        "This builds the word index",
        "This turns strings into lists of integer indices.",
        "You could also directly get the one-hot binary representations.",
        "Note that other vectorization modes than one-hot encoding are supported!",
        "This is how you can recover the word index that was computed",
        "We will store our words as vectors of size 1000.",
        "Note that if you have close to 1000 words (or more)",
        "you will start seeing many hash collisions, which",
        "will decrease the accuracy of this encoding method.",
        "Hash the word into a \"random\" integer index",
        "that is between 0 and 1000"
    ],
    "docstrings": [],
    "functions": [],
    "classes": []
}