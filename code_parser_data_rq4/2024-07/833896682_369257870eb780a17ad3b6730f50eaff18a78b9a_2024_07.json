{
    "identifiers": [
        "os",
        "math",
        "nltk",
        "corpus",
        "stopwords",
        "collections",
        "defaultdict",
        "nltk",
        "tokenize",
        "RegexpTokenizer",
        "nltk",
        "stem",
        "porter",
        "PorterStemmer",
        "file_path",
        "tokens",
        "stemmed_docs",
        "tokens",
        "defaultdict",
        "word",
        "word_set",
        "doc",
        "stemmed_docs",
        "word",
        "doc",
        "term2docfreq",
        "word",
        "term2docfreq",
        "tokens",
        "tokens",
        "word",
        "wordset",
        "tokens",
        "count",
        "word",
        "wordfreq",
        "filtered_tokens",
        "word",
        "filtered_tokens",
        "word",
        "stopwords",
        "words",
        "processed_tokens",
        "append",
        "stemmer",
        "stem",
        "word",
        "processed_tokens",
        "file_path",
        "current",
        "filename",
        "os",
        "listdir",
        "file_path",
        "current",
        "filename",
        "open",
        "os",
        "path",
        "join",
        "file_path",
        "filename",
        "file",
        "read",
        "file",
        "close",
        "doc",
        "lower",
        "RegexpTokenizer",
        "sorted",
        "processed_list",
        "tokenizer",
        "tokenize",
        "doc",
        "tokens",
        "token",
        "docs",
        "append",
        "token",
        "tokens",
        "docs",
        "docname2docid",
        "docid2docname",
        "current",
        "PorterStemmer",
        "os",
        "getcwd",
        "load_data",
        "DATA_DIR",
        "defaultdict",
        "idx",
        "token",
        "docs",
        "tf",
        "docs",
        "idx",
        "df",
        "DATA_DIR",
        "tokens",
        "docs",
        "doc_frequency",
        "tokens",
        "N",
        "n",
        "tokens",
        "word",
        "word_set",
        "doc_id",
        "N",
        "word",
        "doc_frequency",
        "math",
        "log",
        "N",
        "doc_frequency",
        "word",
        "current_weight",
        "token2idf",
        "calculate_idf",
        "doc_freq",
        "tokens",
        "N",
        "n",
        "term_frequency",
        "doc_frequency",
        "tokens",
        "N",
        "tokens",
        "N",
        "word",
        "word_set",
        "doc_id",
        "N",
        "word",
        "term_frequency",
        "doc_id",
        "math",
        "log",
        "term_frequency",
        "doc_id",
        "word",
        "math",
        "log",
        "N",
        "doc_frequency",
        "word",
        "summs",
        "doc_id",
        "current_weight",
        "word",
        "token2weight",
        "token2weight",
        "word",
        "current_weight",
        "i",
        "len",
        "summs",
        "math",
        "sqrt",
        "summs",
        "i",
        "token2weight",
        "summs",
        "calculate_tf_idf_weight",
        "doc2tf",
        "doc_freq",
        "tokens",
        "N",
        "n",
        "query",
        "test",
        "query",
        "strip",
        "query",
        "lower",
        "RegexpTokenizer",
        "sorted",
        "processed_list",
        "tokenizer",
        "tokenize",
        "query",
        "tf",
        "tokens",
        "word",
        "query2tf",
        "math",
        "log",
        "query2tf",
        "word",
        "summ",
        "token2qweight",
        "word",
        "test",
        "word",
        "token2qweight",
        "word",
        "math",
        "sqrt",
        "summ",
        "word",
        "token2qweight",
        "token2qweight",
        "word",
        "summ",
        "token2qweight",
        "token2weight",
        "defaultdict",
        "word",
        "token2weight",
        "doc",
        "token2weight",
        "word",
        "token2weight",
        "word",
        "doc",
        "posted_list",
        "word",
        "append",
        "doc",
        "token2weight",
        "word",
        "doc",
        "word",
        "posted_list",
        "sorted",
        "posted_list",
        "word",
        "key",
        "x",
        "x",
        "reverse",
        "posted_list",
        "create_posted_list",
        "token2weight",
        "query",
        "posted_list",
        "N",
        "n",
        "query",
        "strip",
        "query_processed",
        "lower",
        "RegexpTokenizer",
        "sorted",
        "processed_list",
        "tokenizer",
        "tokenize",
        "query_processed",
        "tokens",
        "N",
        "calculate_query_wght",
        "query",
        "test",
        "word",
        "word_set",
        "word",
        "posted_list",
        "doc",
        "wght",
        "posted_list",
        "word",
        "wght",
        "summs",
        "doc",
        "similarity_scores",
        "doc",
        "wght",
        "token2qweight",
        "word",
        "similarity_scores",
        "query",
        "posted_list",
        "N",
        "query",
        "strip",
        "query_processed",
        "lower",
        "RegexpTokenizer",
        "sorted",
        "processed_list",
        "tokenizer",
        "tokenize",
        "query_processed",
        "tokens",
        "N",
        "calculate_query_wght",
        "query",
        "test",
        "word",
        "word_set",
        "word",
        "posted_list",
        "doc",
        "wght",
        "posted_list",
        "word",
        "wght",
        "summs",
        "doc",
        "visited",
        "add",
        "doc",
        "similarity_scores",
        "doc",
        "wght",
        "token2qweight",
        "word",
        "doc",
        "N",
        "doc",
        "visited",
        "wght",
        "summs",
        "doc",
        "similarity_scores",
        "doc",
        "wght",
        "posted_list",
        "word",
        "similarity_scores",
        "query",
        "posted_list",
        "N",
        "n",
        "query",
        "strip",
        "query_processed",
        "lower",
        "RegexpTokenizer",
        "sorted",
        "processed_list",
        "tokenizer",
        "tokenize",
        "query_processed",
        "tokens",
        "N",
        "calculate_query_wght",
        "query",
        "test",
        "word",
        "word_set",
        "word",
        "posted_list",
        "doc",
        "wght",
        "posted_list",
        "word",
        "visited",
        "add",
        "doc",
        "doc",
        "N",
        "doc",
        "visited",
        "wght",
        "summs",
        "doc",
        "similarity_scores",
        "doc",
        "wght",
        "posted_list",
        "word",
        "similarity_scores",
        "scores",
        "docid2docname",
        "max",
        "scores",
        "scores",
        "index",
        "maxi_score",
        "maxi_score",
        "document_num",
        "maxi_score",
        "token2weight",
        "i",
        "word",
        "token2weight",
        "token2weight",
        "word",
        "token2weight",
        "word",
        "i",
        "summs",
        "i",
        "token2weight",
        "normalize_wghts",
        "token2weight",
        "doc_name",
        "token",
        "token",
        "token2normalized_wghts",
        "token2normalized_wghts",
        "token",
        "docname2docid",
        "doc_name",
        "getweight",
        "getweight",
        "getweight",
        "getweight",
        "getweight",
        "token",
        "token",
        "token2idf",
        "token2idf",
        "token",
        "getidf",
        "getidf",
        "getidf",
        "getidf",
        "getidf",
        "getidf",
        "qstring",
        "calc_cosine_similarity_one",
        "qstring",
        "posted_list",
        "N",
        "n",
        "calc_cosine_similarity_two",
        "qstring",
        "posted_list",
        "N",
        "n",
        "calc_cosine_similarity_three",
        "qstring",
        "posted_list",
        "N",
        "n",
        "get_the_best_document",
        "score_one",
        "docid2docname",
        "get_the_best_document",
        "score_two",
        "docid2docname",
        "get_the_best_document",
        "score_three",
        "docid2docname",
        "best_doc_one",
        "best_doc_two",
        "best_doc_three",
        "best_score_one",
        "best_score_two",
        "best_score_one",
        "best_score_three",
        "docid2docname",
        "best_doc_one",
        "best_score_one",
        "best_score_two",
        "best_score_one",
        "best_score_two",
        "best_score_three",
        "docid2docname",
        "best_doc_two",
        "best_score_two",
        "best_score_three",
        "best_score_one",
        "best_score_three",
        "best_score_two",
        "docid2docname",
        "best_doc_three",
        "best_score_three",
        "query",
        "query",
        "query",
        "query"
    ],
    "literals": [
        "'english'",
        "\"r\"",
        "r'[a-z]+[0-9]*'",
        "\"\\data\"",
        "r'[a-z]+[0-9]*'",
        "\"Query weight of\"",
        "\"=\"",
        "r'[a-z]+[0-9]*'",
        "r'[a-z]+[0-9]*'",
        "r'[a-z]+[0-9]*'",
        "\"%.12f\"",
        "\"2012-10-03.txt\"",
        "\"health\"",
        "\"%.12f\"",
        "\"1960-10-21.txt\"",
        "\"reason\"",
        "\"%.12f\"",
        "\"1976-10-22.txt\"",
        "\"agenda\"",
        "\"%.12f\"",
        "\"2012-10-16.txt\"",
        "\"hispan\"",
        "\"%.12f\"",
        "\"2012-10-16.txt\"",
        "\"hispanic\"",
        "\"%.12f\"",
        "\"health\"",
        "\"%.12f\"",
        "\"agenda\"",
        "\"%.12f\"",
        "\"vector\"",
        "\"%.12f\"",
        "\"reason\"",
        "\"%.12f\"",
        "\"hispan\"",
        "\"%.12f\"",
        "\"hispanic\"",
        "\"None\"",
        "\"(%s, %.12f)\"",
        "\"health insurance wall street\"",
        "\"(%s, %.12f)\"",
        "\"particular constitutional amendment\"",
        "\"(%s, %.12f)\"",
        "\"terror attack\"",
        "\"(%s, %.12f)\"",
        "\"vector entropy\""
    ],
    "variables": [
        "word_set",
        "term2docfreq",
        "docs",
        "wordset",
        "wordfreq",
        "wordfreq",
        "word",
        "processed_tokens",
        "docs",
        "tokens",
        "x",
        "docname2docid",
        "docid2docname",
        "docname2docid",
        "filename",
        "docid2docname",
        "current",
        "file",
        "doc",
        "doc",
        "tokenizer",
        "token",
        "stemmer",
        "DATA_DIR",
        "tokens",
        "docs",
        "docname2docid",
        "docid2docname",
        "n",
        "doc2tf",
        "doc2tf",
        "idx",
        "doc_freq",
        "token2idf",
        "word_set",
        "current_weight",
        "current_weight",
        "token2idf",
        "word",
        "token2idf",
        "word",
        "token2idf",
        "token2weight",
        "word_set",
        "summs",
        "current_weight",
        "current_weight",
        "token2weight",
        "word",
        "doc_id",
        "summs",
        "i",
        "token2weight",
        "summs",
        "query",
        "query",
        "tokenizer",
        "tokens",
        "query2tf",
        "token2qweight",
        "summ",
        "token2qweight",
        "word",
        "summ",
        "token2qweight",
        "word",
        "posted_list",
        "posted_list",
        "word",
        "posted_list",
        "query_processed",
        "query_processed",
        "tokenizer",
        "tokens",
        "word_set",
        "query_top10",
        "docs",
        "similarity_scores",
        "token2qweight",
        "wght",
        "query_processed",
        "query_processed",
        "tokenizer",
        "tokens",
        "word_set",
        "query_top10",
        "docs",
        "similarity_scores",
        "token2qweight",
        "visited",
        "wght",
        "wght",
        "query_processed",
        "query_processed",
        "tokenizer",
        "tokens",
        "word_set",
        "query_top10",
        "docs",
        "similarity_scores",
        "token2qweight",
        "visited",
        "wght",
        "maxi_score",
        "document_num",
        "i",
        "token2normalized_wghts",
        "score_one",
        "score_two",
        "score_three",
        "best_doc_one",
        "best_score_one",
        "best_doc_two",
        "best_score_two",
        "best_doc_three",
        "best_score_three"
    ],
    "comments": [
        "coding: utf-8",
        "In[1]:",
        "----Importing required libraries----",
        "In[2]:",
        "----Function returns the document frequency of the tokens----",
        "In[3]:",
        "----Function returns term frequency of the tokens----",
        "In[4]:",
        "----Function processes the tokens to remove stop words from them & stem it----",
        "In[5]:",
        "----Function that loads the documents to be use----",
        "In[6]:",
        "For specifying directory of data",
        "In[7]:",
        "In[8]:",
        "Calling the tf function for tokens in the doc",
        "In[9]:",
        "Calling df function for document frequency",
        "In[10]:",
        "----Function returns the inverse document frequency----",
        "{token{document:weight}}",
        "In[11]:",
        "In[12]:",
        "----Function returns the normalized TF-IDF weight of the tokens----",
        "{token{document:weight}}",
        "In[13]:",
        "In[14]:",
        "----Function returns the normalized weight of the query----",
        "In[15]:",
        "----Function returns the posting list containg top 10 elements in descending order of weights----",
        "In[16]:",
        "Calling postings list by passing the weights",
        "In[17]:",
        "----Function returns d's cosine similarity score if document d appears in the top-10 elements of every query token----",
        "In[18]:",
        "----Function returns d's cosine similarity score if document d doesn't appear in the top-10 elements of some query token t----",
        "In[19]:",
        "----Function returns d's cosine similarity score if document d doesn't appear in the top-10 elements of any query token t----",
        "In[20]:",
        "----Function returns the document with the best similarity socre----",
        "In[21]:",
        "----Function returns the normalized weights for the tokens----",
        "In[22]:",
        "In[23]:",
        "----Function returns the weight for document and token passed----",
        "In[24]:",
        "---Test Case for getweight---",
        "0.008528366190",
        "In[25]:",
        "---Test Case for getweight---",
        "0.000000000000",
        "In[26]:",
        "---Test Case for getweight---",
        "0.012683891289",
        "In[27]:",
        "---Test Case for getweight---",
        "0.023489163449",
        "In[28]:",
        "---Test Case for getweight---",
        "0.000000000000",
        "In[29]:",
        "----Function returns the inverse document frequency for the token passed----",
        "---Test Case for getidf---",
        "0.079181246048",
        "In[30]:",
        "---Test Case for getidf---",
        "0.363177902413",
        "In[31]:",
        "---Test Case for getidf---",
        "-1.000000000000",
        "In[32]:",
        "---Test Case for getidf---",
        "0.000000000000",
        "In[33]:",
        "---Test Case for getidf---",
        "0.632023214705",
        "In[34]:",
        "---Test Case for getidf---",
        "-1.000000000000",
        "In[35]:",
        "----Function returns the document and the weight after computing the similarity according to the 3 cases----",
        "In[36]:",
        "---Test Case for query---",
        "(2012-10-03.txt, 0.033877975254)",
        "In[37]:",
        "---Test Case for query---",
        "(fetch more, 0.000000000000)",
        "In[38]:",
        "---Test Case for query---",
        "(2004-09-30.txt, 0.026893338131)",
        "In[39]:",
        "---Test Case for query---",
        "(None, 0.000000000000)"
    ],
    "docstrings": [],
    "functions": [
        "df",
        "tf",
        "processed_list",
        "load_data",
        "calculate_idf",
        "calculate_tf_idf_weight",
        "calculate_query_wght",
        "create_posted_list",
        "calc_cosine_similarity_one",
        "calc_cosine_similarity_two",
        "calc_cosine_similarity_three",
        "get_the_best_document",
        "normalize_wghts",
        "getweight",
        "getidf",
        "query"
    ],
    "classes": []
}