{
    "identifiers": [
        "argparse",
        "asyncio",
        "copy",
        "json",
        "os",
        "subprocess",
        "typing",
        "fastapi",
        "FastAPI",
        "Request",
        "BackgroundTasks",
        "fastapi",
        "responses",
        "StreamingResponse",
        "JSONResponse",
        "uvicorn",
        "dashinfer",
        "helper",
        "EngineHelper",
        "ConfigManager",
        "fastchat",
        "constants",
        "ErrorCode",
        "SERVER_ERROR_MSG",
        "fastchat",
        "serve",
        "base_model_worker",
        "BaseModelWorker",
        "fastchat",
        "serve",
        "model_worker",
        "logger",
        "worker_id",
        "fastchat",
        "utils",
        "build_logger",
        "get_context_length",
        "is_partial_stop",
        "FastAPI",
        "model_id",
        "revision",
        "os",
        "environ",
        "get",
        "lower",
        "logger",
        "info",
        "model_id",
        "revision",
        "source",
        "source",
        "modelscope",
        "snapshot_download",
        "snapshot_download",
        "model_id",
        "revision",
        "revision",
        "source",
        "huggingface_hub",
        "snapshot_download",
        "snapshot_download",
        "repo_id",
        "model_id",
        "ValueError",
        "logger",
        "info",
        "model_dir",
        "model_dir",
        "BaseModelWorker",
        "controller_addr",
        "worker_addr",
        "worker_id",
        "model_path",
        "model_names",
        "limit_worker_concurrency",
        "revision",
        "no_register",
        "config",
        "json",
        "conv_template",
        "controller_addr",
        "worker_addr",
        "worker_id",
        "model_path",
        "model_names",
        "limit_worker_concurrency",
        "conv_template",
        "logger",
        "info",
        "model_names",
        "worker_id",
        "os",
        "path",
        "exists",
        "model_path",
        "download_model",
        "model_path",
        "revision",
        "EngineHelper",
        "config",
        "engine_helper",
        "init_tokenizer",
        "model_path",
        "engine_helper",
        "convert_model",
        "model_path",
        "engine_helper",
        "init_engine",
        "engine_helper",
        "engine_config",
        "engine_helper",
        "tokenizer",
        "engine_helper",
        "no_register",
        "init_heart_beat",
        "call_ct",
        "pop",
        "get",
        "get",
        "get",
        "get",
        "get",
        "get",
        "get",
        "tokenizer",
        "eos_token_id",
        "stop_token_ids",
        "append",
        "tokenizer",
        "eos_token_id",
        "get",
        "get",
        "get",
        "get",
        "get",
        "get",
        "get",
        "copy",
        "deepcopy",
        "engine_helper",
        "default_gen_cfg",
        "temperature",
        "temperature",
        "top_k",
        "top_k",
        "top_k",
        "dashinfer_style_top_k",
        "top_p",
        "top_p",
        "repetition_penalty",
        "repetition_penalty",
        "presence_penalty",
        "presence_penalty",
        "len",
        "stop_token_ids",
        "id",
        "id",
        "stop_token_ids",
        "logger",
        "info",
        "dashinfer_style_stop_token_ids",
        "dashinfer_style_stop_token_ids",
        "seed",
        "seed",
        "logprobs",
        "logprobs",
        "frequency_penalty",
        "logger",
        "warning",
        "stop",
        "logger",
        "warning",
        "use_beam_search",
        "logger",
        "warning",
        "best_of",
        "logger",
        "warning",
        "logger",
        "info",
        "context",
        "gen_cfg",
        "engine_helper",
        "create_request",
        "context",
        "gen_cfg",
        "gen_cfg",
        "request_list",
        "gen_cfg",
        "max_new_tokens",
        "engine_req",
        "in_tokens_len",
        "max_new_tokens",
        "engine_req",
        "in_tokens_len",
        "max_tokens",
        "max_tokens",
        "engine_req",
        "in_tokens_len",
        "ErrorCode",
        "CONTEXT_OVERFLOW",
        "json",
        "dumps",
        "ret",
        "encode",
        "max_tokens",
        "logger",
        "info",
        "engine_req",
        "engine_helper",
        "process_one_request_stream",
        "engine_req",
        "generate_text",
        "results_generator",
        "echo",
        "context",
        "generate_text",
        "generate_text",
        "engine_req",
        "in_tokens_len",
        "engine_req",
        "out_tokens_len",
        "output_text",
        "prompt_tokens",
        "completion_tokens",
        "prompt_tokens",
        "completion_tokens",
        "json",
        "dumps",
        "ret",
        "encode",
        "e",
        "SERVER_ERROR_MSG",
        "e",
        "ErrorCode",
        "INTERNAL_ERROR",
        "json",
        "dumps",
        "ret",
        "encode",
        "x",
        "generate_stream",
        "json",
        "loads",
        "x",
        "decode",
        "worker",
        "semaphore",
        "release",
        "worker",
        "semaphore",
        "asyncio",
        "Semaphore",
        "worker",
        "limit_worker_concurrency",
        "worker",
        "semaphore",
        "acquire",
        "BackgroundTasks",
        "background_tasks",
        "add_task",
        "release_worker_semaphore",
        "background_tasks",
        "app",
        "post",
        "request",
        "Request",
        "request",
        "json",
        "acquire_worker_semaphore",
        "worker",
        "generate_stream",
        "create_background_tasks",
        "StreamingResponse",
        "generator",
        "background",
        "background_tasks",
        "app",
        "post",
        "request",
        "Request",
        "request",
        "json",
        "acquire_worker_semaphore",
        "worker",
        "generate",
        "release_worker_semaphore",
        "JSONResponse",
        "output",
        "app",
        "post",
        "request",
        "Request",
        "worker",
        "get_status",
        "app",
        "post",
        "request",
        "Request",
        "request",
        "json",
        "worker",
        "count_token",
        "app",
        "post",
        "request",
        "Request",
        "worker",
        "get_conv_template",
        "app",
        "post",
        "request",
        "Request",
        "worker",
        "context_len",
        "argparse",
        "ArgumentParser",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "s",
        "s",
        "split",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "action",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "metavar",
        "help",
        "parser",
        "parse_args",
        "ConfigManager",
        "get_config_from_json",
        "args",
        "config_file",
        "subprocess",
        "run",
        "cmd",
        "stdout",
        "subprocess",
        "PIPE",
        "stderr",
        "subprocess",
        "PIPE",
        "shell",
        "text",
        "package_location",
        "stdout",
        "strip",
        "os",
        "environ",
        "package_location",
        "os",
        "environ",
        "len",
        "config",
        "os",
        "environ",
        "config",
        "DashInferWorker",
        "args",
        "controller_address",
        "args",
        "worker_address",
        "worker_id",
        "args",
        "model_path",
        "args",
        "model_names",
        "args",
        "limit_worker_concurrency",
        "args",
        "revision",
        "args",
        "no_register",
        "config",
        "args",
        "conv_template",
        "uvicorn",
        "run",
        "app",
        "host",
        "args",
        "host",
        "port",
        "args",
        "port",
        "log_level"
    ],
    "literals": [
        "\"huggingface\"",
        "\"FASTCHAT_USE_MODELSCOPE\"",
        "\"False\"",
        "\"true\"",
        "\"modelscope\"",
        "f\"Downloading model {model_id} (revision: {revision}) from {source}\"",
        "\"modelscope\"",
        "\"huggingface\"",
        "\"Unknown source\"",
        "f\"Save model to path {model_dir}\"",
        "f\"Loading the model {self.model_names} on worker {worker_id}, worker type: dash-infer worker...\"",
        "\"engine_max_length\"",
        "\"prompt\"",
        "\"temperature\"",
        "\"top_k\"",
        "\"top_p\"",
        "\"repetition_penalty\"",
        "\"presence_penalty\"",
        "\"max_new_tokens\"",
        "\"stop_token_ids\"",
        "\"seed\"",
        "\"echo\"",
        "\"logprobs\"",
        "\"frequency_penalty\"",
        "\"stop\"",
        "\"use_beam_search\"",
        "\"best_of\"",
        "\"temperature\"",
        "\"top_k\"",
        "\"top_p\"",
        "\"repetition_penalty\"",
        "\"presence_penalty\"",
        "f\"dashinfer_style_stop_token_ids = {dashinfer_style_stop_token_ids}\"",
        "\"stop_words_ids\"",
        "\"seed\"",
        "\"logprobs\"",
        "\"top_logprobs\"",
        "\"dashinfer worker does not support `frequency_penalty` parameter\"",
        "\"dashinfer worker does not support `stop` parameter\"",
        "\"dashinfer worker does not support `use_beam_search` parameter\"",
        "\"dashinfer worker does not support `best_of` parameter\"",
        "f\"dashinfer engine helper creates request with context: {context}, gen_cfg: {gen_cfg}\"",
        "\"max_length\"",
        "\"text\"",
        "f\"This model's maximum generated tokens include context are {max_tokens}, However, your context resulted in {engine_req.in_tokens_len} tokens\"",
        "\"error_code\"",
        "b\"\\0\"",
        "\"max_length\"",
        "f\"dashinfer is going to process one request in stream mode: {engine_req}\"",
        "\"text\"",
        "\"error_code\"",
        "\"usage\"",
        "\"prompt_tokens\"",
        "\"completion_tokens\"",
        "\"total_tokens\"",
        "\"\\0\"",
        "\"text\"",
        "f\"{SERVER_ERROR_MSG}\\n\\n({e})\"",
        "\"error_code\"",
        "b\"\\0\"",
        "\"/worker_generate_stream\"",
        "\"/worker_generate\"",
        "\"/worker_get_status\"",
        "\"/count_token\"",
        "\"/worker_get_conv_template\"",
        "\"/model_details\"",
        "\"context_length\"",
        "\"__main__\"",
        "\"--host\"",
        "\"localhost\"",
        "\"--port\"",
        "\"--worker-address\"",
        "\"http://localhost:21002\"",
        "\"--controller-address\"",
        "\"http://localhost:21001\"",
        "\"--model-path\"",
        "\"qwen/Qwen-7B-Chat\"",
        "\"--model-names\"",
        "\",\"",
        "\"Optional display comma separated names\"",
        "\"--limit-worker-concurrency\"",
        "\"--no-register\"",
        "\"store_true\"",
        "\"--revision\"",
        "\"main\"",
        "\"Hugging Face Hub model revision identifier\"",
        "\"--conv-template\"",
        "\"Conversation prompt template.\"",
        "\"config_file\"",
        "\"config-file\"",
        "\"config_qwen_v10_7b.json\"",
        "\"A model config file which dash-inferread\"",
        "f\"pip show dashinfer | grep 'Location' | cut -d ' ' -f 2\"",
        "\"AS_DAEMON_PATH\"",
        "\"/dashinfer/allspark/bin\"",
        "\"AS_NUMA_NUM\"",
        "\"device_ids\"",
        "\"AS_NUMA_OFFSET\"",
        "\"device_ids\"",
        "\"info\""
    ],
    "variables": [
        "app",
        "source",
        "source",
        "model_dir",
        "model_dir",
        "model_path",
        "engine_helper",
        "context_len",
        "tokenizer",
        "engine_helper",
        "context",
        "temperature",
        "top_k",
        "top_p",
        "repetition_penalty",
        "presence_penalty",
        "max_new_tokens",
        "stop_token_ids",
        "seed",
        "echo",
        "logprobs",
        "frequency_penalty",
        "stop",
        "use_beam_search",
        "best_of",
        "gen_cfg",
        "gen_cfg",
        "dashinfer_style_top_k",
        "gen_cfg",
        "gen_cfg",
        "gen_cfg",
        "gen_cfg",
        "dashinfer_style_stop_token_ids",
        "gen_cfg",
        "gen_cfg",
        "gen_cfg",
        "gen_cfg",
        "request_list",
        "engine_req",
        "max_tokens",
        "ret",
        "gen_cfg",
        "results_generator",
        "output_text",
        "output_text",
        "prompt_tokens",
        "completion_tokens",
        "ret",
        "ret",
        "worker",
        "semaphore",
        "background_tasks",
        "generator",
        "background_tasks",
        "output",
        "parser",
        "args",
        "config",
        "cmd",
        "package_location",
        "package_location",
        "worker"
    ],
    "comments": [
        "check if model_path is existed at local path",
        "not supported parameters",
        "check if prompt tokens exceed the max_tokens"
    ],
    "docstrings": [
        "\"\"\"\nA model worker that executes the model based on dash-infer.\n\nSee documentations at docs/dashinfer_integration.md\n\"\"\""
    ],
    "functions": [
        "download_model",
        "generate_stream",
        "generate",
        "release_worker_semaphore",
        "acquire_worker_semaphore",
        "create_background_tasks",
        "api_generate_stream",
        "api_generate",
        "api_get_status",
        "api_count_token",
        "api_get_conv",
        "api_model_details"
    ],
    "classes": [
        "DashInferWorker"
    ]
}