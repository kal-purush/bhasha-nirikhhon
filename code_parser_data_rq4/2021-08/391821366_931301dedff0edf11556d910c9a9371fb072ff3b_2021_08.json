{
    "identifiers": [
        "math",
        "torch",
        "torch",
        "nn",
        "helper",
        "NestedTensor",
        "nn",
        "Module",
        "num_pos_feats",
        "temperature",
        "normalize",
        "scale",
        "num_pos_feats",
        "temperature",
        "normalize",
        "scale",
        "normalize",
        "ValueError",
        "scale",
        "math",
        "pi",
        "scale",
        "tensor_list",
        "NestedTensor",
        "tensor_list",
        "tensors",
        "tensor_list",
        "mask",
        "mask",
        "mask",
        "not_mask",
        "cumsum",
        "dtype",
        "torch",
        "float32",
        "not_mask",
        "cumsum",
        "dtype",
        "torch",
        "float32",
        "normalize",
        "y_embed",
        "y_embed",
        "eps",
        "scale",
        "x_embed",
        "x_embed",
        "eps",
        "scale",
        "torch",
        "arange",
        "num_pos_feats",
        "dtype",
        "torch",
        "float32",
        "device",
        "x",
        "device",
        "temperature",
        "dim_t",
        "num_pos_feats",
        "x_embed",
        "dim_t",
        "y_embed",
        "dim_t",
        "torch",
        "stack",
        "pos_x",
        "sin",
        "pos_x",
        "cos",
        "dim",
        "flatten",
        "torch",
        "stack",
        "pos_y",
        "sin",
        "pos_y",
        "cos",
        "dim",
        "flatten",
        "torch",
        "cat",
        "pos_y",
        "pos_x",
        "dim",
        "permute",
        "pos",
        "nn",
        "Module",
        "num_pos_feats",
        "nn",
        "Embedding",
        "num_pos_feats",
        "nn",
        "Embedding",
        "num_pos_feats",
        "reset_parameters",
        "nn",
        "init",
        "uniform_",
        "row_embed",
        "weight",
        "nn",
        "init",
        "uniform_",
        "col_embed",
        "weight",
        "tensor_list",
        "NestedTensor",
        "tensor_list",
        "tensors",
        "x",
        "shape",
        "torch",
        "arange",
        "w",
        "device",
        "x",
        "device",
        "torch",
        "arange",
        "h",
        "device",
        "x",
        "device",
        "col_embed",
        "i",
        "row_embed",
        "j",
        "torch",
        "cat",
        "x_emb",
        "unsqueeze",
        "repeat",
        "h",
        "y_emb",
        "unsqueeze",
        "repeat",
        "w",
        "dim",
        "permute",
        "unsqueeze",
        "repeat",
        "x",
        "shape",
        "pos",
        "args",
        "args",
        "hidden_dim",
        "args",
        "position_embedding",
        "PositionEmbeddingSine",
        "N_steps",
        "normalize",
        "args",
        "position_embedding",
        "PositionEmbeddingLearned",
        "N_steps",
        "ValueError",
        "args",
        "position_embedding",
        "position_embedding"
    ],
    "literals": [
        "\"normalize should be True if scale is passed\"",
        "'v2'",
        "'sine'",
        "'v3'",
        "'learned'",
        "f\"not supported {args.position_embedding}\""
    ],
    "variables": [
        "num_pos_feats",
        "temperature",
        "normalize",
        "scale",
        "scale",
        "x",
        "mask",
        "not_mask",
        "y_embed",
        "x_embed",
        "eps",
        "y_embed",
        "x_embed",
        "dim_t",
        "dim_t",
        "pos_x",
        "pos_y",
        "pos_x",
        "pos_y",
        "pos",
        "row_embed",
        "col_embed",
        "x",
        "h",
        "w",
        "i",
        "j",
        "x_emb",
        "y_emb",
        "pos",
        "N_steps",
        "position_embedding",
        "position_embedding"
    ],
    "comments": [
        "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
        "TODO find a better way of exposing other arguments"
    ],
    "docstrings": [
        "\"\"\"\nVarious positional encodings for the transformer.\n\"\"\"",
        "\"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"",
        "\"\"\"\n    Absolute pos embedding, learned.\n    \"\"\""
    ],
    "functions": [
        "forward",
        "reset_parameters",
        "forward",
        "build_position_encoding"
    ],
    "classes": [
        "PositionEmbeddingSine",
        "PositionEmbeddingLearned"
    ]
}