{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "torch",
        "nn",
        "CrossEntropyLoss",
        "logging",
        "bert_model",
        "BertPreTrainedModel",
        "BertPreTrainingHeads",
        "BertModel",
        "BertEncoder",
        "BertPooler",
        "BertLayerNorm",
        "logging",
        "getLogger",
        "nn",
        "Module",
        "config",
        "DualPositionBertEmbeddings",
        "nn",
        "Embedding",
        "config",
        "vocab_size",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "max_position_embeddings",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "max_position_embeddings",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "type_vocab_size",
        "config",
        "hidden_size",
        "BertLayerNorm",
        "config",
        "hidden_size",
        "eps",
        "nn",
        "Dropout",
        "config",
        "hidden_dropout_prob",
        "input_ids",
        "token_type_ids",
        "position_ids",
        "position_ids_second",
        "token_type_ids",
        "torch",
        "zeros_like",
        "input_ids",
        "word_embeddings",
        "input_ids",
        "position_embeddings",
        "position_ids",
        "position_embeddings",
        "position_ids_second",
        "token_type_embeddings",
        "token_type_ids",
        "words_embeddings",
        "position_embeddings",
        "position_embeddings_second",
        "token_type_embeddings",
        "LayerNorm",
        "embeddings",
        "dropout",
        "embeddings",
        "embeddings",
        "BertModel",
        "config",
        "DualPositionBertModel",
        "config",
        "DualPositionBertEmbeddings",
        "config",
        "BertEncoder",
        "config",
        "BertPooler",
        "config",
        "apply",
        "init_bert_weights",
        "logger",
        "info",
        "input_ids",
        "token_type_ids",
        "attention_mask",
        "output_all_encoded_layers",
        "checkpoint_activations",
        "position_ids",
        "position_ids_second",
        "attention_mask",
        "torch",
        "ones_like",
        "input_ids",
        "token_type_ids",
        "torch",
        "zeros_like",
        "input_ids",
        "attention_mask",
        "unsqueeze",
        "unsqueeze",
        "extended_attention_mask",
        "to",
        "dtype",
        "next",
        "parameters",
        "dtype",
        "extended_attention_mask",
        "embeddings",
        "input_ids",
        "token_type_ids",
        "position_ids",
        "position_ids_second",
        "encoder",
        "embedding_output",
        "extended_attention_mask",
        "output_all_encoded_layers",
        "output_all_encoded_layers",
        "checkpoint_activations",
        "checkpoint_activations",
        "encoded_layers",
        "pooler",
        "sequence_output",
        "output_all_encoded_layers",
        "encoded_layers",
        "encoded_layers",
        "pooled_output",
        "BertPreTrainedModel",
        "config",
        "DualPositionBertForPreTrainingPreLN",
        "config",
        "DualPositionBertModel",
        "config",
        "BertPreTrainingHeads",
        "config",
        "bert",
        "embeddings",
        "word_embeddings",
        "weight",
        "apply",
        "init_bert_weights",
        "input_ids",
        "token_type_ids",
        "attention_mask",
        "masked_lm_labels",
        "position_ids",
        "position_ids_second",
        "log",
        "bert",
        "input_ids",
        "input_ids",
        "token_type_ids",
        "token_type_ids",
        "attention_mask",
        "attention_mask",
        "output_all_encoded_layers",
        "checkpoint_activations",
        "position_ids",
        "position_ids",
        "position_ids_second",
        "position_ids_second",
        "masked_lm_labels",
        "torch",
        "nonzero",
        "masked_lm_labels",
        "view",
        "view",
        "cls",
        "sequence_output",
        "pooled_output",
        "masked_token_indexes",
        "torch",
        "index_select",
        "masked_lm_labels",
        "view",
        "masked_token_indexes",
        "CrossEntropyLoss",
        "ignore_index",
        "loss_fct",
        "prediction_scores",
        "view",
        "config",
        "vocab_size",
        "target",
        "masked_lm_loss",
        "cls",
        "sequence_output",
        "pooled_output",
        "prediction_scores"
    ],
    "literals": [
        "\"Init BERT pretrain model\""
    ],
    "variables": [
        "logger",
        "word_embeddings",
        "position_embeddings",
        "position_embeddings_second",
        "token_type_embeddings",
        "LayerNorm",
        "dropout",
        "token_type_ids",
        "words_embeddings",
        "position_embeddings",
        "position_embeddings_second",
        "token_type_embeddings",
        "embeddings",
        "embeddings",
        "embeddings",
        "embeddings",
        "encoder",
        "pooler",
        "attention_mask",
        "token_type_ids",
        "extended_attention_mask",
        "extended_attention_mask",
        "extended_attention_mask",
        "embedding_output",
        "encoded_layers",
        "sequence_output",
        "pooled_output",
        "encoded_layers",
        "bert",
        "cls",
        "sequence_output",
        "pooled_output",
        "masked_token_indexes",
        "prediction_scores",
        "_",
        "target",
        "loss_fct",
        "masked_lm_loss",
        "prediction_scores",
        "_"
    ],
    "comments": [
        "self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load",
        "any TensorFlow checkpoint file",
        "fp16 compatibility",
        "filter out all masked labels."
    ],
    "docstrings": [
        "\"\"\"Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"",
        "\"\"\"BERT model with pre-training heads and dual position\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    \"\"\""
    ],
    "functions": [
        "forward",
        "forward",
        "forward"
    ],
    "classes": [
        "DualPositionBertEmbeddings",
        "DualPositionBertModel",
        "DualPositionBertForPreTrainingPreLN"
    ]
}