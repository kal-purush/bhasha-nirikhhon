{
    "identifiers": [
        "math",
        "log",
        "collections",
        "collections",
        "Counter",
        "itertools",
        "numpy",
        "np",
        "np",
        "exp",
        "special_token",
        "isinstance",
        "special_token",
        "special_token",
        "corpus",
        "n",
        "isinstance",
        "n",
        "n",
        "Counter",
        "tokens",
        "corpus",
        "isinstance",
        "tokens",
        "mle_count",
        "update",
        "_get_ngram_iter",
        "tokens",
        "n",
        "n",
        "mle_count",
        "len",
        "mle_count",
        "ValueError",
        "mle_count",
        "tokens",
        "n",
        "_special_token",
        "n",
        "n",
        "token",
        "tokens",
        "isinstance",
        "token",
        "basestring",
        "len",
        "token",
        "ngram",
        "append",
        "token",
        "len",
        "ngram",
        "n",
        "ngram",
        "ngram",
        "pop",
        "_special_token",
        "ngram",
        "append",
        "ngram",
        "corpus",
        "n0",
        "isinstance",
        "corpus",
        "isinstance",
        "n0",
        "n0",
        "tokens",
        "normalize",
        "isinstance",
        "tokens",
        "isinstance",
        "normalize",
        "isinstance",
        "_BaseNGram",
        "corpus",
        "n0",
        "_validate_fit_input",
        "corpus",
        "n0",
        "_get_mle_count",
        "corpus",
        "n",
        "sum",
        "mle_count",
        "values",
        "Counter",
        "mle_count",
        "values",
        "n0",
        "sorted",
        "n_mle_count",
        "keys",
        "r",
        "len",
        "counts",
        "counts",
        "r",
        "counts",
        "r",
        "n0",
        "counts",
        "r",
        "n_mle_count",
        "counts",
        "r",
        "n_mle_count",
        "counts",
        "r",
        "counts",
        "counts",
        "k",
        "v",
        "corpus_length",
        "k",
        "v",
        "gt_count",
        "iteritems",
        "reduce",
        "x",
        "y",
        "x",
        "y",
        "r",
        "n_mle_count",
        "r",
        "gt_prob",
        "r",
        "gt_prob",
        "keys",
        "k",
        "v",
        "prob_mass",
        "k",
        "v",
        "gt_prob",
        "iteritems",
        "gt_prob",
        "mle_count",
        "n_mle_count",
        "tokens",
        "normalize",
        "e",
        "_validate_predict_input",
        "tokens",
        "normalize",
        "unigram",
        "unigram",
        "_get_ngram_iter",
        "tokens",
        "len",
        "unigram_list",
        "ValueError",
        "logprob",
        "u",
        "u",
        "unigram_list",
        "reduce",
        "x",
        "y",
        "x",
        "y",
        "scores",
        "normalize",
        "score",
        "len",
        "unigram_list",
        "score",
        "ngram",
        "e",
        "mle_count_",
        "gt_prob_",
        "ngram",
        "mle_count",
        "log",
        "gt_prob",
        "mle_count",
        "ngram",
        "log",
        "gt_prob",
        "score",
        "_BaseNGram",
        "corpus",
        "n0",
        "_validate_fit_input",
        "corpus",
        "n0",
        "itertools",
        "tee",
        "corpus",
        "_get_mle_count",
        "corpus_iter_bi",
        "n",
        "Unigram",
        "_special_token",
        "unigram",
        "fit",
        "corpus_iter_un",
        "n0",
        "mle_count",
        "unigram",
        "tokens",
        "alpha",
        "normalize",
        "e",
        "_validate_predict_input",
        "tokens",
        "normalize",
        "isinstance",
        "alpha",
        "alpha",
        "alpha",
        "bigram",
        "bigram",
        "_get_ngram_iter",
        "tokens",
        "len",
        "bigram_list",
        "ValueError",
        "mle_count_",
        "unigram_",
        "mle_count_",
        "unigram_",
        "log",
        "bigram_mle_count",
        "x",
        "unigram_mle_count",
        "x",
        "x",
        "bigram_mle_count",
        "log",
        "alpha",
        "unigram",
        "logprob",
        "x",
        "x",
        "bigram_list",
        "reduce",
        "a",
        "b",
        "a",
        "b",
        "scores",
        "normalize",
        "score",
        "len",
        "bigram_list",
        "score"
    ],
    "literals": [
        "\"special token must be bool.\"",
        "\"n must be integer >= 1.\"",
        "\"element of corpus must be Iterable: %s\"",
        "\"corpus must be non-empty.\"",
        "\"<s>\"",
        "\"<s>\"",
        "\"token must be non-empty str.\"",
        "\"</s>\"",
        "\"corpus must be Iterable.\"",
        "\"out-of-vocabulary size must be an integer >= 0.\"",
        "\"tokens must be Iterable.\"",
        "\"normalize must be bool.\"",
        "\"base must be float >= 0.\"",
        "\"tokens must be non-empty.\"",
        "\"alpha must be float >= 0 and <= 1.\"",
        "\"tokens must be non-empty.\""
    ],
    "variables": [
        "e",
        "_special_token",
        "mle_count",
        "mle_count",
        "ngram",
        "ngram",
        "mle_count",
        "corpus_length",
        "n_mle_count",
        "gt_count",
        "n_mle_count",
        "counts",
        "gt_count",
        "gt_count",
        "gt_prob",
        "prob_mass",
        "gt_prob",
        "gt_prob_",
        "mle_count_",
        "n_mle_count_",
        "unigram_list",
        "scores",
        "score",
        "score",
        "mle_count",
        "gt_prob",
        "score",
        "score",
        "corpus_iter_bi",
        "corpus_iter_un",
        "mle_count",
        "unigram",
        "mle_count_",
        "unigram_",
        "bigram_list",
        "bigram_mle_count",
        "unigram_mle_count",
        "unigram",
        "scores",
        "score",
        "score"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\"Base class for NGram models\n\n    Parameters\n    ----------\n    special_token : bool, optional, default False\n        If True, special token `<S>` and `</S>` will be appended to the \n        begining and end of document.\"\"\"",
        "\"\"\"Compute the Maximum-likelihodd estimate of ngram count \n\n        Parameters\n        ----------\n        corpus : Iterable\n            Training corpus. Element of the corpus corresponds to document (\n            Iterable of str)\n\n        n : int, optional, default 1\n            Size of the token sequence. For example, n = 1 for unigram\n\n        Returns\n        -------\n        mle_count : dict\n            Contains the unique ngrams as keys, counts as values.\"\"\"",
        "\"\"\"Convert sequence to tokens into ngrams\n\n        Example: [\"a\", \"b\", \"c\"] => [(\"a\", \"b\"), (\"b\", \"c\")] for bigram\n\n        Parameters\n        ----------\n        tokens : Iterable\n            A document represented as sequence of str\n        \n        n : int, optional, default 1\n            Size of the token sequence. For example, n = 1 for unigram\n\n        Returns\n        -------\n        Generator returns one ngram at a time. Each ngram is represented as a\n        tuple of str.\"\"\"",
        "\"\"\"Validate input of fit function\"\"\"",
        "\"\"\"Validate input of predict function\"\"\"",
        "\"\"\"Unigram model\"\"\"",
        "\"\"\"Fit Unigram model from training corpus\n\n        Parameters\n        ----------\n        corpus : Iterable\n            Training corpus. Element of the corpus corresponds to document (\n            Iterable of str)\n\n        n0 : int, optional, default 0\n            The number of unique out-of-vocabulary tokens (i.e. N0)\"\"\"",
        "\"\"\"Predict the log-transformed probability of a list of tokens\n        \n        Parameters\n        ----------\n        tokens : Iterable\n            Sequence of tokens to be evaluated\n\n        normalize : bool, optional, default True\n            If True, divide the sum of log-probabilities by the length of token\n                \n        base : float, optional, default 2.\n            Base of logarithm\n\n        Returns\n        -------\n        score : float\n            Probability score of input sequence of tokens\"\"\"",
        "\"\"\"Compute log-transformed probability of unigram\n\n        Parameters\n        ----------\n        ngram : tuple\n            Input ngram (tuple of str)\n\n        base : float, optional, default 2.\n            Base of logarithm\n\n        Returns\n        -------\n        score : float\n            Probaility score of input unigram\"\"\"",
        "\"\"\"Bigram language model\"\"\"",
        "\"\"\"Fit Bigram model from training corpus\n\n        Parameters\n        ----------\n        corpus : Iterable\n            Training corpus. Element of the corpus corresponds to document (\n            Iterable of str)\n\n        n0 : int, optional, default 0\n            The number of unique out-of-vocabulary tokens (i.e. N0)\"\"\"",
        "\"\"\"Predict the log-transformed probability of a list of tokens\n        \n        Parameters\n        ----------\n        tokens : Iterable\n            Sequence of tokens to be evaluated\n\n        alpha : float\n            Parameter of the Stupid backoff method\n\n        normalize : bool, optional, default True\n            If True, divide the sum of log-probabilities by the length of token\n                \n        base : float, optional, default 2.\n            Base of logarithm\n\n        Returns\n        -------\n        score : float\n            Probability score of input sequence of tokens\"\"\""
    ],
    "functions": [
        "_get_mle_count",
        "_get_ngram_iter",
        "_validate_fit_input",
        "_validate_predict_input",
        "fit",
        "predict",
        "logprob",
        "fit",
        "predict"
    ],
    "classes": [
        "_BaseNGram",
        "Unigram",
        "Bigram"
    ]
}