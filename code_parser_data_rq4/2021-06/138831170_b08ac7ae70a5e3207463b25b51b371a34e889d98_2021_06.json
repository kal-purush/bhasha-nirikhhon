{
    "identifiers": [
        "logging",
        "math",
        "os",
        "torch",
        "mmf",
        "common",
        "registry",
        "registry",
        "mmf",
        "models",
        "BaseModel",
        "mmf",
        "modules",
        "embeddings",
        "BertVisioLinguisticEmbeddings",
        "mmf",
        "utils",
        "configuration",
        "get_mmf_cache_dir",
        "mmf",
        "utils",
        "file_io",
        "PathManager",
        "mmf",
        "utils",
        "modeling",
        "get_optimizer_parameters_for_bert",
        "mmf",
        "utils",
        "transform",
        "transform_to_batch_sequence",
        "transform_to_batch_sequence_dim",
        "omegaconf",
        "OmegaConf",
        "torch",
        "nn",
        "transformers",
        "modeling_bert",
        "BertConfig",
        "BertEncoder",
        "BertLayer",
        "BertModel",
        "BertPooler",
        "BertPredictionHeadTransform",
        "BertPreTrainedModel",
        "logging",
        "getLogger",
        "registry",
        "register_model",
        "BaseModel",
        "config",
        "config",
        "build",
        "classmethod",
        "cls",
        "config",
        "visual_bert",
        "hidden_size",
        "config",
        "graph_module",
        "node_hid_dim",
        "config",
        "feed_vb_to_graph",
        "config",
        "feed_q_to_graph",
        "config",
        "feed_mode",
        "config",
        "feed_graph_to_vb",
        "config",
        "feed_special_node",
        "config",
        "topk_ans_feed",
        "config",
        "compress_crossmodel",
        "config",
        "crossmodel_compress_dim",
        "config",
        "analysis_mode",
        "config",
        "noback_vb_to_graph",
        "config",
        "feed_q_to_graph",
        "BertModel",
        "from_pretrained",
        "q_enc",
        "config",
        "hidden_size",
        "projects",
        "krisp",
        "graphnetwork_module",
        "GraphNetworkModule",
        "GraphNetworkModule",
        "config",
        "graph_module",
        "extra_config",
        "VisualBERTModule",
        "config",
        "visual_bert",
        "extra_config",
        "nn",
        "Linear",
        "vb_module",
        "model",
        "bert",
        "config",
        "hidden_size",
        "config",
        "num_labels",
        "config",
        "graph_logit_mode",
        "GraphPtrNet",
        "vb_module",
        "model",
        "bert",
        "config",
        "hidden_size",
        "config",
        "graph_module",
        "node_hid_dim",
        "config",
        "graph_logit_mode",
        "config",
        "graph_logit_mode",
        "nn",
        "Linear",
        "config",
        "graph_module",
        "node_hid_dim",
        "config",
        "num_labels",
        "config",
        "output_combine",
        "torch",
        "LongTensor",
        "config",
        "num_labels",
        "fill_",
        "missing_ans_inds",
        "graph_module",
        "index_in_ans",
        "sample_list",
        "config",
        "feed_graph_to_vb",
        "config",
        "feed_vb_to_graph",
        "config",
        "feed_mode",
        "config",
        "feed_mode",
        "graph_module",
        "gn",
        "output_special_node",
        "config",
        "feed_mode",
        "graph_module",
        "sample_list",
        "graph_output",
        "vb_module",
        "sample_list",
        "vocab_fc",
        "vb_hidden",
        "config",
        "feed_vb_to_graph",
        "config",
        "feed_mode",
        "vb_module",
        "sample_list",
        "vocab_fc",
        "vb_hidden",
        "vb_hidden",
        "vb_logits",
        "config",
        "feed_q_to_graph",
        "sample_list",
        "q_enc",
        "input_ids",
        "sample_list",
        "attention_mask",
        "attention_mask_q",
        "token_type_ids",
        "sample_list",
        "q_enc_out",
        "graph_module",
        "sample_list",
        "config",
        "graph_logit_mode",
        "config",
        "noback_vb_to_blinear",
        "graph_ptr_net",
        "vb_hidden",
        "detach",
        "graph_output",
        "graph_ptr_net",
        "vb_hidden",
        "graph_output",
        "config",
        "graph_logit_mode",
        "graph_output",
        "config",
        "graph_module",
        "output_type",
        "config",
        "graph_logit_mode",
        "graph_logit_fc",
        "graph_output",
        "config",
        "output_combine",
        "config",
        "graph_module",
        "output_order",
        "torch",
        "cat",
        "vb_logits",
        "graph_logits",
        "dim",
        "config",
        "output_combine",
        "config",
        "graph_module",
        "output_order",
        "graph_logits",
        "size",
        "vb_logits",
        "size",
        "missing_ans_inds",
        "vb_logits",
        "graph_logits",
        "config",
        "zerobias",
        "logits",
        "logits",
        "config",
        "analysis_mode",
        "graph_module",
        "add_analysis_to_output",
        "output",
        "output",
        "nn",
        "Module",
        "hidden_size",
        "graph_hidden_size",
        "hidden_size",
        "graph_hidden_size",
        "nn",
        "Linear",
        "hidden_size",
        "hidden_size",
        "nn",
        "Linear",
        "graph_hidden_size",
        "hidden_size",
        "bl_hidden",
        "graph_hidden",
        "bl_w",
        "bl_hidden",
        "bl_hidden",
        "dim",
        "bl_hidden",
        "unsqueeze",
        "graph_w",
        "graph_hidden",
        "torch",
        "matmul",
        "bl_hidden",
        "graph_hidden",
        "transpose",
        "scores",
        "math",
        "sqrt",
        "hidden_size",
        "scores",
        "squeeze",
        "scores",
        "BertPreTrainedModel",
        "config",
        "visual_embedding_dim",
        "embedding_strategy",
        "bypass_transformer",
        "output_attentions",
        "output_hidden_states",
        "config",
        "config",
        "visual_embedding_dim",
        "embedding_strategy",
        "bypass_transformer",
        "output_attentions",
        "output_hidden_states",
        "BertVisioLinguisticEmbeddings",
        "config",
        "BertEncoder",
        "config",
        "BertPooler",
        "config",
        "config",
        "bypass_transformer",
        "bypass_transformer",
        "BertLayer",
        "config",
        "config",
        "output_attentions",
        "config",
        "output_hidden_states",
        "_",
        "len",
        "encoder",
        "layer",
        "init_weights",
        "input_ids",
        "attention_mask",
        "token_type_ids",
        "visual_embeddings",
        "position_embeddings_visual",
        "visual_embeddings_type",
        "image_text_alignment",
        "graph_input",
        "attention_mask",
        "torch",
        "ones_like",
        "input_ids",
        "token_type_ids",
        "torch",
        "zeros_like",
        "input_ids",
        "attention_mask",
        "unsqueeze",
        "unsqueeze",
        "extended_attention_mask",
        "to",
        "dtype",
        "next",
        "parameters",
        "dtype",
        "extended_attention_mask",
        "position_embeddings_visual",
        "embeddings",
        "input_ids",
        "token_type_ids",
        "visual_embeddings",
        "visual_embeddings",
        "visual_embeddings_type",
        "visual_embeddings_type",
        "image_text_alignment",
        "image_text_alignment",
        "bypass_transformer",
        "visual_embeddings",
        "output_hidden_states",
        "input_ids",
        "size",
        "embedding_output",
        "text_length",
        "embedding_output",
        "text_length",
        "extended_attention_mask",
        "text_length",
        "text_length",
        "encoder",
        "text_embedding_output",
        "text_extended_attention_mask",
        "fixed_head_masks",
        "encoded_layers",
        "torch",
        "cat",
        "sequence_output",
        "visual_part",
        "dim",
        "additional_layer",
        "new_input",
        "extended_attention_mask",
        "pooler",
        "final_sequence_output",
        "final_sequence_output",
        "pooled_output",
        "graph_input",
        "torch",
        "cat",
        "embedding_output",
        "graph_input",
        "dim",
        "torch",
        "zeros",
        "graph_input",
        "size",
        "graph_input",
        "size",
        "to",
        "extended_attention_mask",
        "device",
        "torch",
        "cat",
        "extended_attention_mask",
        "graph_att_mask",
        "dim",
        "encoder",
        "embedding_output",
        "extended_attention_mask",
        "fixed_head_masks",
        "encoded_layers",
        "pooler",
        "sequence_output",
        "output_attentions",
        "encoded_layers",
        "sequence_output",
        "pooled_output",
        "attn_data_list",
        "nn",
        "Module",
        "config",
        "extra_config",
        "config",
        "config",
        "output_attentions",
        "config",
        "output_hidden_states",
        "config",
        "get",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "feed_graph_to_vb",
        "nn",
        "Sequential",
        "nn",
        "Linear",
        "graph_node_hid_dim",
        "config",
        "hidden_size",
        "nn",
        "LayerNorm",
        "config",
        "hidden_size",
        "eps",
        "nn",
        "Dropout",
        "config",
        "hidden_dropout_prob",
        "getattr",
        "config",
        "BertConfig",
        "from_dict",
        "OmegaConf",
        "to_container",
        "config",
        "resolve",
        "bert_model_name",
        "bert_model_name",
        "VisualBERTBase",
        "bert_config",
        "visual_embedding_dim",
        "config",
        "visual_embedding_dim",
        "embedding_strategy",
        "config",
        "embedding_strategy",
        "bypass_transformer",
        "config",
        "bypass_transformer",
        "output_attentions",
        "config",
        "output_attentions",
        "output_hidden_states",
        "config",
        "output_hidden_states",
        "VisualBERTBase",
        "from_pretrained",
        "config",
        "bert_model_name",
        "config",
        "bert_config",
        "cache_dir",
        "os",
        "path",
        "join",
        "get_mmf_cache_dir",
        "format",
        "visual_embedding_dim",
        "config",
        "visual_embedding_dim",
        "embedding_strategy",
        "config",
        "embedding_strategy",
        "bypass_transformer",
        "config",
        "bypass_transformer",
        "output_attentions",
        "config",
        "output_attentions",
        "output_hidden_states",
        "config",
        "output_hidden_states",
        "config",
        "training_head_type",
        "nn",
        "Dropout",
        "bert",
        "config",
        "hidden_dropout_prob",
        "config",
        "training_head_type",
        "bert",
        "config",
        "hidden_size",
        "nn",
        "Sequential",
        "BertPredictionHeadTransform",
        "bert",
        "config",
        "init_weights",
        "config",
        "random_initialize",
        "bert_model_name",
        "bert",
        "init_weights",
        "classifier",
        "apply",
        "bert",
        "_init_weights",
        "input_ids",
        "input_mask",
        "attention_mask",
        "token_type_ids",
        "visual_embeddings",
        "position_embeddings_visual",
        "visual_embeddings_type",
        "image_text_alignment",
        "masked_lm_labels",
        "graph_input",
        "feed_graph_to_vb",
        "graph_feed_mode",
        "graph_input",
        "dim",
        "graph_input",
        "size",
        "input_ids",
        "size",
        "graph_input",
        "size",
        "graph_node_hid_dim",
        "graph_input",
        "unsqueeze",
        "graph_feed_mode",
        "graph_input",
        "dim",
        "graph_input",
        "size",
        "input_ids",
        "size",
        "graph_input",
        "size",
        "graph_topk",
        "graph_input",
        "size",
        "graph_node_hid_dim",
        "graph_embedding",
        "graph_input",
        "bert",
        "input_ids",
        "attention_mask",
        "token_type_ids",
        "visual_embeddings",
        "position_embeddings_visual",
        "visual_embeddings_type",
        "image_text_alignment",
        "graph_input",
        "training_head_type",
        "pooled_output",
        "size",
        "torch",
        "cat",
        "pooled_output",
        "b",
        "pooled_output",
        "b",
        "dim",
        "output_attentions",
        "attention_weights",
        "output_hidden_states",
        "sequence_output",
        "pooled_output",
        "pooler_strategy",
        "input_mask",
        "sum",
        "torch",
        "gather",
        "sequence_output",
        "index_to_gather",
        "unsqueeze",
        "unsqueeze",
        "expand",
        "index_to_gather",
        "size",
        "sequence_output",
        "size",
        "dropout",
        "pooled_output",
        "classifier",
        "pooled_output",
        "squeeze",
        "output",
        "nn",
        "Module",
        "config",
        "extra_config",
        "config",
        "extra_config",
        "extra_config",
        "build",
        "config",
        "training_head_type",
        "VisualBERTForClassification",
        "config",
        "extra_config",
        "config",
        "special_visual_initialize",
        "model",
        "bert",
        "embeddings",
        "initialize_visual_from_pretrained",
        "config",
        "load_from_pretrained",
        "config",
        "pretrained_file",
        "PathManager",
        "open",
        "pretrained_file",
        "f",
        "torch",
        "load",
        "f",
        "map_location",
        "storage",
        "loc",
        "storage",
        "ckpt",
        "key",
        "model_ckpt",
        "key",
        "key",
        "split",
        "model_ckpt",
        "key",
        "model_ckpt_new",
        "model",
        "load_state_dict",
        "model_ckpt",
        "strict",
        "len",
        "incompatible_keys",
        "missing_keys",
        "logger",
        "warning",
        "incompatible_keys",
        "missing_keys",
        "incompatible_keys",
        "unexpected_keys",
        "len",
        "incompatible_keys",
        "unexpected_keys",
        "logger",
        "warning",
        "incompatible_keys",
        "unexpected_keys",
        "getattr",
        "config",
        "p",
        "model",
        "bert",
        "parameters",
        "extra_config",
        "extra_config",
        "extra_config",
        "feed_graph_to_vb",
        "extra_config",
        "sample_list",
        "to_be_flattened",
        "to_be_flattened_dim",
        "to_be_flattened",
        "to_be_flattened_dim",
        "key",
        "to_be_flattened",
        "getattr",
        "sample_list",
        "key",
        "transform_to_batch_sequence",
        "sample_list",
        "key",
        "key",
        "to_be_flattened_dim",
        "getattr",
        "sample_list",
        "key",
        "transform_to_batch_sequence_dim",
        "sample_list",
        "key",
        "sample_list",
        "visual_embeddings_type",
        "sample_list",
        "image_mask",
        "torch",
        "zeros_like",
        "sample_list",
        "image_mask",
        "sample_list",
        "image_mask",
        "torch",
        "cat",
        "sample_list",
        "input_mask",
        "sample_list",
        "image_mask",
        "dim",
        "sample_list",
        "masked_lm_labels",
        "sample_list",
        "masked_lm_labels",
        "size",
        "sample_list",
        "input_mask",
        "size",
        "torch",
        "ones_like",
        "attention_mask",
        "sample_list",
        "masked_lm_labels",
        "size",
        "len",
        "size_masked_lm_labels",
        "size_masked_lm_labels",
        "size_masked_lm_labels",
        "sample_list",
        "masked_lm_labels",
        "new_lm_labels",
        "sample_list",
        "input_mask",
        "attention_mask",
        "sample_list",
        "config",
        "get_optimizer_parameters_for_bert",
        "model",
        "config",
        "sample_list",
        "kwargs",
        "flatten",
        "sample_list",
        "to_be_flattened",
        "to_be_flattened_dim",
        "flattened",
        "sample_list",
        "sample_list",
        "input_ids",
        "sample_list",
        "input_mask",
        "sample_list",
        "segment_ids",
        "config",
        "training_head_type",
        "torch",
        "cat",
        "bert_input_ids",
        "bert_input_ids",
        "torch",
        "cat",
        "bert_input_mask",
        "bert_input_mask",
        "torch",
        "cat",
        "bert_input_type_ids",
        "bert_input_type_ids",
        "getattr",
        "sample_list",
        "getattr",
        "img0",
        "getattr",
        "image_info",
        "getattr",
        "img0",
        "getattr",
        "sample_list",
        "getattr",
        "img1",
        "getattr",
        "image_info",
        "getattr",
        "img1",
        "torch",
        "cat",
        "image_feat_variable_0",
        "image_feat_variable_1",
        "torch",
        "cat",
        "image_dim_variable_0",
        "image_dim_variable_1",
        "getattr",
        "sample_list",
        "getattr",
        "image_info",
        "getattr",
        "sample_list",
        "image_feat_variable",
        "image_dim_variable",
        "bert_input_ids",
        "bert_input_mask",
        "bert_input_type_ids",
        "sample_list",
        "sample_list",
        "getattr",
        "sample_list",
        "getattr",
        "sample_list",
        "getattr",
        "sample_list",
        "visual_embeddings",
        "image_dim",
        "torch",
        "arange",
        "visual_embeddings",
        "size",
        "device",
        "visual_embeddings",
        "device",
        "expand",
        "visual_embeddings",
        "size",
        "len",
        "image_dim",
        "size",
        "len",
        "image_mask",
        "size",
        "image_dim",
        "unsqueeze",
        "len",
        "image_dim",
        "size",
        "len",
        "image_mask",
        "size",
        "image_mask",
        "image_dim",
        "image_mask",
        "sample_list",
        "classmethod",
        "cls",
        "key",
        "key",
        "replace",
        "replace",
        "replace",
        "sample_list",
        "update_sample_list_based_on_head",
        "sample_list",
        "add_custom_params",
        "sample_list",
        "flatten_for_bert",
        "sample_list",
        "feed_graph_to_vb",
        "graph_feed_mode",
        "sample_list",
        "sample_list",
        "model",
        "sample_list",
        "input_ids",
        "sample_list",
        "input_mask",
        "sample_list",
        "attention_mask",
        "sample_list",
        "token_type_ids",
        "sample_list",
        "visual_embeddings",
        "sample_list",
        "position_embeddings_visual",
        "sample_list",
        "visual_embeddings_type",
        "sample_list",
        "image_text_alignment",
        "sample_list",
        "masked_lm_labels",
        "graph_input",
        "output"
    ],
    "literals": [
        "\"krisp\"",
        "\"configs/models/krisp/defaults.yaml\"",
        "\"vb_hid_sz\"",
        "\"node_hid_dim\"",
        "\"feed_vb_to_graph\"",
        "\"feed_q_to_graph\"",
        "\"feed_mode\"",
        "\"feed_graph_to_vb\"",
        "\"feed_special_node\"",
        "\"topk_ans_feed\"",
        "\"compress_crossmodel\"",
        "\"crossmodel_compress_dim\"",
        "\"analysis_mode\"",
        "\"noback_vb\"",
        "\"bert-base-uncased\"",
        "\"q_hid_sz\"",
        "\"Import error with KRISP dependencies. Fix dependencies if \"",
        "\"you want to use KRISP\"",
        "\"mc4\"",
        "\"in_graph\"",
        "\"logit_fc\"",
        "\"add\"",
        "\"feed_graph_hid_to_vb\"",
        "\"feed_top_node_to_vb\"",
        "\"feed_graph_hid_to_vb\"",
        "\"Unknown feed mode %s\"",
        "\"graph_output\"",
        "\"feed_vb_hid_to_graph\"",
        "\"feed_vb_logit_to_graph\"",
        "\"vb_hidden\"",
        "\"vb_logits\"",
        "\"input_ids\"",
        "\"input_ids\"",
        "\"token_type_ids\"",
        "\"q_encoded\"",
        "\"mc4\"",
        "\"in_graph\"",
        "\"graph_prediction\"",
        "\"logit_fc\"",
        "\"concat\"",
        "\"alpha\"",
        "\"add\"",
        "\"ans\"",
        "\"scores\"",
        "\"plain\"",
        "\"pooler_strategy\"",
        "\"default\"",
        "\"feed_graph_to_vb\"",
        "\"node_hid_dim\"",
        "\"feed_mode\"",
        "\"topk_ans_feed\"",
        "\"bert_model_name\"",
        "\"nopretrain\"",
        "\"distributed_{}\"",
        "\"nlvr2\"",
        "\"feed_graph_hid_to_vb\"",
        "\"feed_top_node_to_vb\"",
        "\"nlvr2\"",
        "\"attention_weights\"",
        "\"sequence_output\"",
        "\"pooled_output\"",
        "\"vqa\"",
        "\"pretraining\"",
        "\"rb\"",
        "\"model\"",
        "\"bert\"",
        "\"model.\"",
        "f\"Missing keys {incompatible_keys.missing_keys} in the\"",
        "\" checkpoint.\\n\"",
        "\"If this is not your checkpoint, please open up an \"",
        "\"issue on MMF GitHub. \\n\"",
        "f\"Unexpected keys if any: {incompatible_keys.unexpected_keys}\"",
        "\"Unexpected keys in state dict: \"",
        "f\"{incompatible_keys.unexpected_keys} \\n\"",
        "\"This is usually not a problem with pretrained models, but \"",
        "\"if this is your own model, please double check. \\n\"",
        "\"If you think this is an issue, please open up a \"",
        "\"bug at MMF GitHub.\"",
        "\"freeze_base\"",
        "\"feed_graph_to_vb\"",
        "\"node_hid_dim\"",
        "\"feed_mode\"",
        "\"compress_crossmodel\"",
        "\"input_ids\"",
        "\"token_type_ids\"",
        "\"input_mask\"",
        "\"image_mask\"",
        "\"masked_lm_labels\"",
        "\"visual_embeddings\"",
        "\"nlvr2\"",
        "\"img0\"",
        "\"image_info_0\"",
        "\"max_features\"",
        "\"image_feature_0\"",
        "\"img1\"",
        "\"image_info_0\"",
        "\"max_features\"",
        "\"image_feature_0\"",
        "\"image_info_0\"",
        "\"max_features\"",
        "\"image_feature_0\"",
        "\"visual_embeddings\"",
        "\"image_dim\"",
        "\"lm_label_ids\"",
        "\"bert.bert\"",
        "\"model.bert\"",
        "\"bert.cls\"",
        "\"model.cls\"",
        "\"bert.classifier\"",
        "\"model.classifier\"",
        "\"feed_graph_hid_to_vb\"",
        "\"graph_special_node_out\"",
        "\"graph_special_node_out\""
    ],
    "variables": [
        "logger",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "extra_config",
        "q_enc",
        "extra_config",
        "graph_module",
        "vb_module",
        "vocab_fc",
        "graph_ptr_net",
        "graph_logit_fc",
        "missing_ans_inds",
        "graph_output",
        "sample_list",
        "vb_hidden",
        "vb_logits",
        "vb_hidden",
        "vb_logits",
        "sample_list",
        "sample_list",
        "attention_mask_q",
        "q_enc_out",
        "sample_list",
        "graph_output",
        "graph_logits",
        "graph_logits",
        "graph_logits",
        "graph_logits",
        "logits",
        "graph_logits",
        "logits",
        "output",
        "output",
        "hidden_size",
        "graph_hidden_size",
        "bl_w",
        "graph_w",
        "bl_hidden",
        "bl_hidden",
        "graph_hidden",
        "scores",
        "scores",
        "scores",
        "config",
        "config",
        "visual_embedding_dim",
        "config",
        "embedding_strategy",
        "config",
        "bypass_transformer",
        "config",
        "output_attentions",
        "config",
        "output_hidden_states",
        "embeddings",
        "encoder",
        "pooler",
        "bypass_transformer",
        "additional_layer",
        "output_attentions",
        "output_hidden_states",
        "fixed_head_masks",
        "attention_mask",
        "token_type_ids",
        "extended_attention_mask",
        "extended_attention_mask",
        "extended_attention_mask",
        "embedding_output",
        "text_length",
        "text_embedding_output",
        "visual_part",
        "text_extended_attention_mask",
        "encoded_layers",
        "sequence_output",
        "new_input",
        "final_sequence_output",
        "pooled_output",
        "embedding_output",
        "graph_att_mask",
        "extended_attention_mask",
        "encoded_layers",
        "sequence_output",
        "pooled_output",
        "attn_data_list",
        "attn_data_list",
        "config",
        "output_attentions",
        "output_hidden_states",
        "pooler_strategy",
        "feed_graph_to_vb",
        "graph_node_hid_dim",
        "graph_feed_mode",
        "graph_topk",
        "graph_embedding",
        "bert_model_name",
        "bert_config",
        "bert",
        "bert",
        "training_head_type",
        "dropout",
        "classifier",
        "graph_input",
        "graph_input",
        "sequence_output",
        "pooled_output",
        "attention_weights",
        "b",
        "h",
        "pooled_output",
        "output_dict",
        "output_dict",
        "output_dict",
        "output_dict",
        "index_to_gather",
        "pooled_output",
        "pooled_output",
        "output",
        "config",
        "extra_config",
        "extra_config",
        "model",
        "pretrained_file",
        "ckpt",
        "model_ckpt",
        "model_ckpt_new",
        "model_ckpt_new",
        "model_ckpt",
        "incompatible_keys",
        "p",
        "requires_grad",
        "feed_graph_to_vb",
        "graph_node_hid_dim",
        "graph_feed_mode",
        "to_be_flattened",
        "to_be_flattened_dim",
        "sample_list",
        "key",
        "sample_list",
        "key",
        "sample_list",
        "key",
        "sample_list",
        "key",
        "sample_list",
        "visual_embeddings_type",
        "attention_mask",
        "new_lm_labels",
        "size_masked_lm_labels",
        "new_lm_labels",
        "sample_list",
        "masked_lm_labels",
        "attention_mask",
        "sample_list",
        "attention_mask",
        "to_be_flattened",
        "to_be_flattened_dim",
        "flattened",
        "bert_input_ids",
        "bert_input_mask",
        "bert_input_type_ids",
        "bert_input_ids",
        "bert_input_mask",
        "bert_input_type_ids",
        "img0",
        "image_info",
        "image_dim_variable_0",
        "image_feat_variable_0",
        "img1",
        "image_info",
        "image_dim_variable_1",
        "image_feat_variable_1",
        "image_feat_variable",
        "image_dim_variable",
        "image_info",
        "image_dim_variable",
        "image_feat_variable",
        "sample_list",
        "visual_embeddings",
        "sample_list",
        "image_dim",
        "sample_list",
        "input_ids",
        "sample_list",
        "input_mask",
        "sample_list",
        "token_type_ids",
        "visual_embeddings",
        "image_dim",
        "sample_list",
        "masked_lm_labels",
        "image_mask",
        "image_dim",
        "image_mask",
        "sample_list",
        "image_mask",
        "sample_list",
        "image_mask",
        "sample_list",
        "position_embeddings_visual",
        "sample_list",
        "visual_embeddings_type",
        "sample_list",
        "image_text_alignment",
        "sample_list",
        "sample_list",
        "sample_list",
        "graph_input",
        "graph_input",
        "output"
    ],
    "comments": [
        "Copyright (c) Facebook, Inc. and its affiliates.",
        "This model essentially wraps GraphNetworkModule and multi-modal models",
        "Each method need to define a build method where the model's modules",
        "are actually build and assigned to the model",
        "Get any cross-model info we need for building network",
        "(like hidden sizes)",
        "Also pass arguments to know if it needs to feed in something",
        "If feed q, make the question module here",
        "We can just make it a BERT model really easily",
        "Import graph network module",
        "Putting in try-catch to avoid adding dependencies to mmf",
        "Builds the graph network module",
        "Make VisualBERT module (without the final hidden logit layer)",
        "Final hidden layer for the vb module",
        "There's whether to use the bilinear and then whether to add or concat features",
        "These are not mutally exclusive really",
        "If output combine is ptr net, make GraphPtr Net for combining outputs",
        "Bilinear network",
        "Logits is already computed",
        "Compute logits from single hidden layer",
        "Answer indices not in graph",
        "Now any index stil set to 1 is missing from graph",
        "Each model in MMF gets a dict called sample_list which contains",
        "all of the necessary information returned from the image",
        "If we have different combine modes, may need to call in different order",
        "Can't be both (would create circular dep)",
        "Check mode",
        "Can be feed_graph_hid_to_vb, where we pass in some vector",
        "rep of graph into vb or feed_top_node_to_vb which is similar,",
        "but it feeds in k node hidden states",
        "Forward through graph module",
        "Put graph_output into sample_list",
        "Forward through vb module",
        "Get vocab logit preds",
        "Check mode",
        "Can be feed_vb_hid_to_graph where we feed final",
        "vb state into graph as a node input",
        "Or feed_vb_logit_to_graph where we feed vg_predicted logits into graph",
        "Forward through vb module",
        "Get vocab logit preds",
        "If we feed seperate Q feats into graph",
        "Now sample_list has all the processed inputs for us",
        "Get pooled output",
        "Forward through graph module",
        "Compute graph logits",
        "Use bilinear network",
        "Logits is already computed",
        "Compute logits from single hidden layer",
        "Now combine outputs",
        "Output order should be alphabetical",
        "Combine both logits",
        "Output order should be ans",
        "Set invalid inds to zero here",
        "Do zerobias",
        "For loss calculations (automatically done by MMF",
        "as per the loss defined in the config),",
        "we need to return a dict with \"scores\" key as logits",
        "If we're in eval / analysis mode, add more to output",
        "MMF will automatically calculate loss",
        "Compute Eq. 4 from Iterative Answer Prediction with",
        "Pointer-Augmented Multimodal Transformers for TextVQA",
        "bl_hidden is bs x hidden_size",
        "graph_hidden is bs x graph_hidden_size",
        "Compute BL half",
        "Compute graph hidden half",
        "Assume we've already subsampled to only valid answer nodes",
        "Now we have bl_hidden as a bs x 1 x hid vec",
        "graph_hidden as a bs x num_nodes x hid vec",
        "Combine",
        "Normalize",
        "Scores is now a bs x #nodes matrix",
        "We create a 3D attention mask from a 2D tensor mask.",
        "Sizes are [batch_size, 1, 1, to_seq_length]",
        "So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
        "this attention mask is more simple than the triangular masking of",
        "causal attention used in OpenAI GPT, we just need to prepare the",
        "broadcast dimension here.",
        "Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
        "masked positions, this operation will create a tensor which is 0.0 for",
        "positions we want to attend and -10000.0 for masked positions.",
        "Since we are adding it to the raw scores before the softmax, this is",
        "effectively the same as removing these entirely.",
        "fp16 compatibility",
        "Don't support this for the bypass model",
        "If it takes graph input(s)",
        "Do forward here through its own embedding",
        "Then concat to embedding_output",
        "And concat onto the extended_attention_mask to inclode this too",
        "Concat onto embeddings",
        "Graph input params",
        "If doing graph, make a graph embedding layer",
        "hidden_dropout_prb",
        "If bert_model_name is not specified, you will need to specify",
        "all of the required parameters for BERTConfig and a pretrained",
        "model won't be loaded",
        "No pretrained model, init weights",
        "Classifier needs to be initialized always as it is task specific",
        "If we have a graph input, do the embedding first",
        "Sanity check sizes",
        "Add extra dim",
        "Do the graph embedding",
        "2B * H => B * 2H",
        "In VQA2 pooling strategy, we use representation from second last token",
        "Initialize from pretrained model",
        "Load the raw checkpoint",
        "Remove \"model\" in fron of keys",
        "Load the checkpoint",
        "Print any missing / wrong keys for debug",
        "Graph input params",
        "Not implemented for this model",
        "Make sure these keys are present or otherwise set these keys to None",
        "\"position_embeddings_visual\",",
        "\"visual_embeddings_type\",",
        "\"image_text_alignment\",",
        "We want to convert everything into: batch x sequence_length x (dim).",
        "image input",
        "pretraining labels",
        "image_feat_variable = batch x ( num_choice x ) image_feature_length x dim",
        "Prepare Mask",
        "Backward compatibility for code from original VisualBERT"
    ],
    "docstrings": [],
    "functions": [
        "config_path",
        "build",
        "forward",
        "forward",
        "forward",
        "init_weights",
        "forward",
        "build",
        "flatten",
        "get_optimizer_parameters",
        "flatten_for_bert",
        "update_sample_list_based_on_head",
        "add_custom_params",
        "format_state_key",
        "forward"
    ],
    "classes": [
        "KRISP",
        "GraphPtrNet",
        "VisualBERTBase",
        "VisualBERTForClassification",
        "VisualBERTModule"
    ]
}