{
    "identifiers": [
        "helper",
        "ActorCritic",
        "keras",
        "backend",
        "K",
        "tensorflow",
        "tf",
        "numpy",
        "np",
        "test",
        "t",
        "gym",
        "tf",
        "Session",
        "sess",
        "tf",
        "Session",
        "K",
        "set_session",
        "sess",
        "gym",
        "make",
        "ActorCritic",
        "env",
        "sess",
        "learning_rate",
        "hidden_size",
        "batch_size",
        "epsilon",
        "epsilon_decay",
        "gamma",
        "env",
        "reset",
        "env",
        "action_space",
        "sample",
        "episode",
        "train_episodes",
        "state",
        "reshape",
        "actor_critic",
        "act",
        "state",
        "env",
        "step",
        "action",
        "new_state",
        "reshape",
        "np",
        "float64",
        "reward",
        "format",
        "episode",
        "format",
        "reward",
        "actor_critic",
        "remember",
        "state",
        "action",
        "reward",
        "new_state",
        "done",
        "done",
        "env",
        "reset",
        "env",
        "action_space",
        "sample",
        "env",
        "step",
        "env",
        "action_space",
        "sample",
        "len",
        "rewards_list",
        "round",
        "reward",
        "round",
        "rewards_list",
        "round",
        "rewards_list",
        "round",
        "rewards_list",
        "round",
        "rewards_list",
        "round",
        "rewards_list",
        "env",
        "reset",
        "env",
        "action_space",
        "sample",
        "env",
        "step",
        "env",
        "action_space",
        "sample",
        "actor_critic",
        "train",
        "rewards_list",
        "append",
        "episode",
        "reward",
        "actor_critic",
        "save_actor",
        "actor_critic",
        "save_critic",
        "matplotlib",
        "pyplot",
        "plt",
        "x",
        "N",
        "np",
        "cumsum",
        "np",
        "insert",
        "x",
        "cumsum",
        "N",
        "cumsum",
        "N",
        "N",
        "np",
        "array",
        "rewards_list",
        "T",
        "running_mean",
        "rews",
        "T",
        "plt",
        "plot",
        "eps",
        "len",
        "smoothed_rews",
        "smoothed_rews",
        "plt",
        "plot",
        "eps",
        "rews",
        "color",
        "alpha",
        "plt",
        "xlabel",
        "plt",
        "ylabel",
        "plt",
        "show",
        "t",
        "test",
        "learning_rate",
        "hidden_size",
        "batch_size",
        "epsilon",
        "epsilon_decay",
        "gamma"
    ],
    "literals": [
        "'Training starting..'",
        "'BipedalWalker-v2'",
        "'[Training]'",
        "'Episode: {}'",
        "'Total reward: {}'",
        "'grey'",
        "'Episode'",
        "'Total Reward'"
    ],
    "variables": [
        "train_episodes",
        "gamma",
        "epsilon",
        "epsilon_decay",
        "hidden_size",
        "learning_rate",
        "batch_size",
        "score_requirement",
        "sess",
        "env",
        "actor_critic",
        "state",
        "action",
        "rewards_list",
        "state",
        "action",
        "new_state",
        "reward",
        "done",
        "_",
        "new_state",
        "reward",
        "action",
        "state",
        "reward",
        "done",
        "_",
        "action",
        "state",
        "_",
        "done",
        "_",
        "reward",
        "cumsum",
        "eps",
        "rews",
        "smoothed_rews"
    ],
    "comments": [
        "============================== #",
        "Hyperparameters         #",
        "============================== #",
        "exploration probability at start",
        "number of units in each Q-network hidden layer",
        "Q-network learning rate",
        "experience mini-batch size",
        "============================== #",
        "Training             #",
        "============================== #",
        "actor_critic.load_actor()",
        "actor_critic.load_critic()",
        "in case the enviroment gives us -100",
        "Add experience to memory",
        "============================== #",
        "Finished Game         #",
        "============================== #",
        "======================================= #",
        "Reset in Stuck case            #",
        "======================================= #",
        "serialize weights to HDF5",
        "============================================================ #",
        "Visualizing the training progress                   #",
        "============================================================ #",
        "%matplotlib inline"
    ],
    "docstrings": [],
    "functions": [
        "running_mean"
    ],
    "classes": []
}