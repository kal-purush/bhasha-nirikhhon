{
    "identifiers": [
        "os",
        "wandb",
        "torch",
        "dill",
        "pickle",
        "tqdm",
        "tqdm",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "general_code",
        "utils",
        "evalTools",
        "acc_metrics",
        "transformers",
        "BertForTokenClassification",
        "BertTokenizer",
        "general_code",
        "models",
        "bert_multitask",
        "BertForMultiTask",
        "general_code",
        "dataset",
        "for_bert_finetuning",
        "dataset_bert_finetuning",
        "collate_fun",
        "inputs",
        "tokenizer",
        "os",
        "path",
        "exists",
        "open",
        "f",
        "pickle",
        "load",
        "f",
        "output",
        "sen",
        "tqdm",
        "inputs",
        "desc",
        "sen",
        "split",
        "torch",
        "tensor",
        "token",
        "tokens",
        "tokenizer",
        "token",
        "return_tensors",
        "add_special_tokens",
        "padding",
        "out_ids",
        "size",
        "out_ids",
        "unsqueeze",
        "unsqueeze",
        "torch",
        "cat",
        "sen_ids",
        "out_ids",
        "squeeze",
        "dim",
        "torch",
        "cat",
        "sen_ids",
        "torch",
        "tensor",
        "dim",
        "sen_ids",
        "output",
        "append",
        "sen_ids",
        "open",
        "f",
        "pickle",
        "dump",
        "output",
        "f",
        "output",
        "dataset_path",
        "open",
        "dataset_path",
        "f",
        "dill",
        "load",
        "f",
        "train",
        "train",
        "test",
        "test",
        "train_labels",
        "test_labels",
        "train_path",
        "test_path",
        "open",
        "train_path",
        "f",
        "pickle",
        "load",
        "f",
        "open",
        "test_path",
        "f",
        "pickle",
        "load",
        "f",
        "tokens",
        "train_data_pre",
        "tokens",
        "token",
        "tokens",
        "temp",
        "token",
        "train_data",
        "append",
        "temp",
        "tokens",
        "test_data_pre",
        "tokens",
        "token",
        "tokens",
        "temp",
        "token",
        "test_data",
        "append",
        "temp",
        "train_data",
        "test_data",
        "args",
        "torch",
        "manual_seed",
        "args",
        "seed",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "wandb",
        "login",
        "host",
        "key",
        "wandb",
        "init",
        "project",
        "args",
        "project",
        "BertForTokenClassification",
        "from_pretrained",
        "args",
        "bert_path",
        "to",
        "device",
        "BertTokenizer",
        "from_pretrained",
        "args",
        "bert_path",
        "sentence_reshape",
        "train_path",
        "args",
        "train_sentences_path",
        "test_path",
        "args",
        "test_sentence_path",
        "labels_extrace",
        "args",
        "labels_path",
        "os",
        "path",
        "exists",
        "args",
        "input_ids_filename",
        "data_reprocess",
        "train",
        "tokenizer",
        "open",
        "args",
        "input_ids_filename",
        "f",
        "pickle",
        "load",
        "f",
        "dataset_bert_finetuning",
        "input_ids",
        "train_labels",
        "train_labels",
        "DataLoader",
        "batch_size",
        "args",
        "batch_size",
        "shuffle",
        "drop_last",
        "dataset",
        "dataset_for_fine",
        "collate_fn",
        "collate_fun",
        "torch",
        "optim",
        "Adam",
        "model",
        "parameters",
        "lr",
        "args",
        "lr",
        "epoch",
        "args",
        "epochs",
        "tqdm",
        "train_loader",
        "desc",
        "model",
        "train",
        "step",
        "batch",
        "iteration",
        "torch",
        "tensor",
        "batch",
        "batch",
        "model",
        "ids",
        "labels",
        "y",
        "contiguous",
        "output_hidden_states",
        "wandb",
        "log",
        "loss",
        "item",
        "optimizer",
        "zero_grad",
        "loss",
        "backward",
        "optimizer",
        "step",
        "args",
        "torch",
        "manual_seed",
        "args",
        "seed",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "wandb",
        "login",
        "host",
        "key",
        "wandb",
        "init",
        "project",
        "args",
        "project",
        "BertForMultiTask",
        "pretrain_path",
        "args",
        "bert_path",
        "hidden_size",
        "args",
        "hidden_size",
        "out_size1",
        "out_size2",
        "to",
        "device",
        "BertTokenizer",
        "from_pretrained",
        "args",
        "bert_path",
        "sentence_reshape",
        "train_path",
        "args",
        "train_sentences_path",
        "test_path",
        "args",
        "test_sentence_path",
        "labels_extrace",
        "args",
        "labels_path",
        "os",
        "path",
        "exists",
        "args",
        "input_ids_filename",
        "data_reprocess",
        "train",
        "tokenizer",
        "open",
        "args",
        "input_ids_filename",
        "f",
        "pickle",
        "load",
        "f",
        "dataset_bert_finetuning",
        "input_ids",
        "train_labels",
        "train_labels",
        "DataLoader",
        "batch_size",
        "args",
        "batch_size",
        "shuffle",
        "drop_last",
        "dataset",
        "dataset_for_fine",
        "collate_fn",
        "collate_fun",
        "torch",
        "optim",
        "Adam",
        "model",
        "parameters",
        "lr",
        "args",
        "lr",
        "torch",
        "nn",
        "CrossEntropyLoss",
        "epoch",
        "args",
        "epochs",
        "tqdm",
        "train_loader",
        "desc",
        "model",
        "train",
        "step",
        "batch",
        "iteration",
        "torch",
        "tensor",
        "batch",
        "batch",
        "model",
        "ids",
        "lossfun",
        "out1",
        "permute",
        "y",
        "lossfun",
        "out2",
        "permute",
        "z",
        "loss1",
        "args",
        "alpha",
        "loss2",
        "args",
        "alpha",
        "wandb",
        "log",
        "loss1",
        "item",
        "wandb",
        "log",
        "loss2",
        "item",
        "wandb",
        "log",
        "loss",
        "item",
        "optimizer",
        "zero_grad",
        "loss",
        "backward",
        "optimizer",
        "step"
    ],
    "literals": [
        "\".\\\\hot_data\\\\input_ids.pkl\"",
        "\".\\\\hot_data\\\\input_ids.pkl\"",
        "\"rb\"",
        "\"Reprocessing Data...\"",
        "\"pt\"",
        "'input_ids'",
        "\".\\\\hot_data\\\\input_ids.pkl\"",
        "\"wb\"",
        "\"rb\"",
        "\"rb\"",
        "\"rb\"",
        "\" \"",
        "\"\"",
        "\" \"",
        "\"\"",
        "\"cuda\"",
        "'cpu'",
        "\"http://47.108.152.202:8080\"",
        "\"local-86eb7fd9098b0b6aa0e6ddd886a989e62b6075f0\"",
        "\"rb\"",
        "\"Running train process:\"",
        "'loss'",
        "\"Loss1:\"",
        "\"cuda\"",
        "'cpu'",
        "\"http://47.108.152.202:8080\"",
        "\"local-86eb7fd9098b0b6aa0e6ddd886a989e62b6075f0\"",
        "\"rb\"",
        "\"Running train process:\"",
        "\"loss1:\"",
        "\"loss2:\"",
        "\"loss:\""
    ],
    "variables": [
        "output",
        "output",
        "tokens",
        "sen_ids",
        "out_ids",
        "out_ids",
        "sen_ids",
        "sen_ids",
        "sen_ids",
        "train",
        "test",
        "train_labels",
        "test_labels",
        "train_data_pre",
        "test_data_pre",
        "train_data",
        "test_data",
        "temp",
        "temp",
        "temp",
        "temp",
        "device",
        "model",
        "tokenizer",
        "train",
        "test",
        "train_labels",
        "test_labels",
        "input_ids",
        "input_ids",
        "dataset_for_fine",
        "train_loader",
        "optimizer",
        "iteration",
        "ids",
        "y",
        "z",
        "loss",
        "device",
        "model",
        "tokenizer",
        "train",
        "test",
        "train_labels",
        "test_labels",
        "input_ids",
        "input_ids",
        "dataset_for_fine",
        "train_loader",
        "optimizer",
        "lossfun",
        "iteration",
        "ids",
        "y",
        "z",
        "out1",
        "out2",
        "loss1",
        "loss2",
        "loss"
    ],
    "comments": [
        "开始编码"
    ],
    "docstrings": [
        "\"\"\"用第一个token作为wordpiece的结果\"\"\"",
        "\"\"\"从dataset文件中抽取出标签\"\"\"",
        "\"\"\"从token还原句子，留做模型输入\"\"\"",
        "\"\"\"\n    需要准备的数据只有两个，第一个是句子，第二个是标签即可\n\n    Args:\n        args: 参数类\n\n    Returns:\n       无\n    \"\"\"",
        "\"\"\"直接用bert作为底层完成多任务\"\"\""
    ],
    "functions": [
        "data_reprocess",
        "labels_extrace",
        "sentence_reshape",
        "trainer_bert_finetuning",
        "trainer_multitask_bert"
    ],
    "classes": []
}