{
    "identifiers": [
        "torch",
        "transformers",
        "BertTokenizer",
        "BertForSequenceClassification",
        "Trainer",
        "TrainingArguments",
        "datasets",
        "load_dataset",
        "load_dataset",
        "BertTokenizer",
        "from_pretrained",
        "batch",
        "tokenizer",
        "batch",
        "padding",
        "truncation",
        "dataset",
        "tokenize",
        "batched",
        "batch_size",
        "len",
        "dataset",
        "dataset",
        "tokenize",
        "batched",
        "batch_size",
        "len",
        "dataset",
        "train_dataset",
        "set_format",
        "columns",
        "test_dataset",
        "set_format",
        "columns",
        "BertForSequenceClassification",
        "from_pretrained",
        "TrainingArguments",
        "output_dir",
        "num_train_epochs",
        "per_device_train_batch_size",
        "per_device_eval_batch_size",
        "warmup_steps",
        "weight_decay",
        "logging_dir",
        "logging_steps",
        "Trainer",
        "model",
        "model",
        "args",
        "training_args",
        "train_dataset",
        "train_dataset",
        "eval_dataset",
        "test_dataset",
        "trainer",
        "train"
    ],
    "literals": [
        "\"imdb\"",
        "'bert-base-uncased'",
        "'text'",
        "'train'",
        "'train'",
        "'test'",
        "'test'",
        "'torch'",
        "'input_ids'",
        "'attention_mask'",
        "'label'",
        "'torch'",
        "'input_ids'",
        "'attention_mask'",
        "'label'",
        "'bert-base-uncased'",
        "'./results'",
        "'./logs'"
    ],
    "variables": [
        "dataset",
        "tokenizer",
        "train_dataset",
        "test_dataset",
        "model",
        "training_args",
        "trainer"
    ],
    "comments": [
        "train_model.py",
        "Load the IMDB dataset",
        "Load the BERT tokenizer",
        "Tokenize the dataset",
        "Load the BERT model for sequence classification",
        "Define the training arguments",
        "output directory",
        "total number of training epochs",
        "batch size for training",
        "batch size for evaluation",
        "number of warmup steps for learning rate scheduler",
        "strength of weight decay",
        "directory for storing logs",
        "Initialize the Trainer",
        "the instantiated ðŸ¤— Transformers model to be trained",
        "training arguments, defined above",
        "training dataset",
        "evaluation dataset",
        "Train the model"
    ],
    "docstrings": [],
    "functions": [
        "tokenize"
    ],
    "classes": []
}