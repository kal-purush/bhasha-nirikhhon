{
    "identifiers": [
        "torchao",
        "utils",
        "TORCH_VERSION_AT_LEAST_2_5",
        "unwrap_tensor_subclass",
        "pytest",
        "TORCH_VERSION_AT_LEAST_2_5",
        "pytest",
        "skip",
        "allow_module_level",
        "numpy",
        "full",
        "torch",
        "testing",
        "_internal",
        "common_utils",
        "run_tests",
        "torch",
        "_inductor",
        "test_case",
        "TestCase",
        "InductorTestCase",
        "torch",
        "testing",
        "_internal",
        "common_utils",
        "torch",
        "_dynamo",
        "testing",
        "CompileCounterWithBackend",
        "torchao",
        "quantization",
        "quantize_",
        "float8_weight_only",
        "float8_dynamic_activation_float8_weight",
        "torchao",
        "float8",
        "float8_utils",
        "compute_error",
        "torch",
        "unittest",
        "pytest",
        "tempfile",
        "copy",
        "random",
        "unittest",
        "mock",
        "patch",
        "random",
        "seed",
        "torch",
        "manual_seed",
        "torch",
        "cuda",
        "is_available",
        "torch",
        "cuda",
        "get_device_capability",
        "torch",
        "cuda",
        "is_available",
        "torch",
        "cuda",
        "get_device_capability",
        "torch",
        "nn",
        "Module",
        "in_features",
        "out_features",
        "torch",
        "nn",
        "Linear",
        "in_features",
        "out_features",
        "bias",
        "torch",
        "nn",
        "Linear",
        "out_features",
        "in_features",
        "bias",
        "x",
        "linear1",
        "x",
        "linear2",
        "x",
        "x",
        "InductorTestCase",
        "unittest",
        "skipIf",
        "torch",
        "cuda",
        "is_available",
        "unittest",
        "skipIf",
        "is_cuda_8_9",
        "common_utils",
        "parametrize",
        "torch",
        "bfloat16",
        "torch",
        "float32",
        "common_utils",
        "parametrize",
        "common_utils",
        "parametrize",
        "common_utils",
        "parametrize",
        "dtype",
        "torch",
        "dtype",
        "mode",
        "compile",
        "sizes",
        "sizes",
        "torch",
        "randn",
        "M",
        "K",
        "dtype",
        "dtype",
        "device",
        "float8_dynamic_activation_float8_weight",
        "float8_weight_only",
        "ToyLinearModel",
        "K",
        "N",
        "eval",
        "to",
        "dtype",
        "to",
        "copy",
        "deepcopy",
        "model",
        "mode_map",
        "mode",
        "quantize_",
        "model",
        "factory",
        "compile",
        "torch",
        "compile",
        "quantized_model",
        "fullgraph",
        "model",
        "input_tensor",
        "quantized_model",
        "input_tensor",
        "compute_error",
        "output_original",
        "output_quantized",
        "compute_error",
        "output_original",
        "output_quantized",
        "error",
        "common_utils",
        "instantiate_parametrized_tests",
        "TestAffineQuantizedFloat8Compile",
        "pytest",
        "main"
    ],
    "literals": [
        "\"Unsupported PyTorch version\"",
        "\"Need CUDA available\"",
        "\"Requires GPU with compute capability >= 8.9\"",
        "\"dtype\"",
        "\"mode\"",
        "\"dynamic\"",
        "\"weight-only\"",
        "\"compile\"",
        "\"sizes\"",
        "\"cuda\"",
        "\"dynamic\"",
        "\"weight-only\"",
        "\"cuda\"",
        "f\"Quantization error is too high got a SQNR of {error}\"",
        "\"__main__\""
    ],
    "variables": [
        "is_H100",
        "is_cuda_8_9",
        "linear1",
        "linear2",
        "x",
        "x",
        "M",
        "N",
        "K",
        "input_tensor",
        "mode_map",
        "model",
        "quantized_model",
        "factory",
        "quantized_model",
        "output_original",
        "output_quantized",
        "error"
    ],
    "comments": [
        "Inputs are (M,..), K, N",
        "Create a linear layer with bfloat16 dtype"
    ],
    "docstrings": [],
    "functions": [
        "forward",
        "test_fp8_linear_variants"
    ],
    "classes": [
        "ToyLinearModel",
        "TestAffineQuantizedFloat8Compile"
    ]
}