{
    "identifiers": [
        "learn",
        "SEQUENCES_NAME",
        "pickle",
        "array",
        "INPUT_LENGTH",
        "reshape",
        "random",
        "randint",
        "keras",
        "models",
        "load_model",
        "keras",
        "preprocessing",
        "sequence",
        "pad_sequences",
        "sys",
        "model",
        "tokenizer",
        "seq_length",
        "seed_text",
        "n_words",
        "seed_text",
        "_",
        "n_words",
        "tokenizer",
        "texts_to_sequences",
        "in_text",
        "pad_sequences",
        "encoded",
        "maxlen",
        "seq_length",
        "truncating",
        "model",
        "predict_classes",
        "encoded",
        "verbose",
        "word",
        "index",
        "tokenizer",
        "word_index",
        "items",
        "index",
        "yhat",
        "word",
        "in_text",
        "out_word",
        "result",
        "append",
        "out_word",
        "join",
        "result",
        "len",
        "sys",
        "argv",
        "print_error",
        "len",
        "sys",
        "argv",
        "sys",
        "argv",
        "gen_length",
        "isnumeric",
        "print_error",
        "gen_length",
        "sys",
        "argv",
        "len",
        "sys",
        "argv",
        "sys",
        "argv",
        "isnumeric",
        "sys",
        "argv",
        "user_seed",
        "INPUT_LENGTH",
        "INPUT_LENGTH",
        "len",
        "user_seed",
        "user_seed",
        "pickle",
        "load",
        "open",
        "SEQUENCES_NAME",
        "sequences",
        "randint",
        "len",
        "sequences",
        "len",
        "seed",
        "INPUT_LENGTH",
        "gen_length",
        "seed",
        "pickle",
        "load",
        "open",
        "tokenizerPath",
        "load_model",
        "modelFilePath",
        "generate_seq",
        "model",
        "tokenizer",
        "INPUT_LENGTH",
        "seed",
        "gen_length",
        "seq",
        "main"
    ],
    "literals": [
        "'pre'",
        "''",
        "''",
        "''",
        "\"Error â€“ expecting:\"",
        "\"`python3 create.py <'seedText'> <outputLength>`\"",
        "\"or\"",
        "\"`python3 create.py <'seedText'>`\"",
        "\"or\"",
        "\"`python3 create.py`\"",
        "'46'",
        "'50'",
        "'59'",
        "' '",
        "'65'",
        "\"rb\"",
        "\"\\n\"",
        "'tokenizer2.p'",
        "'model.h5'",
        "\"rb\"",
        "\"__main__\""
    ],
    "variables": [
        "result",
        "in_text",
        "encoded",
        "encoded",
        "yhat",
        "out_word",
        "out_word",
        "gen_length",
        "gen_length",
        "seed",
        "gen_length",
        "user_seed",
        "user_seed",
        "seed",
        "gen_length",
        "sequences",
        "seed",
        "tokenizerPath",
        "modelFilePath",
        "tokenizer",
        "model",
        "seq"
    ],
    "comments": [
        "generate a sequence from a language model",
        "generate a fixed number of words",
        "encode the text as integer",
        "print(in_text, encoded)",
        "truncate sequences to a fixed length",
        "predict probabilities for each word",
        "map predicted word index to word",
        "append to input",
        "print(str(sys.argv))",
        "exit(0)"
    ],
    "docstrings": [],
    "functions": [
        "generate_seq",
        "print_error",
        "main"
    ],
    "classes": []
}