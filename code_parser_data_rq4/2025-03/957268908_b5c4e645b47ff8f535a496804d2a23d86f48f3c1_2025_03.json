{
    "identifiers": [
        "csv",
        "os",
        "sys",
        "tempfile",
        "time",
        "datetime",
        "datetime",
        "io",
        "StringIO",
        "chromadb",
        "ollama",
        "streamlit",
        "st",
        "chromadb",
        "utils",
        "embedding_functions",
        "ollama_embedding_function",
        "OllamaEmbeddingFunction",
        "langchain_community",
        "document_loaders",
        "PyMuPDFLoader",
        "langchain_core",
        "documents",
        "Document",
        "langchain_ollama",
        "OllamaEmbeddings",
        "langchain_redis",
        "RedisConfig",
        "RedisVectorStore",
        "langchain_text_splitters",
        "RecursiveCharacterTextSplitter",
        "sentence_transformers",
        "CrossEncoder",
        "streamlit",
        "runtime",
        "uploaded_file_manager",
        "UploadedFile",
        "RedisVectorStore",
        "OllamaEmbeddings",
        "model",
        "RedisVectorStore",
        "embeddings",
        "config",
        "RedisConfig",
        "index_name",
        "redis_url",
        "distance_metric",
        "metadata_schema",
        "uploaded_file",
        "UploadedFile",
        "Document",
        "uploaded_file",
        "getvalue",
        "decode",
        "csv",
        "DictReader",
        "StringIO",
        "data",
        "row",
        "csv_reader",
        "docs",
        "append",
        "Document",
        "page_content",
        "row",
        "metadata",
        "row",
        "get_redis_store",
        "vector_store",
        "add_documents",
        "docs",
        "st",
        "success",
        "query",
        "n_results",
        "threshold",
        "get_redis_store",
        "vector_store",
        "similarity_search_with_score",
        "query",
        "k",
        "n_results",
        "results",
        "abs",
        "results",
        "match_percentage",
        "threshold",
        "results",
        "uploaded_file",
        "UploadedFile",
        "Document",
        "tempfile",
        "NamedTemporaryFile",
        "suffix",
        "temp_file",
        "write",
        "uploaded_file",
        "read",
        "PyMuPDFLoader",
        "temp_file",
        "name",
        "loader",
        "load",
        "os",
        "unlink",
        "temp_file",
        "name",
        "RecursiveCharacterTextSplitter",
        "chunk_size",
        "chunk_overlap",
        "separators",
        "text_splitter",
        "split_documents",
        "docs",
        "chromadb",
        "Collection",
        "OllamaEmbeddingFunction",
        "url",
        "model_name",
        "chromadb",
        "PersistentClient",
        "path",
        "chroma_client",
        "get_or_create_collection",
        "name",
        "embedding_function",
        "ollama_ef",
        "metadata",
        "all_splits",
        "Document",
        "file_name",
        "get_vector_collection",
        "idx",
        "split",
        "all_splits",
        "documents",
        "append",
        "split",
        "page_content",
        "metadatas",
        "append",
        "split",
        "metadata",
        "ids",
        "append",
        "file_name",
        "idx",
        "collection",
        "upsert",
        "documents",
        "documents",
        "metadatas",
        "metadatas",
        "ids",
        "ids",
        "st",
        "success",
        "prompt",
        "n_results",
        "get_vector_collection",
        "collection",
        "query",
        "query_texts",
        "prompt",
        "n_results",
        "n_results",
        "results",
        "context",
        "prompt",
        "show_thinking",
        "timer_placeholder",
        "time",
        "time",
        "ollama",
        "chat",
        "model",
        "stream",
        "messages",
        "system_prompt",
        "context",
        "prompt",
        "chunk",
        "response",
        "timer_placeholder",
        "time",
        "time",
        "start_time",
        "timer_placeholder",
        "metric",
        "elapsed",
        "chunk",
        "chunk",
        "buffer",
        "content",
        "show_thinking",
        "buffer",
        "buffer",
        "buffer",
        "in_think_block",
        "buffer",
        "split",
        "parts",
        "parts",
        "parts",
        "buffer",
        "in_think_block",
        "buffer",
        "split",
        "parts",
        "len",
        "parts",
        "in_think_block",
        "buffer",
        "buffer",
        "buffer",
        "content",
        "buffer",
        "in_think_block",
        "show_thinking",
        "buffer",
        "time",
        "time",
        "start_time",
        "timer_placeholder",
        "timer_placeholder",
        "metric",
        "total_time",
        "st",
        "session_state",
        "total_time",
        "documents",
        "show_thinking",
        "CrossEncoder",
        "encoder_model",
        "rank",
        "prompt",
        "documents",
        "top_k",
        "show_thinking",
        "st",
        "write",
        "ranks",
        "rank",
        "ranks",
        "relevant_text",
        "documents",
        "rank",
        "relevant_text_ids",
        "append",
        "rank",
        "show_thinking",
        "st",
        "write",
        "relevant_text",
        "st",
        "divider",
        "relevant_text",
        "relevant_text_ids",
        "st",
        "session_state",
        "st",
        "session_state",
        "st",
        "sidebar",
        "st",
        "set_page_config",
        "page_title",
        "st",
        "file_uploader",
        "accept_multiple_files",
        "help",
        "st",
        "radio",
        "options",
        "help",
        "uploaded_file",
        "upload_option",
        "uploaded_file",
        "name",
        "split",
        "st",
        "error",
        "sys",
        "exit",
        "st",
        "button",
        "uploaded_file",
        "process",
        "uploaded_file",
        "name",
        "translate",
        "maketrans",
        "upload_option",
        "create_cached_contents",
        "uploaded_file",
        "process_document",
        "uploaded_file",
        "add_to_vector_collection",
        "all_splits",
        "normalize_uploaded_file_name",
        "st",
        "header",
        "st",
        "text_area",
        "st",
        "columns",
        "cols",
        "st",
        "button",
        "use_container_width",
        "cols",
        "st",
        "checkbox",
        "value",
        "help",
        "key",
        "cols",
        "st",
        "session_state",
        "total_time",
        "st",
        "metric",
        "st",
        "session_state",
        "total_time",
        "st",
        "empty",
        "ask",
        "prompt",
        "st",
        "session_state",
        "st",
        "empty",
        "timer_container",
        "markdown",
        "time",
        "time",
        "time",
        "time",
        "start_time",
        "timer_container",
        "markdown",
        "elapsed",
        "update_timer",
        "query_semantic_cache",
        "query",
        "prompt",
        "cached_results",
        "st",
        "write",
        "cached_results",
        "metadata",
        "replace",
        "time",
        "time",
        "start_time",
        "timer_container",
        "markdown",
        "total_time",
        "st",
        "session_state",
        "total_time",
        "update_timer",
        "query_collection",
        "prompt",
        "prompt",
        "update_timer",
        "results",
        "get",
        "context",
        "st",
        "write",
        "sys",
        "exit",
        "update_timer",
        "re_rank_cross_encoders",
        "context",
        "show_thinking",
        "show_thinking",
        "update_timer",
        "st",
        "container",
        "response_container",
        "st",
        "empty",
        "ollama",
        "chat",
        "model",
        "stream",
        "messages",
        "system_prompt",
        "relevant_text",
        "prompt",
        "chunk",
        "response",
        "update_timer",
        "chunk",
        "chunk",
        "buffer",
        "content",
        "show_thinking",
        "buffer",
        "buffer",
        "buffer",
        "in_think_block",
        "buffer",
        "split",
        "parts",
        "response_text",
        "parts",
        "response_placeholder",
        "markdown",
        "response_text",
        "parts",
        "buffer",
        "in_think_block",
        "buffer",
        "split",
        "parts",
        "len",
        "parts",
        "in_think_block",
        "buffer",
        "buffer",
        "response_text",
        "buffer",
        "response_placeholder",
        "markdown",
        "response_text",
        "response_text",
        "content",
        "response_placeholder",
        "markdown",
        "response_text",
        "buffer",
        "in_think_block",
        "show_thinking",
        "response_text",
        "buffer",
        "response_placeholder",
        "markdown",
        "response_text",
        "time",
        "time",
        "start_time",
        "timer_container",
        "markdown",
        "total_time",
        "st",
        "session_state",
        "total_time",
        "show_thinking",
        "st",
        "expander",
        "st",
        "write",
        "results",
        "st",
        "expander",
        "st",
        "write",
        "relevant_text_ids",
        "st",
        "write",
        "relevant_text"
    ],
    "literals": [
        "\"bge-m3:567m\"",
        "\"cached_contents\"",
        "\"redis://localhost:6379\"",
        "\"COSINE\"",
        "\"name\"",
        "\"answer\"",
        "\"type\"",
        "\"text\"",
        "\"utf-8\"",
        "\"question\"",
        "\"answer\"",
        "\"answer\"",
        "\"Cache contents added!\"",
        "\"wb\"",
        "\".pdf\"",
        "\"\\n\\n\"",
        "\"\\n\"",
        "\".\"",
        "\"?\"",
        "\"!\"",
        "\" \"",
        "\"\"",
        "\"http://localhost:11434/api/embeddings\"",
        "\"bge-m3:567m\"",
        "\"./demo-rag-chroma\"",
        "\"rag_app\"",
        "\"hnsw:space\"",
        "\"cosine\"",
        "f\"{file_name}_{idx}\"",
        "\"Data added to the vector store!\"",
        "\"deepseek-r1:8b-llama-distill-q4_K_M\"",
        "\"role\"",
        "\"system\"",
        "\"content\"",
        "\"role\"",
        "\"user\"",
        "\"content\"",
        "f\"Context: {context}, Question: {prompt}\"",
        "\"\"",
        "\"⏱️\"",
        "f\"{elapsed:.1f}s\"",
        "\"done\"",
        "\"message\"",
        "\"content\"",
        "\"<think>\"",
        "\"</think>\"",
        "\"<think>\"",
        "\"<think>\"",
        "\"</think>\"",
        "\"</think>\"",
        "\"\"",
        "\"<think>\"",
        "\"\"",
        "\"⏱️\"",
        "f\"{total_time:.1f}s\"",
        "\"\"",
        "\"BAAI/bge-reranker-v2-m3\"",
        "\"corpus_id\"",
        "\"corpus_id\"",
        "\"__main__\"",
        "'total_time'",
        "\"RAG Question Answer\"",
        "\"**📑 Upload PDF files for QnA**\"",
        "\"pdf\"",
        "\"csv\"",
        "\"Upload csv for cached results only\"",
        "\"Upload options:\"",
        "\"Primary\"",
        "\"Cache\"",
        "\"Choose Primary for uploading document for QnA.\\n\\nChoose Cache for uploading cached results\"",
        "\"Primary\"",
        "\".\"",
        "\"csv\"",
        "\"CSV is only allowed for 'Cache' option.\"",
        "\"⚡️ Process\"",
        "\"-\"",
        "\"_\"",
        "\".\"",
        "\"_\"",
        "\" \"",
        "\"_\"",
        "\"Cache\"",
        "\"🗣️ RAG Question Answer\"",
        "\"**Ask a question related to your document:**\"",
        "\"🔥 Ask\"",
        "\"💭 Afficher la réflexion\"",
        "\"Montre ou cache le processus de réflexion du modèle (balises <think>)\"",
        "\"show_thinking_checkbox\"",
        "\"⏱️\"",
        "f\"{st.session_state.total_time:.1f}s\"",
        "\"⏱️ **0.0s**\"",
        "f\"⏱️ **{elapsed:.1f}s**\"",
        "\"answer\"",
        "\"\\\\n\"",
        "\"\\n\"",
        "f\"⏱️ **{total_time:.1f}s**\"",
        "\"documents\"",
        "\"No results found.\"",
        "\"\"",
        "\"deepseek-r1:8b-llama-distill-q4_K_M\"",
        "\"role\"",
        "\"system\"",
        "\"content\"",
        "\"role\"",
        "\"user\"",
        "\"content\"",
        "f\"Context: {relevant_text}, Question: {prompt}\"",
        "\"\"",
        "\"done\"",
        "\"message\"",
        "\"content\"",
        "\"<think>\"",
        "\"</think>\"",
        "\"<think>\"",
        "\"<think>\"",
        "\"</think>\"",
        "\"</think>\"",
        "\"\"",
        "\"<think>\"",
        "\"\"",
        "f\"⏱️ **{total_time:.1f}s**\"",
        "\"See retrieved documents\"",
        "\"See most relevant document ids\""
    ],
    "variables": [
        "system_prompt",
        "embeddings",
        "data",
        "csv_reader",
        "docs",
        "vector_store",
        "vector_store",
        "results",
        "match_percentage",
        "temp_file",
        "loader",
        "docs",
        "text_splitter",
        "ollama_ef",
        "chroma_client",
        "collection",
        "documents",
        "metadatas",
        "ids",
        "collection",
        "results",
        "start_time",
        "response",
        "buffer",
        "in_think_block",
        "elapsed",
        "content",
        "parts",
        "buffer",
        "in_think_block",
        "parts",
        "buffer",
        "in_think_block",
        "buffer",
        "total_time",
        "total_time",
        "relevant_text",
        "relevant_text_ids",
        "encoder_model",
        "ranks",
        "total_time",
        "uploaded_file",
        "upload_option",
        "process",
        "normalize_uploaded_file_name",
        "all_splits",
        "all_splits",
        "prompt",
        "cols",
        "ask",
        "show_thinking",
        "total_time",
        "timer_container",
        "start_time",
        "elapsed",
        "cached_results",
        "total_time",
        "total_time",
        "results",
        "context",
        "relevant_text",
        "relevant_text_ids",
        "response_container",
        "response_placeholder",
        "response_text",
        "response",
        "buffer",
        "in_think_block",
        "content",
        "parts",
        "buffer",
        "in_think_block",
        "parts",
        "buffer",
        "in_think_block",
        "buffer",
        "total_time",
        "total_time"
    ],
    "comments": [
        "Store uploaded file as a temp file",
        "Delete temp file",
        "Marquer le début de la génération",
        "Pour le traitement des balises <think>",
        "Mettre à jour le chronomètre à chaque chunk reçu",
        "Traiter le contenu pour filtrer les blocs <think>...</think>",
        "Si on trouve une balise d'ouverture",
        "Extraire le contenu avant la balise et l'envoyer",
        "Si on trouve une balise de fermeture",
        "Extraire le contenu après la balise et continuer",
        "Si pas de balise ou traitement incomplet, sortir de la boucle",
        "Envoyer le contenu accumulé s'il n'est pas dans un bloc think",
        "Si l'affichage du processus de réflexion est activé, envoyer tel quel",
        "Traiter tout contenu restant",
        "Enregistrer et afficher le temps total final",
        "Afficher les détails du reranking uniquement si show_thinking est activé",
        "Afficher le texte pertinent uniquement si show_thinking est activé",
        "Initialiser les variables de session pour le chronomètre",
        "Document Upload Area",
        "Question and Answer Area",
        "Nouvelle interface avec bouton plus petit et zone pour le chronomètre",
        "Afficher le chronomètre s'il y a un temps total",
        "Réinitialiser le temps total",
        "Container pour le chronomètre en temps réel - utiliser text au lieu de metric",
        "Démarrer le chronomètre global ici",
        "Fonction pour mettre à jour le chronomètre",
        "Première mise à jour",
        "Afficher les résultats du cache",
        "Mise à jour finale du chrono pour les résultats du cache",
        "Mettre à jour le chronomètre avant chaque étape",
        "Mettre à jour à nouveau",
        "Mettre à jour à nouveau",
        "Passer le paramètre show_thinking à la fonction re_rank_cross_encoders",
        "Mettre à jour à nouveau",
        "Appeler le modèle et afficher la réponse",
        "Placeholder pour la réponse",
        "Générer et afficher la réponse",
        "Utiliser directement ollama.chat pour plus de contrôle",
        "Pour le traitement des balises <think>",
        "Mettre à jour le chrono à chaque chunk",
        "Traiter le contenu pour filtrer les blocs <think>...</think>",
        "Si on trouve une balise d'ouverture",
        "Extraire le contenu avant la balise et l'envoyer",
        "Si on trouve une balise de fermeture",
        "Extraire le contenu après la balise et continuer",
        "Si pas de balise ou traitement incomplet, sortir de la boucle",
        "Envoyer le contenu accumulé s'il n'est pas dans un bloc think",
        "Si l'affichage du processus de réflexion est activé, envoyer tel quel",
        "Traiter tout contenu restant",
        "Enregistrer et afficher le temps total final",
        "Mise à jour finale du chronomètre",
        "Afficher les détails supplémentaires uniquement si show_thinking est activé"
    ],
    "docstrings": [
        "\"\"\"You are an well-being coach expert in question-answering tasks with a human-likedirect style. \n\nHere is the provided context to use to answer the question:\n\n{context} \n\nThink carefully about the above context. \n\nNow, review the user question:\n\n{question}\n\nProvide a factual answer to this questions using only the above provided context. Do not include any external knowledge or references or assumptions not present in the given context. \n\nDo not give an opinion or remarks. Do not make introductions or conclusions. Do not make explanations or summaries. Do not make assumptions. Do not say that you are answering from a provided context.\n\nUse one to three sentences maximum (it can be way less). Be direct in your answer and do not repeat yourself. Keep the answer short, concice and accurate to the provided context with the highest fidelity.\n\nIf the question is irrelevant to your role as a well-being coach, just answer : \"It's not my role to answer this question\".\nIf the question is relevant to your role but don't find the answer in the context or if you have a doubt, don't give explnations, only say :\"I can't give give you an accurate answer to this question yet\".\nIf any question is about you or refering to your behavior, just answer that you are a coach and that you try to help.\n\nAnswer:\"\"\"",
        "\"\"\"Gets or creates a Redis vector store for caching embeddings.\n\n    Creates an Ollama embeddings object using the nomic-embed-text model and initializes\n    a Redis vector store with cosine similarity metric for storing cached question-answer pairs.\n\n    Returns:\n        RedisVectorStore: A Redis vector store configured with Ollama embeddings and\n            metadata schema for storing answers.\n\n    Raises:\n        RedisConnectionError: If unable to connect to Redis.\n    \"\"\"",
        "\"\"\"Creates cached question-answer pairs from an uploaded CSV file.\n\n    Takes an uploaded CSV file containing question-answer pairs, converts them to Document\n    objects and adds them to a Redis vector store for caching.\n\n    Args:\n        uploaded_file: A Streamlit UploadedFile object containing the CSV data with\n            'question' and 'answer' columns.\n\n    Returns:\n        list[Document]: List of Document objects created from the CSV rows.\n\n    Raises:\n        ValueError: If CSV is missing required 'question' or 'answer' columns.\n        RedisConnectionError: If unable to add documents to Redis vector store.\n    \"\"\"",
        "\"\"\"Queries the semantic cache for similar questions and returns cached results if found.\n\n    Args:\n        query: The search query text to find relevant cached results.\n        n_results: Maximum number of results to return. Defaults to 1.\n        threshold: Minimum similarity score threshold (0-100) for returning cached results.\n            Defaults to 80.0.\n\n    Returns:\n        list: List of tuples containing matched Documents and their similarity scores if\n            matches above threshold are found. None if no matches above threshold.\n\n    Raises:\n        RedisConnectionError: If there are issues connecting to Redis.\n    \"\"\"",
        "\"\"\"Processes an uploaded PDF file by converting it to text chunks.\n\n    Takes an uploaded PDF file, saves it temporarily, loads and splits the content\n    into text chunks using recursive character splitting.\n\n    Args:\n        uploaded_file: A Streamlit UploadedFile object containing the PDF file\n\n    Returns:\n        A list of Document objects containing the chunked text from the PDF\n\n    Raises:\n        IOError: If there are issues reading/writing the temporary file\n    \"\"\"",
        "\"\"\"Gets or creates a ChromaDB collection for vector storage.\n\n    Creates an Ollama embedding function using the nomic-embed-text model and initializes\n    a persistent ChromaDB client. Returns a collection that can be used to store and\n    query document embeddings.\n\n    Returns:\n        chromadb.Collection: A ChromaDB collection configured with the Ollama embedding\n            function and cosine similarity space.\n    \"\"\"",
        "\"\"\"Adds document splits to a vector collection for semantic search.\n\n    Takes a list of document splits and adds them to a ChromaDB vector collection\n    along with their metadata and unique IDs based on the filename.\n\n    Args:\n        all_splits: List of Document objects containing text chunks and metadata\n        file_name: String identifier used to generate unique IDs for the chunks\n\n    Returns:\n        None. Displays a success message via Streamlit when complete.\n\n    Raises:\n        ChromaDBError: If there are issues upserting documents to the collection\n    \"\"\"",
        "\"\"\"Queries the vector collection with a given prompt to retrieve relevant documents.\n\n    Args:\n        prompt: The search query text to find relevant documents.\n        n_results: Maximum number of results to return. Defaults to 10.\n\n    Returns:\n        dict: Query results containing documents, distances and metadata from the collection.\n\n    Raises:\n        ChromaDBError: If there are issues querying the collection.\n    \"\"\"",
        "\"\"\"Calls the language model with context and prompt to generate a response.\n\n    Uses Ollama to stream responses from a language model by providing context and a\n    question prompt. The model uses a system prompt to format and ground its responses appropriately.\n\n    Args:\n        context: String containing the relevant context for answering the question\n        prompt: String containing the user's question\n        show_thinking: Boolean indicating whether to show the model's thinking process\n        timer_placeholder: Placeholder Streamlit pour afficher le chronomètre\n\n    Yields:\n        String chunks of the generated response as they become available from the model\n\n    Raises:\n        OllamaError: If there are issues communicating with the Ollama API\n    \"\"\"",
        "\"\"\"Re-ranks documents using a cross-encoder model for more accurate relevance scoring.\n\n    Uses the MS MARCO MiniLM cross-encoder model to re-rank the input documents based on\n    their relevance to the query prompt. Returns the concatenated text of the top 3 most\n    relevant documents along with their indices.\n\n    Args:\n        documents: List of document strings to be re-ranked.\n        show_thinking: Boolean indicating whether to show the reranking process.\n\n    Returns:\n        tuple: A tuple containing:\n            - relevant_text (str): Concatenated text from the top 3 ranked documents\n            - relevant_text_ids (list[int]): List of indices for the top ranked documents\n\n    Raises:\n        ValueError: If documents list is empty\n        RuntimeError: If cross-encoder model fails to load or rank documents\n    \"\"\""
    ],
    "functions": [
        "get_redis_store",
        "create_cached_contents",
        "query_semantic_cache",
        "process_document",
        "get_vector_collection",
        "add_to_vector_collection",
        "query_collection",
        "call_llm",
        "re_rank_cross_encoders",
        "update_timer"
    ],
    "classes": []
}