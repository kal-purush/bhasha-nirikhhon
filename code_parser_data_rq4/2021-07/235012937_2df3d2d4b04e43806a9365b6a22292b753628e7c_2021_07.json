{
    "identifiers": [
        "ray",
        "ray",
        "tune",
        "ray",
        "tune",
        "registry",
        "register_trainable",
        "register_env",
        "ray",
        "rllib",
        "env",
        "wrappers",
        "pettingzoo_env",
        "ParallelPettingZooEnv",
        "ray",
        "rllib",
        "contrib",
        "maddpg",
        "maddpg",
        "maddpg",
        "pettingzoo",
        "mpe",
        "supersuit",
        "ss",
        "argparse",
        "importlib",
        "import_module",
        "ray",
        "tune",
        "CLIReporter",
        "wandb",
        "wandb",
        "init",
        "argparse",
        "ArgumentParser",
        "parser",
        "add_argument",
        "choices",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "parse_args",
        "args",
        "ray",
        "init",
        "maddpg",
        "MADDPGTrainer",
        "register_trainable",
        "MADDPGAgent",
        "args",
        "env_name",
        "args",
        "env_type",
        "env_name",
        "config",
        "import_module",
        "env_str",
        "env",
        "parallel_env",
        "max_cycles",
        "args",
        "max_episode_len",
        "continuous_actions",
        "ss",
        "pad_observations_v0",
        "env",
        "ss",
        "pad_action_space_v0",
        "env",
        "env",
        "register_env",
        "env_name",
        "config",
        "ParallelPettingZooEnv",
        "env_creator",
        "config",
        "ParallelPettingZooEnv",
        "env_creator",
        "args",
        "env",
        "observation_spaces",
        "env",
        "action_spaces",
        "obs_space",
        "act_space",
        "env",
        "agents",
        "i",
        "args",
        "adv_policy",
        "i",
        "args",
        "num_adversaries",
        "args",
        "good_policy",
        "i",
        "len",
        "env",
        "agents",
        "env",
        "observation_spaces",
        "agents",
        "i",
        "env",
        "action_spaces",
        "agents",
        "i",
        "i",
        "use_local_critic",
        "i",
        "i",
        "gen_policy",
        "i",
        "i",
        "len",
        "env",
        "agents",
        "policies",
        "keys",
        "env_name",
        "args",
        "num_workers",
        "args",
        "num_gpus",
        "args",
        "num_envs_per_worker",
        "args",
        "max_episode_len",
        "args",
        "good_policy",
        "args",
        "adv_policy",
        "args",
        "num_units",
        "args",
        "num_units",
        "args",
        "n_step",
        "args",
        "gamma",
        "args",
        "replay_buffer",
        "args",
        "lr",
        "args",
        "lr",
        "args",
        "train_batch_size",
        "args",
        "max_episode_len",
        "args",
        "rollout_fragment_length",
        "args",
        "train_batch_size",
        "policies",
        "name",
        "policy_ids",
        "agents",
        "index",
        "name",
        "args",
        "eval_freq",
        "args",
        "eval_num_episodes",
        "args",
        "record",
        "args",
        "render",
        "tune",
        "run",
        "name",
        "config",
        "config",
        "progress_reporter",
        "CLIReporter",
        "stop",
        "args",
        "num_episodes",
        "checkpoint_freq",
        "args",
        "checkpoint_freq",
        "local_dir",
        "args",
        "local_dir",
        "args",
        "env_name",
        "restore",
        "args",
        "restore",
        "verbose",
        "parse_args",
        "main",
        "args"
    ],
    "literals": [
        "\"RLLib MADDPG with PettingZoo environments\"",
        "\"--env-type\"",
        "\"mpe\"",
        "\"sisl\"",
        "\"atari\"",
        "\"butterfly\"",
        "\"classic\"",
        "\"magent\"",
        "\"mpe\"",
        "\"The PettingZoo environment type\"",
        "\"--env-name\"",
        "\"simple_spread_v2\"",
        "\"The PettingZoo environment to use\"",
        "\"--max-episode-len\"",
        "\"maximum episode length\"",
        "\"--num-episodes\"",
        "\"number of episodes\"",
        "\"--num-adversaries\"",
        "\"number of adversaries\"",
        "\"--good-policy\"",
        "\"maddpg\"",
        "\"policy for good agents\"",
        "\"--adv-policy\"",
        "\"maddpg\"",
        "\"policy of adversaries\"",
        "\"--lr\"",
        "\"learning rate for Adam optimizer\"",
        "\"--gamma\"",
        "\"discount factor\"",
        "\"--rollout-fragment-length\"",
        "\"number of data points sampled /update /worker\"",
        "\"--train-batch-size\"",
        "\"number of data points /update\"",
        "\"--n-step\"",
        "\"length of multistep value backup\"",
        "\"--num-units\"",
        "\"number of units in the mlp\"",
        "\"--replay-buffer\"",
        "\"size of replay buffer in training\"",
        "\"--checkpoint-freq\"",
        "\"save model once every time this many iterations are completed\"",
        "\"--local-dir\"",
        "\"~/ray_results\"",
        "\"path to save checkpoints\"",
        "\"--restore\"",
        "\"directory in which training state and model are loaded\"",
        "\"--num-workers\"",
        "\"--num-envs-per-worker\"",
        "\"--num-gpus\"",
        "\"--eval-freq\"",
        "\"evaluate model every time this many iterations are completed\"",
        "\"--eval-num-episodes\"",
        "\"Number of episodes to run for evaluation\"",
        "\"--render\"",
        "\"render environment for evaluation\"",
        "\"--record\"",
        "\"path to store evaluation videos\"",
        "\"MADDPG\"",
        "\"pettingzoo.\"",
        "\".\"",
        "\"observation spaces: \"",
        "\"action spaces: \"",
        "\"ddpg\"",
        "\"ddpg\"",
        "\"agent_id\"",
        "\"use_local_critic\"",
        "\"policy_%d\"",
        "\"log_level\"",
        "\"ERROR\"",
        "\"env\"",
        "\"num_workers\"",
        "\"num_gpus\"",
        "\"num_gpus_per_worker\"",
        "\"num_envs_per_worker\"",
        "\"horizon\"",
        "\"good_policy\"",
        "\"adv_policy\"",
        "\"actor_hiddens\"",
        "\"actor_hidden_activation\"",
        "\"relu\"",
        "\"critic_hiddens\"",
        "\"critic_hidden_activation\"",
        "\"relu\"",
        "\"n_step\"",
        "\"gamma\"",
        "\"tau\"",
        "\"buffer_size\"",
        "\"actor_lr\"",
        "\"critic_lr\"",
        "\"learning_starts\"",
        "\"rollout_fragment_length\"",
        "\"train_batch_size\"",
        "\"batch_mode\"",
        "\"truncate_episodes\"",
        "\"multiagent\"",
        "\"policies\"",
        "\"policy_mapping_fn\"",
        "\"evaluation_interval\"",
        "\"evaluation_num_episodes\"",
        "\"evaluation_config\"",
        "\"record_env\"",
        "\"render_env\"",
        "\"contrib/MADDPG\"",
        "\"PZ_MADDPG\"",
        "\"episodes_total\"",
        "\"/\"",
        "'__main__'"
    ],
    "variables": [
        "parser",
        "MADDPGAgent",
        "env_name",
        "env_str",
        "env",
        "env",
        "env",
        "env",
        "env",
        "obs_space",
        "act_space",
        "agents",
        "use_local_critic",
        "policies",
        "policy_ids",
        "config",
        "args"
    ],
    "comments": [
        "Environment",
        "Core training parameters",
        "Checkpoint",
        "Parallelism",
        "Evaluation",
        "=== Setup ===",
        "=== Policy Config ===",
        "--- Model ---",
        "--- Exploration ---",
        "--- Replay buffer ---",
        "--- Optimization ---",
        "=== Multi-agent setting ===",
        "Workaround because MADDPG requires agent_id: int but actual ids are strings like 'speaker_0'",
        "=== Evaluation and rendering ==="
    ],
    "docstrings": [
        "\"\"\"\nThis script trains a PettingZoo MPE environment with continuous action\nspaces using RLlib's implementation of MADDPG.\nNote that the PettingZoo's MPE environments are not directly comparable to the \noriginal OpenAI MPE environments due to minor changes and bug fixes.\n\"\"\""
    ],
    "functions": [
        "parse_args",
        "main",
        "env_creator",
        "gen_policy"
    ],
    "classes": []
}