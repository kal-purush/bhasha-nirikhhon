{
    "identifiers": [
        "math",
        "torch",
        "torch",
        "nn",
        "nn",
        "util",
        "ComplexGroupNorm",
        "nn",
        "Module",
        "hidden_size",
        "gamma",
        "precision",
        "SimpleRetention",
        "precision",
        "NotImplementedError",
        "torch",
        "complex32",
        "torch",
        "float16",
        "precision",
        "torch",
        "complex64",
        "torch",
        "float32",
        "precision",
        "hidden_size",
        "gamma",
        "torch",
        "torch",
        "tensor",
        "torch",
        "tensor",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "dtype",
        "real_type",
        "hidden_size",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "dtype",
        "real_type",
        "hidden_size",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "dtype",
        "real_type",
        "hidden_size",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "nn",
        "Parameter",
        "theta",
        "X",
        "X",
        "shape",
        "_get_D",
        "sequence_length",
        "X",
        "dtype",
        "complex_type",
        "torch",
        "X",
        "torch",
        "zeros_like",
        "X",
        "to",
        "complex_type",
        "i",
        "to",
        "X",
        "device",
        "torch",
        "arange",
        "sequence_length",
        "dtype",
        "real_type",
        "device",
        "X",
        "device",
        "torch",
        "ns",
        "torch",
        "zeros_like",
        "ns",
        "to",
        "complex_type",
        "n",
        "ns",
        "Theta",
        "append",
        "torch",
        "exp",
        "i",
        "n",
        "theta",
        "torch",
        "stack",
        "Theta",
        "dim",
        "Theta",
        "conj",
        "X",
        "W_Q",
        "to",
        "complex_type",
        "Theta",
        "unsqueeze",
        "X",
        "W_K",
        "to",
        "complex_type",
        "Theta_bar",
        "unsqueeze",
        "X",
        "W_V",
        "to",
        "complex_type",
        "Q",
        "K",
        "permute",
        "D",
        "unsqueeze",
        "att",
        "V",
        "x_n",
        "s_n_1",
        "n",
        "x_n",
        "dtype",
        "complex_type",
        "torch",
        "x_n",
        "torch",
        "zeros_like",
        "x_n",
        "to",
        "complex_type",
        "torch",
        "tensor",
        "n",
        "dtype",
        "complex_type",
        "device",
        "x_n",
        "device",
        "torch",
        "exp",
        "i",
        "n",
        "theta",
        "Theta",
        "conj",
        "x_n",
        "W_Q",
        "to",
        "complex_type",
        "Theta",
        "x_n",
        "W_K",
        "to",
        "complex_type",
        "Theta_bar",
        "x_n",
        "W_V",
        "to",
        "complex_type",
        "gamma",
        "s_n_1",
        "K",
        "unsqueeze",
        "V",
        "unsqueeze",
        "Q",
        "unsqueeze",
        "s_n",
        "squeeze",
        "s_n",
        "sequence_length",
        "torch",
        "zeros",
        "sequence_length",
        "sequence_length",
        "dtype",
        "real_type",
        "requires_grad",
        "n",
        "sequence_length",
        "m",
        "sequence_length",
        "n",
        "m",
        "gamma",
        "n",
        "m",
        "D",
        "to",
        "complex_type",
        "nn",
        "Module",
        "hidden_size",
        "heads",
        "precision",
        "MultiScaleRetention",
        "hidden_size",
        "heads",
        "precision",
        "hidden_size",
        "heads",
        "hidden_size",
        "heads",
        "precision",
        "NotImplementedError",
        "torch",
        "complex32",
        "torch",
        "float16",
        "precision",
        "torch",
        "complex64",
        "torch",
        "float32",
        "torch",
        "exp",
        "torch",
        "linspace",
        "math",
        "log",
        "math",
        "log",
        "heads",
        "dtype",
        "real_type",
        "detach",
        "cpu",
        "tolist",
        "x",
        "x",
        "torch",
        "sigmoid",
        "x",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "dtype",
        "complex_type",
        "hidden_size",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "hidden_size",
        "hidden_size",
        "dtype",
        "complex_type",
        "hidden_size",
        "ComplexGroupNorm",
        "heads",
        "hidden_size",
        "nn",
        "ModuleList",
        "SimpleRetention",
        "head_size",
        "gamma",
        "gamma",
        "gammas",
        "X",
        "X",
        "dtype",
        "complex_type",
        "torch",
        "X",
        "torch",
        "zeros_like",
        "X",
        "to",
        "complex_type",
        "i",
        "heads",
        "Y",
        "append",
        "retentions",
        "i",
        "X",
        "i",
        "head_size",
        "i",
        "head_size",
        "torch",
        "cat",
        "Y",
        "dim",
        "group_norm",
        "Y",
        "reshape",
        "hidden_size",
        "reshape",
        "X",
        "shape",
        "swish",
        "X",
        "W_G",
        "Y",
        "W_O",
        "x_n",
        "s_n_1s",
        "n",
        "x_n",
        "dtype",
        "complex_type",
        "torch",
        "x_n",
        "torch",
        "zeros_like",
        "x_n",
        "to",
        "complex_type",
        "torch",
        "tensor",
        "n",
        "dtype",
        "complex_type",
        "device",
        "x_n",
        "device",
        "i",
        "heads",
        "retentions",
        "i",
        "forward_recurrent",
        "x_n",
        "i",
        "head_size",
        "i",
        "head_size",
        "s_n_1s",
        "i",
        "n",
        "Y",
        "append",
        "y",
        "s_ns",
        "append",
        "s_n",
        "torch",
        "cat",
        "Y",
        "dim",
        "group_norm",
        "Y",
        "swish",
        "x_n",
        "W_G",
        "Y",
        "W_O",
        "s_ns"
    ],
    "literals": [
        "\"single\"",
        "\"half\"",
        "\"batchmm does not support half precision complex yet.\"",
        "\"single\"",
        "\"single\"",
        "\"hidden_size must be divisible by heads\"",
        "\"half\"",
        "\"batchmm does not support half precision complex yet.\"",
        "\"single\""
    ],
    "variables": [
        "complex_type",
        "real_type",
        "complex_type",
        "real_type",
        "precision",
        "hidden_size",
        "gamma",
        "i",
        "W_Q",
        "W_K",
        "W_V",
        "theta",
        "theta",
        "sequence_length",
        "D",
        "X",
        "i",
        "ns",
        "ns",
        "Theta",
        "Theta",
        "Theta_bar",
        "Q",
        "K",
        "V",
        "att",
        "x_n",
        "n",
        "Theta",
        "Theta_bar",
        "Q",
        "K",
        "V",
        "s_n",
        "D",
        "D",
        "n",
        "m",
        "hidden_size",
        "heads",
        "precision",
        "head_size",
        "complex_type",
        "real_type",
        "complex_type",
        "real_type",
        "gammas",
        "swish",
        "W_G",
        "W_O",
        "group_norm",
        "retentions",
        "X",
        "Y",
        "Y",
        "Y",
        "x_n",
        "n",
        "Y",
        "s_ns",
        "y",
        "s_n",
        "Y",
        "Y"
    ],
    "comments": [
        "K: (batch_size, hidden_size)",
        "V: (batch_size, hidden_size)",
        "s_n_1: (batch_size, hidden_size, hidden_size)",
        "s_n = gamma * s_n_1 + K^T @ V",
        "D[n,m] = gamma ** (n - m) if n >= m else 0",
        "apply each individual retention mechanism to a slice of X",
        "apply each individual retention mechanism to a slice of X"
    ],
    "docstrings": [
        "\"\"\"\n        Simple retention mechanism based on the paper\n        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n        \"\"\"",
        "\"\"\"\n        Parallel (default) representation of the retention mechanism.\n        X: (batch_size, sequence_length, hidden_size)\n        \"\"\"",
        "\"\"\"\n        Recurrent representation of the retention mechanism.\n        x_n: (batch_size, hidden_size)\n        s_n_1: (batch_size, hidden_size)\n        \"\"\"",
        "\"\"\"\n        Multi-scale retention mechanism based on the paper\n        \"Retentive Network: A Successor to Transformer for Large Language Models\"[https://arxiv.org/pdf/2307.08621.pdf]\n        \"\"\"",
        "\"\"\"\n        parallel representation of the multi-scale attention mechanism\n        \"\"\"",
        "\"\"\"\n        recurrent representation of the multi-scale attention mechanism\n        \"\"\""
    ],
    "functions": [
        "forward",
        "forward_recurrent",
        "_get_D",
        "forward",
        "forward_recurrent"
    ],
    "classes": [
        "SimpleRetention",
        "MultiScaleRetention"
    ]
}