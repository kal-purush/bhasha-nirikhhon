{
    "identifiers": [
        "numpy",
        "np",
        "Z",
        "np",
        "exp",
        "Z",
        "Z",
        "A",
        "shape",
        "Z",
        "shape",
        "A",
        "cache",
        "Z",
        "np",
        "maximum",
        "Z",
        "Z",
        "A",
        "shape",
        "Z",
        "shape",
        "A",
        "cache",
        "dA",
        "cache",
        "cache",
        "np",
        "exp",
        "Z",
        "dA",
        "s",
        "s",
        "dZ",
        "shape",
        "Z",
        "shape",
        "dZ",
        "dA",
        "cache",
        "cache",
        "np",
        "array",
        "dA",
        "copy",
        "Z",
        "dZ",
        "shape",
        "Z",
        "shape",
        "dZ",
        "layer_dims",
        "np",
        "random",
        "seed",
        "SEED_VALUE",
        "len",
        "layer_dims",
        "layer",
        "L",
        "layer",
        "np",
        "random",
        "randn",
        "layer_dims",
        "layer",
        "layer_dims",
        "layer",
        "layer",
        "np",
        "zeros",
        "layer_dims",
        "layer",
        "parameters",
        "layer",
        "shape",
        "layer_dims",
        "layer",
        "layer_dims",
        "layer",
        "parameters",
        "layer",
        "shape",
        "layer_dims",
        "layer",
        "parameters",
        "A",
        "W",
        "b",
        "np",
        "dot",
        "W",
        "A",
        "b",
        "Z",
        "shape",
        "W",
        "shape",
        "A",
        "shape",
        "A",
        "W",
        "b",
        "Z",
        "cache",
        "A_prev",
        "W",
        "b",
        "activation",
        "activation",
        "linear_forward",
        "A_prev",
        "W",
        "b",
        "sigmoid",
        "Z",
        "linear_forward",
        "A_prev",
        "W",
        "b",
        "relu",
        "Z",
        "A",
        "shape",
        "W",
        "shape",
        "A_prev",
        "shape",
        "linear_cache",
        "activation_cache",
        "A",
        "cache",
        "X",
        "parameters",
        "X",
        "len",
        "parameters",
        "l",
        "L",
        "A",
        "linear_activation_forward",
        "A_prev",
        "parameters",
        "l",
        "parameters",
        "l",
        "activation",
        "caches",
        "append",
        "cache",
        "linear_activation_forward",
        "A",
        "parameters",
        "L",
        "parameters",
        "L",
        "activation",
        "caches",
        "append",
        "cache",
        "AL",
        "shape",
        "X",
        "shape",
        "AL",
        "caches",
        "AL",
        "Y",
        "Y",
        "shape",
        "np",
        "sum",
        "np",
        "multiply",
        "Y",
        "np",
        "log",
        "AL",
        "np",
        "multiply",
        "Y",
        "np",
        "log",
        "AL",
        "m",
        "np",
        "squeeze",
        "cost",
        "cost",
        "shape",
        "cost",
        "dZ",
        "cache",
        "cache",
        "A_prev",
        "shape",
        "np",
        "dot",
        "dZ",
        "A_prev",
        "T",
        "m",
        "np",
        "sum",
        "dZ",
        "axis",
        "keepdims",
        "m",
        "np",
        "dot",
        "W",
        "T",
        "dZ",
        "dA_prev",
        "shape",
        "A_prev",
        "shape",
        "dW",
        "shape",
        "W",
        "shape",
        "db",
        "shape",
        "b",
        "shape",
        "dA_prev",
        "dW",
        "db",
        "dA",
        "cache",
        "activation",
        "cache",
        "activation",
        "relu_backward",
        "dA",
        "activation_cache",
        "linear_backward",
        "dZ",
        "linear_cache",
        "sigmoid_backward",
        "dA",
        "activation_cache",
        "linear_backward",
        "dZ",
        "linear_cache",
        "dA_prev",
        "dW",
        "db",
        "AL",
        "Y",
        "caches",
        "len",
        "caches",
        "Y",
        "reshape",
        "AL",
        "shape",
        "np",
        "divide",
        "Y",
        "AL",
        "np",
        "divide",
        "Y",
        "AL",
        "caches",
        "L",
        "grads",
        "L",
        "grads",
        "L",
        "grads",
        "L",
        "linear_activation_backward",
        "dAL",
        "current_cache",
        "activation",
        "layer",
        "reversed",
        "L",
        "caches",
        "layer",
        "linear_activation_backward",
        "grads",
        "layer",
        "current_cache",
        "activation",
        "layer",
        "dA_prev_temp",
        "layer",
        "dW_temp",
        "layer",
        "db_temp",
        "grads",
        "parameters",
        "grads",
        "learning_rate",
        "len",
        "parameters",
        "layer",
        "L",
        "layer",
        "parameters",
        "layer",
        "learning_rate",
        "grads",
        "layer",
        "layer",
        "parameters",
        "layer",
        "learning_rate",
        "grads",
        "layer",
        "parameters",
        "X",
        "Y",
        "layers_dims",
        "learning_rate",
        "num_iterations",
        "print_cost",
        "np",
        "random",
        "seed",
        "SEED_VALUE",
        "initialize_parameters",
        "layers_dims",
        "i",
        "num_iterations",
        "L_model_forward",
        "X",
        "parameters",
        "compute_cost",
        "AL",
        "Y",
        "L_model_backward",
        "AL",
        "Y",
        "caches",
        "update_parameters",
        "parameters",
        "grads",
        "learning_rate",
        "print_cost",
        "i",
        "i",
        "cost",
        "costs",
        "append",
        "cost",
        "parameters",
        "X",
        "parameters",
        "Y",
        "X",
        "shape",
        "np",
        "zeros",
        "m",
        "L_model_forward",
        "X",
        "parameters",
        "i",
        "probabilities",
        "shape",
        "probabilities",
        "i",
        "Y",
        "np",
        "sum",
        "P",
        "Y",
        "m",
        "P"
    ],
    "literals": [
        "'W'",
        "'b'",
        "'W'",
        "'b'",
        "\"sigmoid\"",
        "'W'",
        "'b'",
        "\"relu\"",
        "'W'",
        "'b'",
        "\"sigmoid\"",
        "\"relu\"",
        "\"dA\"",
        "\"dW\"",
        "\"db\"",
        "\"sigmoid\"",
        "\"dA\"",
        "\"relu\"",
        "\"dA\"",
        "\"dW\"",
        "\"db\"",
        "\"W\"",
        "\"W\"",
        "\"dW\"",
        "\"b\"",
        "\"b\"",
        "\"db\"",
        "\"Cost after iteration %i: %f\"",
        "\"Accuracy: \""
    ],
    "variables": [
        "SEED_VALUE",
        "LAYERS_DIMS",
        "A",
        "cache",
        "A",
        "cache",
        "Z",
        "s",
        "dZ",
        "Z",
        "dZ",
        "dZ",
        "parameters",
        "L",
        "parameters",
        "parameters",
        "Z",
        "cache",
        "Z",
        "linear_cache",
        "A",
        "activation_cache",
        "Z",
        "linear_cache",
        "A",
        "activation_cache",
        "cache",
        "caches",
        "A",
        "L",
        "A_prev",
        "A",
        "cache",
        "AL",
        "cache",
        "m",
        "cost",
        "cost",
        "A_prev",
        "W",
        "b",
        "m",
        "dW",
        "db",
        "dA_prev",
        "linear_cache",
        "activation_cache",
        "dZ",
        "dA_prev",
        "dW",
        "db",
        "dZ",
        "dA_prev",
        "dW",
        "db",
        "grads",
        "L",
        "Y",
        "dAL",
        "current_cache",
        "current_cache",
        "dA_prev_temp",
        "dW_temp",
        "db_temp",
        "grads",
        "grads",
        "grads",
        "L",
        "parameters",
        "parameters",
        "costs",
        "parameters",
        "AL",
        "caches",
        "cost",
        "grads",
        "parameters",
        "m",
        "P",
        "probabilities",
        "_",
        "P",
        "i",
        "P",
        "i"
    ],
    "comments": [
        "Generic L-layer (ReLU-Sigmoid) 'straight in Python' Deep Neural Network implementation using basic Python/numpy.",
        "Other activation functions, mini-batches, regularization, optimization and batch normalization to be implemented.",
        "",
        "usage example:    params = L_layer_model(trainX, trainY, LAYERS_DIMS, num_iterations=1500)",
        "predictTrain = predict(trainX, parameters, trainY)",
        "predictDev = predict(devX, parameters, devY)",
        "predictTest = predict(testX, parameters, testY)",
        "main package",
        "seed global variable",
        "constants defining the model",
        "[12288, 20, 7, 5, 1] example of 4 layer model, 3 hidden + output, 12288 is the # of input features",
        "number of layers in the network",
        "Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".",
        "elif activation == \"relu\":",
        "Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".",
        "number of layers in the network",
        "Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.",
        "Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.",
        "Compute loss from AL and Y.",
        "to make sure cost's shape is as expected (fow example, this turns [[17]] into 17).",
        "elif activation == \"sigmoid\":",
        "the number of layers",
        "Initializing the backpropagation",
        "Lth layer (SIGMOID -> LINEAR) gradients.",
        "Inputs: \"dAL, current_cache\".",
        "Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]",
        "Loop from l=L-2 to l=0",
        "lth layer: (RELU -> LINEAR) gradients.",
        "Inputs: \"grads[\"dA\" + str(layer + 1)], current_cache\".",
        "Outputs: \"grads[\"dA\" + str(layer)], grads[\"dW\" + str(layer + 1)], grads[\"db\" + str(layer + 1)]",
        "number of layers in the neural network",
        "keep track of cost",
        "parameters initialization",
        "loop gradient descent",
        "forward propagation, for now [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.",
        "compute cost",
        "backward propagation",
        "update parameters",
        "print the cost every 100 training example",
        "forward propagation",
        "convert probabilities to 0/1 predictions"
    ],
    "docstrings": [
        "\"\"\"\n    Implements the sigmoid activation function, vectorized version for all units in a layer (array Z).\n    \n    Params:\n    :Z: numpy array of any shape, output of the linear layer\n    \n    Returns:\n    :A: post-activation output of sigmoid(z), same shape as Z\n    :cache: cached Z needed for backpropagation\n\n    \"\"\"",
        "\"\"\"\n    Implements the ReLU activation function, vectorized version for all units in a layer (array Z).\n\n    Params:\n    :Z: numpy array of any shape, output of the linear layer\n\n    Returns:\n    :A: post-activation output of relu(Z), of the same shape as Z\n    :cache: cached Z needed for backpropagation\n\n    \"\"\"",
        "\"\"\"\n    Implements the backward propagation for SIGMOID activation function, (vectorized version) for all units in a layer.\n\n    Params:\n    :dA: post-activation gradient, numpy array of any shape\n    :cache: Z stored earlier for computing backward propagation efficiently\n\n    Returns:\n    :dZ: gradient of the cost function with respect to Z\n    \n    \"\"\"",
        "\"\"\"\n    Implements the backward propagation for ReLU activation function, (vectorized version) for all units in a layer.\n\n    Params:\n    :dA: post-activation gradient, numpy array of any shape\n    :cache: Z stored earlier for computing backward propagation efficiently\n\n    Returns:\n    :dZ: gradient of the cost function with respect to Z\n    \n    \"\"\"",
        "\"\"\"\n    Initialize weight matrices and bias vectors for all layers of a network.\n    \n    Params:\n    :layer_dims: python array (list) containing the dimensions (# of units) \n                 of each layer in network\n    \n    Returns:\n    :parameters: python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1)\n\n    \"\"\"",
        "\"\"\"\n    Implements the linear part of a layer's forward propagation (wa + b), vectorized version.\n\n    Parameters:\n    :A: activations from previous layer (input data) of shape number of units of previous layer by number of examples\n    :W: weights matrix, numpy array of shape size (# of units) of current layer by size of previous layer\n    :b: bias vector, numpy array of shape size of the current layer by 1\n\n    Returns:\n    :Z: the input of the activation function, pre-activation parameter \n    :cache: a Python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n\n    \"\"\"",
        "\"\"\"\n    Implements the forward propagation for the LINEAR->ACTIVATION layer.\n\n    Params:\n    :A_prev: activations from previous layer (or input data for the first layer) of shape size of previous layer by\n            number of examples\n    :W: weights matrix, numpy array of shape size of current layer by size of previous layer\n    :b: bias vector, numpy array of shape size of the current layer by 1\n    :activation: the activation function to be used in layer, stored as a text string: \"sigmoid\" or \"relu\" (only those\n                two for now)\n\n    Returns:\n    :A: the output of the activation function, post-activation value \n    :cache: a Python tuple containing linear cache and activation cache stored for computing the backward pass\n            efficiently\n\n    \"\"\"",
        "\"\"\"\n    Implements forward propagation for the layers in a network, for now hard coded [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n    (as output layer) computation.\n    \n    Params:\n    :X: data, numpy array of shape input size (# of features) by number of examples\n    :parameters: output of initialize_parameters()\n    \n    Returns:\n    :AL: last post-activation value (from the output layer, prediction probability)\n    :caches: list of caches containing every cache of linear_activation_forward() (L-1 of them, indexed from 0 to L-1)\n    \n    \"\"\"",
        "\"\"\"\n    Implements the cost function defined by equation -[y*log(y_hat)+(1-y)*log(1-y_hat)].\n\n    Params:\n    :AL: probability vector corresponding to \"label\" predictions, y_hat in the aforementioned function, shape 1 by\n        number of examples\n    :Y: true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape 1 by number of examples\n\n    Returns:\n    :cost: cross-entropy cost\n    \n    \"\"\"",
        "\"\"\"\n    Implements the linear portion of backward propagation for a single layer (layer l).\n\n    Params:\n    :dZ: gradient of the cost with respect to the linear output (of current layer l)\n    :cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    :dA_prev: gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    :dW: gradient of the cost with respect to W (current layer l), same shape as W\n    :db: gradient of the cost with respect to b (current layer l), same shape as b\n    \n    \"\"\"",
        "\"\"\"\n    Implements the backward propagation for the LINEAR->ACTIVATION layer (for now only relu and sigmoid).\n    \n    Parameters:\n    :dA: post-activation gradient for current layer l \n    :cache: tuple of values (linear_cache, activation_cache) stored for computing backward propagation efficiently\n    :activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    :dA_prev: gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    :dW: gradient of the cost with respect to W (current layer l), same shape as W\n    :db: gradient of the cost with respect to b (current layer l), same shape as b\n    \n    \"\"\"",
        "\"\"\"\n    Implements the backward propagation for the model, for now hard coded as [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID\n        (output layer).\n    \n    Params:\n    :AL: probability vector, output of the forward propagation L_model_forward()\n    :Y: true \"label\" vector (for example containing 0 if non-cat, 1 if cat)\n    :caches: list of caches containing:\n            every cache of linear_activation_forward() with \"relu\" it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n            the cache of linear_activation_forward() with \"sigmoid\" it's caches[L-1]\n    \n    Returns:\n    :grads: a dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \n    \"\"\"",
        "\"\"\"\n    Update parameters using gradient descent.\n    \n    Params:\n    :parameters: Python dictionary containing weights and bias parameters \n    :grads: Python dictionary containing gradients, output of L_model_backward\n    :learning_rate: model's learning rate\n    \n    Returns:\n    :parameters: Python dictionary containing updated parameters \n                parameters[\"W\" + str(l)] = ... \n                parameters[\"b\" + str(l)] = ...\n    \n    \"\"\"",
        "\"\"\"\n    Implements a L-layer neural network with [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID (output layer) activation functions.\n\n    Params:\n    :X: data, numpy array of shape n_x (number of features) by m (number of examples)\n    :Y: true \"label\" vector (for example containing 0 if cat, 1 if non-cat) of shape 1 by number of examples\n    :layers_dims: list containing the input size and each layer size, of length number of layers + 1\n    :learning_rate: learning rate of the gradient descent update rule\n    :num_iterations: number of iterations of the optimization loop\n    :print_cost: if True, it prints the cost at every 100 steps\n\n    Returns:\n    :parameters: parameters learnt by the model (used to predict)\n\n    \"\"\"",
        "\"\"\"\n    Function used to predict the results of a L-layer neural network.\n\n    Params:\n    :X: data set of examples to label, numpy array of shape n_x (number of features) by m (number of examples)\n    :parameters: parameters of the trained model, returned by L_layer_model()\n    :Y: if given, true \"label\" vector of shape 1 by number of examples to print the accuracy\n\n    Returns:\n    :P: predictions for the given dataset X, shape 1 by number of examples\n\n    \"\"\""
    ],
    "functions": [
        "sigmoid",
        "relu",
        "sigmoid_backward",
        "relu_backward",
        "initialize_parameters",
        "linear_forward",
        "linear_activation_forward",
        "L_model_forward",
        "compute_cost",
        "linear_backward",
        "linear_activation_backward",
        "L_model_backward",
        "update_parameters",
        "L_layer_model",
        "predict"
    ],
    "classes": []
}