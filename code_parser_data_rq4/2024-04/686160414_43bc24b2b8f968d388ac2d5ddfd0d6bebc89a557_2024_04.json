{
    "identifiers": [
        "torch",
        "torch",
        "multiprocessing",
        "mp",
        "megatron",
        "core",
        "parallel_state",
        "megatron",
        "core",
        "utils",
        "divide",
        "omegaconf",
        "omegaconf",
        "OmegaConf",
        "open_dict",
        "nemo",
        "collections",
        "nlp",
        "parts",
        "megatron_trainer_builder",
        "MegatronStableDiffusionTrainerBuilder",
        "nemo",
        "collections",
        "nlp",
        "parts",
        "peft_config",
        "PEFT_CONFIG_MAP",
        "nemo",
        "core",
        "config",
        "hydra_runner",
        "nemo",
        "utils",
        "logging",
        "nemo",
        "utils",
        "exp_manager",
        "exp_manager",
        "nemo_aligner",
        "algorithms",
        "supervised",
        "SupervisedTrainer",
        "nemo_aligner",
        "data",
        "mm",
        "text_webdataset",
        "nemo_aligner",
        "data",
        "nlp",
        "builders",
        "build_dataloader",
        "nemo_aligner",
        "models",
        "mm",
        "stable_diffusion",
        "image_text_rms",
        "get_reward_model",
        "nemo_aligner",
        "models",
        "mm",
        "stable_diffusion",
        "megatron_sd_draftp_model",
        "MegatronSDDRaFTPModel",
        "nemo_aligner",
        "utils",
        "distributed",
        "Timer",
        "nemo_aligner",
        "utils",
        "train_script_utils",
        "CustomLoggerWrapper",
        "add_custom_checkpoint_callback",
        "extract_optimizer_scheduler_from_ptl_model",
        "init_distributed",
        "init_peft",
        "init_using_ptl",
        "retrieve_custom_trainer_state_dict",
        "temp_pop_from_config",
        "mp",
        "set_start_method",
        "force",
        "cfg",
        "pop_trainer_key",
        "OmegaConf",
        "resolve",
        "cfg",
        "temp_pop_from_config",
        "cfg",
        "trainer",
        "pop_trainer_key",
        "MegatronStableDiffusionTrainerBuilder",
        "cfg",
        "create_trainer",
        "hydra_runner",
        "config_path",
        "config_name",
        "cfg",
        "logging",
        "info",
        "logging",
        "info",
        "OmegaConf",
        "to_yaml",
        "cfg",
        "torch",
        "backends",
        "cuda",
        "matmul",
        "cfg",
        "model",
        "data",
        "train",
        "cfg",
        "model",
        "data",
        "webdataset",
        "local_root_path",
        "_",
        "cfg",
        "trainer",
        "devices",
        "cfg",
        "model",
        "data",
        "validation",
        "cfg",
        "model",
        "data",
        "webdataset",
        "local_root_path",
        "_",
        "cfg",
        "trainer",
        "devices",
        "resolve_and_create_trainer",
        "cfg",
        "exp_manager",
        "trainer",
        "cfg",
        "exp_manager",
        "CustomLoggerWrapper",
        "trainer",
        "loggers",
        "MegatronSDDRaFTPModel",
        "cfg",
        "model",
        "trainer",
        "to",
        "torch",
        "cuda",
        "current_device",
        "init_peft",
        "ptl_model",
        "cfg",
        "model",
        "trainer",
        "ckpt_path",
        "trainer_restore_path",
        "retrieve_custom_trainer_state_dict",
        "trainer",
        "custom_trainer_state_dict",
        "init_distributed",
        "trainer",
        "ptl_model",
        "cfg",
        "model",
        "get",
        "text_webdataset",
        "build_train_valid_datasets",
        "cfg",
        "model",
        "data",
        "consumed_samples",
        "consumed_samples",
        "d",
        "d",
        "train_ds",
        "d",
        "d",
        "validation_ds",
        "build_dataloader",
        "cfg",
        "dataset",
        "train_ds",
        "consumed_samples",
        "consumed_samples",
        "mbs",
        "cfg",
        "model",
        "micro_batch_size",
        "gbs",
        "cfg",
        "model",
        "global_batch_size",
        "load_gbs",
        "build_dataloader",
        "cfg",
        "dataset",
        "validation_ds",
        "consumed_samples",
        "consumed_samples",
        "mbs",
        "cfg",
        "model",
        "micro_batch_size",
        "gbs",
        "cfg",
        "model",
        "global_batch_size",
        "load_gbs",
        "init_using_ptl",
        "trainer",
        "ptl_model",
        "train_dataloader",
        "train_ds",
        "extract_optimizer_scheduler_from_ptl_model",
        "ptl_model",
        "add_custom_checkpoint_callback",
        "trainer",
        "ptl_model",
        "logger",
        "log_hyperparams",
        "OmegaConf",
        "to_container",
        "cfg",
        "get_reward_model",
        "cfg",
        "rm",
        "mbs",
        "cfg",
        "model",
        "micro_batch_size",
        "gbs",
        "cfg",
        "model",
        "global_batch_size",
        "reward_model",
        "add_custom_checkpoint_callback",
        "trainer",
        "ptl_model",
        "Timer",
        "cfg",
        "exp_manager",
        "get",
        "SupervisedTrainer",
        "cfg",
        "cfg",
        "trainer",
        "draftp_sd",
        "model",
        "ptl_model",
        "optimizer",
        "optimizer",
        "scheduler",
        "scheduler",
        "train_dataloader",
        "train_dataloader",
        "val_dataloader",
        "val_dataloader",
        "test_dataloader",
        "logger",
        "logger",
        "ckpt_callback",
        "ckpt_callback",
        "run_timer",
        "timer",
        "custom_trainer_state_dict",
        "draft_p_trainer",
        "load_state_dict",
        "custom_trainer_state_dict",
        "draft_p_trainer",
        "fit",
        "main"
    ],
    "literals": [
        "\"spawn\"",
        "\"conf\"",
        "\"draftp_sd\"",
        "\"\\n\\n************** Experiment configuration ***********\"",
        "f\"\\n{OmegaConf.to_yaml(cfg)}\"",
        "\"draftp_sd\"",
        "\"consumed_samples\"",
        "\"transformer_engine\"",
        "\"captions\"",
        "\"captions\"",
        "\"max_time_per_run\"",
        "\"0:12:00:00\"",
        "\"__main__\""
    ],
    "variables": [
        "allow_tf32",
        "dataset_path",
        "dataset_path",
        "trainer",
        "logger",
        "ptl_model",
        "trainer_restore_path",
        "custom_trainer_state_dict",
        "consumed_samples",
        "custom_trainer_state_dict",
        "consumed_samples",
        "train_ds",
        "validation_ds",
        "train_ds",
        "validation_ds",
        "train_dataloader",
        "val_dataloader",
        "optimizer",
        "scheduler",
        "ckpt_callback",
        "reward_model",
        "ptl_model",
        "reward_model",
        "ckpt_callback",
        "timer",
        "draft_p_trainer"
    ],
    "comments": [
        "Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.",
        "",
        "Licensed under the Apache License, Version 2.0 (the \"License\");",
        "you may not use this file except in compliance with the License.",
        "You may obtain a copy of the License at",
        "",
        "http://www.apache.org/licenses/LICENSE-2.0",
        "",
        "Unless required by applicable law or agreed to in writing, software",
        "distributed under the License is distributed on an \"AS IS\" BASIS,",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
        "See the License for the specific language governing permissions and",
        "limitations under the License.",
        "TODO: has to be set true for PyTorch 1.12 and later.",
        "Instatiating the model here"
    ],
    "docstrings": [
        "\"\"\"resolve the cfg, remove the key before constructing the PTL trainer\n        and then restore it after\n    \"\"\""
    ],
    "functions": [
        "resolve_and_create_trainer",
        "main"
    ],
    "classes": []
}