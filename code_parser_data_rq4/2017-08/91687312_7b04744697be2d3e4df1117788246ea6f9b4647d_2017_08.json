{
    "identifiers": [
        "gym",
        "time",
        "copy",
        "re",
        "numpy",
        "np",
        "tensorflow",
        "tf",
        "matplotlib",
        "pyplot",
        "plt",
        "cv2",
        "matplotlib",
        "Replay_Memory",
        "Replay_Memory",
        "tf",
        "logging",
        "set_verbosity",
        "tf",
        "logging",
        "INFO",
        "np",
        "dtype",
        "np",
        "uint8",
        "gym",
        "make",
        "ENVIRONMENT",
        "env",
        "reset",
        "Replay_Memory",
        "REPLAY_MEMORY_SIZE",
        "rgb",
        "np",
        "dot",
        "rgb",
        "image",
        "cv2",
        "resize",
        "image",
        "interpolation",
        "cv2",
        "INTER_LINEAR",
        "frames",
        "np",
        "stack",
        "frames",
        "step",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "name",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "name",
        "tf",
        "contrib",
        "layers",
        "conv2d",
        "input_tensor",
        "num_outputs",
        "kernel_size",
        "stride",
        "padding",
        "tf",
        "contrib",
        "layers",
        "conv2d",
        "conv_1",
        "num_outputs",
        "kernel_size",
        "stride",
        "padding",
        "tf",
        "contrib",
        "layers",
        "conv2d",
        "conv_2",
        "num_outputs",
        "kernel_size",
        "stride",
        "padding",
        "tf",
        "reshape",
        "conv_3",
        "tf",
        "contrib",
        "layers",
        "relu",
        "conv_3_flat",
        "num_outputs",
        "tf",
        "contrib",
        "layers",
        "fully_connected",
        "relu_1",
        "activation_fn",
        "num_outputs",
        "ACTIONS",
        "tf",
        "one_hot",
        "actions_tensor",
        "ACTIONS",
        "name",
        "tf",
        "multiply",
        "output",
        "actions_one_hot",
        "tf",
        "reduce_sum",
        "apply_action_mask",
        "axis",
        "tf",
        "subtract",
        "Q_of_selected_action",
        "y_tensor",
        "tf",
        "square",
        "delta",
        "tf",
        "reduce_mean",
        "loss",
        "tf",
        "train",
        "RMSPropOptimizer",
        "learning_rate",
        "LEARNING_RATE_RMSPROP",
        "momentum",
        "RMSPROP_MOMENTUM",
        "epsilon",
        "RMSPROP_EPSILON",
        "decay",
        "RMSPROP_DECAY",
        "minimize",
        "cost",
        "tf",
        "summary",
        "scalar",
        "cost",
        "tf",
        "summary",
        "scalar",
        "tf",
        "reduce_mean",
        "output",
        "tf",
        "summary",
        "merge_all",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "name",
        "tf",
        "summary",
        "scalar",
        "avg_Score_l20_plhldr",
        "output",
        "optimizer",
        "merged",
        "avg_Score_l20_plhldr",
        "avg_Score_l20",
        "tf",
        "Session",
        "sess",
        "tf",
        "set_random_seed",
        "TF_RANDOM_SEED",
        "model",
        "tf",
        "summary",
        "FileWriter",
        "LOG_DIRECTORY",
        "RUN_STRING",
        "sess",
        "graph",
        "tf",
        "train",
        "Saver",
        "tf",
        "train",
        "get_checkpoint_state",
        "SAVE_PATH",
        "RUN_STRING",
        "checkpoint",
        "checkpoint",
        "model_checkpoint_path",
        "saver",
        "restore",
        "sess",
        "checkpoint",
        "model_checkpoint_path",
        "checkpoint",
        "model_checkpoint_path",
        "re",
        "match",
        "checkpoint",
        "model_checkpoint_path",
        "group",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "np",
        "random",
        "randint",
        "NO_OP_MAX",
        "step",
        "initial_step",
        "TRAINING_STEPS",
        "env",
        "render",
        "time",
        "sleep",
        "i",
        "initial_no_op",
        "NO_OP_CODE",
        "env",
        "step",
        "action",
        "rgb2gray",
        "observation",
        "downSample",
        "greyObservation",
        "i",
        "frame_stack",
        "pop",
        "frame_stack",
        "append",
        "downObservation",
        "i",
        "stack",
        "frame_stack",
        "sess",
        "run",
        "output",
        "input_tensor",
        "np",
        "array",
        "s_t",
        "ndmin",
        "np",
        "argmax",
        "Q",
        "env",
        "step",
        "action",
        "score",
        "reward",
        "rgb2gray",
        "observation",
        "downSample",
        "greyObservation",
        "frame_stack",
        "pop",
        "frame_stack",
        "append",
        "downObservation",
        "done",
        "game",
        "game_scores",
        "append",
        "score",
        "score",
        "env",
        "reset",
        "np",
        "random",
        "randint",
        "NO_OP_MAX",
        "play"
    ],
    "literals": [
        "\"saved_networks\"",
        "\"tmp/logs/\"",
        "\"lr_0.0002,decay_0.99,momentum_0,discountRate_0.95,replayMemorySize_60000uint8,decaySteps_2000000,bias_0.1,weights_He,fast,fixedReduceSum,fixedSTPlus1\"",
        "'Breakout-v0'",
        "\"input\"",
        "\"actions\"",
        "\"r\"",
        "'SAME'",
        "'SAME'",
        "'SAME'",
        "\"actions_one_hot\"",
        "\"cost\"",
        "\"avg_Q\"",
        "\"avg_scores\"",
        "\"avg_Score_l20\"",
        "\"/\"",
        "\"Successfully loaded:\"",
        "'.*?([0-9]+)$'",
        "\"Could not find old network weights\""
    ],
    "variables": [
        "MEMORY_LENGTH",
        "ACTIONS",
        "LEARNING_RATE_SGD",
        "LEARNING_RATE_RMSPROP",
        "FINAL_EXPLORATION_FRAME",
        "TRAINING_STEPS",
        "DISCOUNT_RATE",
        "RMSPROP_MOMENTUM",
        "RMSPROP_DECAY",
        "RMSPROP_EPSILON",
        "MINIBATCH_SIZE",
        "REPLAY_MEMORY_SIZE",
        "RANDOM_STEPS_REPLAY_MEMORY_INIT",
        "SUMMARY_STEPS",
        "initial_step",
        "NO_OP_MAX",
        "SAVE_PATH",
        "LOG_DIRECTORY",
        "RUN_STRING",
        "ENVIRONMENT",
        "NO_OP_CODE",
        "TF_RANDOM_SEED",
        "env",
        "memory",
        "input_tensor",
        "actions_tensor",
        "y_tensor",
        "conv_1",
        "conv_2",
        "conv_3",
        "conv_3_flat",
        "relu_1",
        "output",
        "actions_one_hot",
        "apply_action_mask",
        "Q_of_selected_action",
        "delta",
        "loss",
        "cost",
        "optimizer",
        "cost_s",
        "avg_Q",
        "merged",
        "avg_Score_l20_plhldr",
        "avg_Score_l20",
        "output",
        "optimizer",
        "merged",
        "avg_Score_l20_plhldr",
        "avg_Score_l20",
        "summary_writer",
        "saver",
        "checkpoint",
        "initial_step",
        "initial_step",
        "score",
        "game_scores",
        "i",
        "frame_stack",
        "initial_no_op",
        "game",
        "action",
        "observation",
        "reward",
        "done",
        "info",
        "greyObservation",
        "downObservation",
        "s_t",
        "Q",
        "action",
        "observation",
        "reward",
        "done",
        "info",
        "greyObservation",
        "downObservation",
        "frame_stack",
        "score",
        "initial_no_op",
        "i"
    ],
    "comments": [
        "Placeholders could be here",
        "Huber loss with delta=1",
        "loss = tf.where(tf.abs(delta) < 1.0, 0.5 * tf.square(delta), tf.abs(delta) - 0.5)",
        "MSE",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE_SGD).minimize(cost)",
        "Summary tensors",
        "WE PERFORM A RANDOM NUMBER OF NO_OP ACTIONS",
        "CHOOSING ACTION",
        "Pick action in a greedy way",
        "Process received frame",
        "Remove oldest frame"
    ],
    "docstrings": [],
    "functions": [
        "rgb2gray",
        "downSample",
        "stack",
        "getEpsilon",
        "model",
        "play"
    ],
    "classes": []
}