{
    "identifiers": [
        "os",
        "openai",
        "json",
        "time",
        "random",
        "sequence_aligner",
        "labelset",
        "LabelSet",
        "sequence_aligner",
        "dataset",
        "TrainingDatasetCRF",
        "sequence_aligner",
        "containers",
        "TraingingBatch",
        "transformers",
        "BertTokenizerFast",
        "seqeval",
        "metrics",
        "f1_score",
        "seqeval",
        "metrics",
        "precision_score",
        "seqeval",
        "metrics",
        "accuracy_score",
        "seqeval",
        "metrics",
        "recall_score",
        "seqeval",
        "metrics",
        "classification_report",
        "seqeval",
        "scheme",
        "BILOU",
        "alist",
        "idxs",
        "idx",
        "idxs",
        "sub_list",
        "append",
        "alist",
        "idx",
        "sub_list",
        "result_list",
        "i",
        "item",
        "result_list",
        "item",
        "item",
        "sorted",
        "annos",
        "key",
        "x",
        "x",
        "indx",
        "sub_annos",
        "sorted_annos",
        "sub_annos",
        "text",
        "find",
        "value",
        "last_start",
        "start",
        "start",
        "len",
        "value",
        "start",
        "len",
        "value",
        "value_list",
        "append",
        "value",
        "start_list",
        "append",
        "start",
        "result_list",
        "data_path",
        "json",
        "load",
        "open",
        "data_path",
        "encoding",
        "item",
        "raw",
        "item",
        "item",
        "split",
        "res",
        "comp_strs",
        "res",
        "find",
        "res",
        "find",
        "index_right",
        "index_left",
        "res",
        "index_left",
        "index_right",
        "json",
        "loads",
        "res",
        "comp_anno_list",
        "append",
        "sub_json",
        "comp_anno_list",
        "compare_list",
        "append",
        "comp",
        "re_position",
        "compare_list",
        "json",
        "load",
        "open",
        "data_path",
        "encoding",
        "it",
        "pre_raw",
        "it",
        "it",
        "split",
        "res",
        "pre_strs",
        "res",
        "find",
        "res",
        "find",
        "index_right",
        "index_left",
        "res",
        "index_left",
        "index_right",
        "json",
        "loads",
        "res",
        "anno_list",
        "append",
        "sub_json",
        "anno_list",
        "result_list",
        "append",
        "result",
        "result_list",
        "re_position",
        "result_list",
        "result_list",
        "compare_list",
        "len",
        "compare_list",
        "len",
        "result_list",
        "compare_list",
        "result_list",
        "compare_list",
        "result_list",
        "idx",
        "item",
        "result_list",
        "item",
        "a",
        "annos",
        "a",
        "item",
        "idx",
        "item",
        "result_list",
        "item",
        "x",
        "x",
        "annos",
        "x",
        "example",
        "result_list",
        "annotation",
        "example",
        "annotation",
        "idx",
        "item",
        "compare_list",
        "item",
        "a",
        "annos",
        "a",
        "item",
        "idx",
        "item",
        "compare_list",
        "item",
        "x",
        "x",
        "annos",
        "x",
        "example",
        "compare_list",
        "annotation",
        "example",
        "annotation",
        "compare_list",
        "result_list",
        "data_path",
        "compare_list",
        "result_list",
        "data_path",
        "replace",
        "replace",
        "open",
        "error_path",
        "encoding",
        "file",
        "compare_list",
        "index",
        "item1",
        "item2",
        "compare_list",
        "result_list",
        "sorted",
        "item1",
        "key",
        "x",
        "x",
        "item1",
        "item2",
        "error_index_list",
        "append",
        "index",
        "file",
        "write",
        "item1",
        "file",
        "write",
        "item1",
        "file",
        "write",
        "item2",
        "file",
        "write",
        "error_path",
        "error_index_list",
        "data_path",
        "error_index_list",
        "json",
        "load",
        "open",
        "data_path",
        "encoding",
        "index2",
        "item",
        "raw_data",
        "index2",
        "error_index_list",
        "compare_list",
        "append",
        "item",
        "random",
        "random",
        "rate",
        "compare_list",
        "append",
        "item",
        "data_path",
        "replace",
        "rate",
        "open",
        "train_pre_save_path",
        "encoding",
        "json_file",
        "json",
        "dump",
        "compare_list",
        "json_file",
        "ensure_ascii",
        "indent",
        "train_pre_save_path",
        "data_path",
        "compare_list",
        "result_list",
        "BertTokenizerFast",
        "from_pretrained",
        "data_path",
        "LabelSet",
        "labels",
        "data_path",
        "LabelSet",
        "labels",
        "data_path",
        "LabelSet",
        "labels",
        "TrainingDatasetCRF",
        "data",
        "compare_list",
        "tokenizer",
        "tokenizer",
        "label_set",
        "label_set",
        "tokens_per_batch",
        "TrainingDatasetCRF",
        "data",
        "result_list",
        "tokenizer",
        "tokenizer",
        "label_set",
        "label_set",
        "tokens_per_batch",
        "i",
        "len",
        "dataset",
        "m",
        "dataset",
        "i",
        "labels",
        "m",
        "sub_list",
        "append",
        "label_set",
        "ids_to_label",
        "m",
        "i",
        "dataset",
        "i",
        "dataset",
        "i",
        "labels",
        "sub_list",
        "n",
        "dataset_openai",
        "i",
        "labels",
        "n",
        "n",
        "pred_sub_list",
        "append",
        "label_set",
        "ids_to_label",
        "n",
        "label_list",
        "append",
        "sub_list",
        "pred_label_list",
        "append",
        "pred_sub_list",
        "classification_report",
        "label_list",
        "pred_label_list",
        "mode",
        "scheme",
        "BILOU",
        "report",
        "get_compare",
        "data_path",
        "check_anno",
        "compare_list",
        "result_list",
        "error_analyze",
        "data_path",
        "compare_list",
        "result_list",
        "contral_error_rate",
        "data_path",
        "error_index_list",
        "report",
        "data_path",
        "compare_list",
        "result_list"
    ],
    "literals": [
        "'content'",
        "'annotations'",
        "'start'",
        "'value'",
        "'start'",
        "'end'",
        "\"utf-8\"",
        "\"content\"",
        "\"content\"",
        "\"output\"",
        "'\\n'",
        "'{'",
        "'}'",
        "\"annotations\"",
        "\"utf-8\"",
        "\"content\"",
        "\"content\"",
        "\"pre\"",
        "'\\n'",
        "'{'",
        "'}'",
        "\"annotations\"",
        "\"LLM预测结果中annotations：\"",
        "\"annotations\"",
        "\"修正起始位置后中annotations：\"",
        "\"annotations\"",
        "\"测试集中真实的 annotations：\"",
        "\"annotations\"",
        "'annotations'",
        "'start'",
        "\"result_list:\"",
        "'annotations'",
        "'annotations'",
        "'start'",
        "'annotations'",
        "'label'",
        "'tag'",
        "'annotations'",
        "'start'",
        "\"compare_list:\"",
        "'annotations'",
        "'annotations'",
        "'start'",
        "'annotations'",
        "'label'",
        "'tag'",
        "\"\\n\\n\\n#################################错误样本分析#####################################\\n\\n\\n\"",
        "\"save_pres\"",
        "\"error_analyze_save_pres\"",
        "\".json\"",
        "\".txt\"",
        "'w'",
        "'UTF-8'",
        "'annotations'",
        "'annotations'",
        "'start'",
        "'annotations'",
        "'annotations'",
        "'content'",
        "\"\\n\"",
        "\"真实标签:\"",
        "'annotations'",
        "\"\\n\"",
        "\"预测标签:\"",
        "'annotations'",
        "\"\\n\"",
        "\"\\n\"",
        "\"错误样本已写入\"",
        "\"文件中\"",
        "\"utf-8\"",
        "\"save_pres\"",
        "\"train_pre_\"",
        "'w'",
        "'utf-8'",
        "\"去除部分正确样本后的训练集预测数据，已成功写入\"",
        "\"文件中\"",
        "'finbert'",
        "'FinEntity'",
        "\"Neutral\"",
        "\"Positive\"",
        "\"Negative\"",
        "'SEntFiN'",
        "\"neutral\"",
        "\"positive\"",
        "\"negative\"",
        "'EFSA'",
        "\"中立\"",
        "\"正面\"",
        "\"负面\"",
        "'strict'",
        "\"../train_pre/EFSA/save_pres_EFSA_2024-07-08-18-58-04.json\""
    ],
    "variables": [
        "sub_list",
        "text",
        "annos",
        "sorted_annos",
        "value_list",
        "start_list",
        "last_start",
        "drop_list",
        "value",
        "start",
        "sub_annos",
        "sub_annos",
        "last_start",
        "raw",
        "compare_list",
        "comp",
        "comp",
        "comp_anno_list",
        "comp_strs",
        "index_left",
        "index_right",
        "res",
        "sub_json",
        "comp",
        "compare_list",
        "pre_raw",
        "result_list",
        "result",
        "result",
        "anno_list",
        "pre_strs",
        "index_left",
        "index_right",
        "res",
        "sub_json",
        "result",
        "result_list",
        "annos",
        "annos",
        "drop_list",
        "item",
        "annotation",
        "annos",
        "annos",
        "drop_list",
        "item",
        "annotation",
        "error_index_list",
        "error_path",
        "item1",
        "raw_data",
        "compare_list",
        "rate",
        "train_pre_save_path",
        "tokenizer",
        "label_set",
        "label_set",
        "label_set",
        "dataset",
        "dataset_openai",
        "label_list",
        "pred_label_list",
        "sub_list",
        "pred_sub_list",
        "n",
        "report",
        "data_path",
        "compare_list",
        "result_list",
        "compare_list",
        "result_list",
        "error_index_list"
    ],
    "comments": [
        "%%",
        "from util.process import ids_to_labels,Metrics,Metrics_e",
        "Correcting start and end tags    对 result_list 中的每个元素进行处理，修正起始和结束标签。",
        "代码确保了注释在文本中的位置不重叠，并且更新了注释的起始位置和结束位置",
        "{}{}",
        "按start值对每组排序",
        "对于每组{\"start\": 74, \"end\": 84, \"value\": \"Reddit Inc\", \"tag\": \"Neutral\"}",
        "\"value\": \"Reddit Inc\"",
        "if value not in value_list:",
        "start = text.find(value)",
        "else:",
        "index_list = []",
        "for j, v in enumerate(value_list):",
        "if v == value:",
        "index_list.append(j)",
        "sub_start = subset(start_list, index_list)",
        "last_start = max(sub_start)  #使用 max(sub_start) 来找到子列表中的最大值，即之前相同值的最后一个起始位置，然后将其加1作为新的起始位置。",
        "start = text.find(value, last_start + 1)  #保证新的注释不会与之前的注释重叠",
        "存储真实标签",
        "对于每一个实体组 {\"start\": 74, \"end\": 84, \"value\": \"Reddit Inc\", \"tag\": \"Neutral\"}",
        "有的句子中含有非法字符，需要重新确定一下位置，以与预测标签可以对应。",
        "pre_raw = json.load(open(data_path+'save_pres_FinEntity_2024-04-03-21-02-48.json'))",
        "存储预测标签",
        "pre_strs = it[\"output\"].split('\\n')",
        "对于每一个实体组 {\"start\": 74, \"end\": 84, \"value\": \"Reddit Inc\", \"tag\": \"Neutral\"}",
        "%%找到 result_list 中所有具有负起始位置的注释，然后打印包含这些注释的整个字典项，以便进行进一步的检查或处理",
        "%%只保留起始位置大于等于 0 的组",
        "We expect the key of label to be label but the data has tag",
        "%%找到 result_list 中所有具有负起始位置的注释，然后打印包含这些注释的整个字典项，以便进行进一步的检查或处理",
        "%%只保留起始位置大于等于 0 的组 , 原数据集中有一条数据的二元组有重复导致reposition后x['start']=-1，导致报错，compare_list经过此函数就可以不报错。",
        "We expect the key of label to be label but the data has tag",
        "print(compare_list[0])",
        "print(result_list[0])",
        "打开一个txt文件用于写入",
        "print(str(item1['content']))",
        "print(\"真实标签:\", item1['annotations'])",
        "print(\"预测标签:\", item2['annotations'])",
        "print()",
        "存储真实标签",
        "将 compare_list 写入 JSON 文件",
        "计算得分",
        "tokenizer = BertTokenizerFast.from_pretrained('yiyanghkust/finbert-pretrain')",
        "label in FinEntity",
        "label in SEntFiN",
        "label in EFSA",
        "data_path = \"../IstructABSA_result/finentity/save_pres_FinEntity_result_eval.json\"",
        "data_path = \"../IstructABSA_result/sentfin/save_pres_SEntFiN_result_eval.json\"",
        "data_path = \"../train_pre/SEntFiN/save_pres_SEntFiN_2024-07-08-17-30-08.json\"",
        "data_path = \"../train_pre/FinEntity/save_pres_FinEntity_2024-07-10-10-36-28.json\"",
        "data_path = \"../correct_result/SEntFiN/save_pres_SEntFiN_correct_2024-07-10-03-03-26.json\"",
        "data_path = \"../correct_result/FinEntity/save_pres_FinEntity_correct_2024-07-10-12-00-17.json\"",
        "获取真实标签和预测标签并修正位置",
        "检查不合规的位置信息，只保留起始位置大于等于 0 的组",
        "错误分析,将错误的样本写入txt文件",
        "随机删除部分正确样本，使训练集错误率和测试集相当",
        "报告序列标注得分"
    ],
    "docstrings": [],
    "functions": [
        "subset",
        "re_position",
        "get_compare",
        "check_anno",
        "error_analyze",
        "contral_error_rate",
        "report"
    ],
    "classes": []
}