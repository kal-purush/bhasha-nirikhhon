{
    "identifiers": [
        "gc",
        "os",
        "sys",
        "warnings",
        "threading",
        "abc",
        "ABCMeta",
        "abstractmethod",
        "format_stack",
        "format_exc",
        "my_exceptions",
        "WorkerInterrupt",
        "TransportableException",
        "_multiprocessing_helpers",
        "mp",
        "_compat",
        "with_metaclass",
        "mp",
        "pool",
        "MemmapingPool",
        "multiprocessing",
        "pool",
        "ThreadPool",
        "with_metaclass",
        "ABCMeta",
        "abstractmethod",
        "n_jobs",
        "abstractmethod",
        "func",
        "callback",
        "n_jobs",
        "parallel",
        "backend_args",
        "parallel",
        "effective_n_jobs",
        "n_jobs",
        "batch_size",
        "duration",
        "ensure_ready",
        "ParallelBackendBase",
        "n_jobs",
        "n_jobs",
        "ValueError",
        "func",
        "callback",
        "ImmediateResult",
        "func",
        "callback",
        "callback",
        "result",
        "result",
        "n_jobs",
        "n_jobs",
        "ValueError",
        "mp",
        "n_jobs",
        "n_jobs",
        "max",
        "mp",
        "cpu_count",
        "n_jobs",
        "n_jobs",
        "_pool",
        "_pool",
        "close",
        "_pool",
        "terminate",
        "func",
        "callback",
        "_pool",
        "apply_async",
        "SafeFunction",
        "func",
        "callback",
        "callback",
        "ensure_ready",
        "terminate",
        "ensure_ready",
        "configure",
        "n_jobs",
        "parallel",
        "n_jobs",
        "parallel",
        "parallel",
        "parallel",
        "_backend_args",
        "_effective_batch_size",
        "_smoothed_batch_duration",
        "batch_duration",
        "batch_duration",
        "MIN_IDEAL_BATCH_DURATION",
        "old_batch_size",
        "MIN_IDEAL_BATCH_DURATION",
        "batch_duration",
        "max",
        "ideal_batch_size",
        "batch_size",
        "parallel",
        "verbose",
        "parallel",
        "_print",
        "batch_duration",
        "batch_size",
        "batch_duration",
        "MAX_IDEAL_BATCH_DURATION",
        "old_batch_size",
        "old_batch_size",
        "batch_size",
        "parallel",
        "verbose",
        "parallel",
        "_print",
        "batch_duration",
        "batch_size",
        "old_batch_size",
        "batch_size",
        "old_batch_size",
        "batch_size",
        "batch_size",
        "duration",
        "batch_size",
        "_effective_batch_size",
        "_smoothed_batch_duration",
        "old_duration",
        "duration",
        "old_duration",
        "duration",
        "new_duration",
        "PoolManagerMixin",
        "ParallelBackendBase",
        "n_jobs",
        "parallel",
        "backend_args",
        "effective_n_jobs",
        "n_jobs",
        "n_jobs",
        "FallbackToBackend",
        "SequentialBackend",
        "parallel",
        "ThreadPool",
        "n_jobs",
        "n_jobs",
        "PoolManagerMixin",
        "AutoBatchingMixin",
        "ParallelBackendBase",
        "n_jobs",
        "mp",
        "current_process",
        "daemon",
        "warnings",
        "warn",
        "stacklevel",
        "threading",
        "current_thread",
        "name",
        "warnings",
        "warn",
        "stacklevel",
        "MultiprocessingBackend",
        "effective_n_jobs",
        "n_jobs",
        "n_jobs",
        "parallel",
        "backend_args",
        "effective_n_jobs",
        "n_jobs",
        "n_jobs",
        "FallbackToBackend",
        "SequentialBackend",
        "os",
        "environ",
        "get",
        "JOBLIB_SPAWNED_PROCESS",
        "already_forked",
        "ImportError",
        "os",
        "environ",
        "JOBLIB_SPAWNED_PROCESS",
        "gc",
        "collect",
        "MemmapingPool",
        "n_jobs",
        "backend_args",
        "parallel",
        "n_jobs",
        "MultiprocessingBackend",
        "terminate",
        "JOBLIB_SPAWNED_PROCESS",
        "os",
        "environ",
        "os",
        "environ",
        "JOBLIB_SPAWNED_PROCESS",
        "batch",
        "batch",
        "results",
        "func",
        "func",
        "args",
        "kwargs",
        "func",
        "args",
        "kwargs",
        "KeyboardInterrupt",
        "WorkerInterrupt",
        "sys",
        "exc_info",
        "format_exc",
        "e_type",
        "e_value",
        "e_tb",
        "context",
        "tb_offset",
        "TransportableException",
        "text",
        "e_type",
        "backend",
        "backend"
    ],
    "literals": [
        "'n_jobs == 0 in Parallel has no meaning'",
        "'n_jobs == 0 in Parallel has no meaning'",
        "\"Batch computation too fast (%.4fs.) \"",
        "\"Setting batch_size=%d.\"",
        "\"Batch computation too slow (%.4fs.) \"",
        "\"Setting batch_size=%d.\"",
        "\"__JOBLIB_SPAWNED_PARALLEL__\"",
        "'Multiprocessing-backed parallel loops cannot be nested,'",
        "' setting n_jobs=1'",
        "'MainThread'",
        "'Multiprocessing backed parallel loops cannot be nested'",
        "' below threads, setting n_jobs=1'",
        "'[joblib] Attempting to do parallel computing '",
        "'without protecting your import on a system that does '",
        "'not support forking. To use parallel-computing in a '",
        "'script, you must protect your main loop using \"if '",
        "\"__name__ == '__main__'\"",
        "'\". Please see the joblib documentation on Parallel '",
        "'for more information'",
        "'1'"
    ],
    "variables": [
        "parallel",
        "result",
        "n_jobs",
        "_pool",
        "MIN_IDEAL_BATCH_DURATION",
        "MAX_IDEAL_BATCH_DURATION",
        "_effective_batch_size",
        "_smoothed_batch_duration",
        "old_batch_size",
        "batch_duration",
        "ideal_batch_size",
        "batch_size",
        "_effective_batch_size",
        "batch_size",
        "_effective_batch_size",
        "batch_size",
        "_smoothed_batch_duration",
        "old_duration",
        "new_duration",
        "new_duration",
        "_smoothed_batch_duration",
        "n_jobs",
        "parallel",
        "_pool",
        "JOBLIB_SPAWNED_PROCESS",
        "n_jobs",
        "already_forked",
        "_pool",
        "parallel",
        "results",
        "func",
        "e_type",
        "e_value",
        "e_tb",
        "text",
        "backend"
    ],
    "comments": [
        "Does nothing by default: to be overriden in subclasses when canceling",
        "tasks is possible.",
        "multiprocessing is not available or disabled, fallback",
        "to sequential mode",
        "terminate does a join()",
        "In seconds, should be big enough to hide multiprocessing dispatching",
        "overhead.",
        "This settings was found by running benchmarks/bench_auto_batching.py",
        "with various parameters on various platforms.",
        "Should not be too high to avoid stragglers: long jobs running alone",
        "on a single worker while other workers have no work to process any more.",
        "Batching counters",
        "The current batch size is too small: the duration of the",
        "processing of a batch of task is not large enough to hide",
        "the scheduling overhead.",
        "Multiply by two to limit oscilations between min and max.",
        "The current batch size is too big. If we schedule overly long",
        "running batches some CPUs might wait with nothing left to do",
        "while a couple of CPUs a left processing a few long running",
        "batches. Better reduce the batch size a bit to limit the",
        "likelihood of scheduling such stragglers.",
        "No batch size adjustment",
        "Reset estimation of the smoothed mean batch duration: this",
        "estimate is updated in the multiprocessing apply_async",
        "CallBack as long as the batch_size is constant. Therefore",
        "we need to reset the estimate whenever we re-tune the batch",
        "size.",
        "Update the smoothed streaming estimate of the duration of a batch",
        "from dispatch to completion",
        "First record of duration for this batch size after the last",
        "reset.",
        "Update the exponentially weighted average of the duration of",
        "batch for the current effective size.",
        "Avoid unnecessary overhead and use sequential backend instead.",
        "Environment variables to protect against bad situations when nesting",
        "Daemonic processes cannot have children",
        "Prevent posix fork inside in non-main posix threads",
        "Set an environment variable to avoid infinite loops",
        "Make sure to free as much memory as possible before forking",
        "Don't delay the application, to avoid keeping the input",
        "arguments in memory",
        "We capture the KeyboardInterrupt and reraise it as",
        "something different, as multiprocessing does not",
        "interrupt processing for a KeyboardInterrupt"
    ],
    "docstrings": [
        "\"\"\"\nBackends for embarrassingly parallel code.\n\"\"\"",
        "\"\"\"Helper abc which defines all methods a ParallelBackend must implement\"\"\"",
        "\"\"\"Determine the number of jobs that can actually run in parallel\n\n        n_jobs is the is the number of workers requested by the callers.\n        Passing n_jobs=-1 means requesting all available workers for instance\n        matching the number of CPU cores on the worker host(s).\n\n        This method should return a guesstimate of the number of workers that\n        can actually perform work concurrently. The primary use case is to make\n        it possible for the caller to know in how many chunks to slice the\n        work.\n\n        In general working on larger data chunks is more efficient (less\n        scheduling overhead and better use of CPU cache prefetching heuristics)\n        as long as all the workers have enough work to do.\n        \"\"\"",
        "\"\"\"Schedule a func to be run\"\"\"",
        "\"\"\"Reconfigure the backend and return the number of workers.\n\n        This makes it possible to reuse an existing backend instance for\n        successive independent calls to Parallel with different parameters.\n        \"\"\"",
        "\"\"\"Shutdown the process or thread pool\"\"\"",
        "\"\"\"Determine the optimal batch size\"\"\"",
        "\"\"\"Callback indicate how long it took to run a batch\"\"\"",
        "\"\"\"List of exception types to be captured.\"\"\"",
        "\"\"\"Abort any running tasks\n\n        This is called when an exception has been raised when executing a tasks\n        and all the remaining tasks will be ignored and can therefore be\n        aborted to spare computation resources.\n\n        If ensure_ready is True, the backend should be left in an operating\n        state as future tasks might be re-submitted via that same backend\n        instance.\n\n        If ensure_ready is False, the implementer of this method can decide\n        to leave the backend in a closed / terminated state as no new task\n        are expected to be submitted to this backend.\n\n        Setting ensure_ready to False is an optimization that can be leveraged\n        when aborting tasks via killing processes from a local process pool\n        managed by the backend it-self: if we expect no new tasks, there is no\n        point in re-creating a new working pool.\n        \"\"\"",
        "\"\"\"A ParallelBackend which will execute all batches sequentially.\n\n    Does not use/create any threading objects, and hence has minimal\n    overhead. Used when n_jobs == 1.\n    \"\"\"",
        "\"\"\"Determine the number of jobs which are going to run in parallel\"\"\"",
        "\"\"\"Schedule a func to be run\"\"\"",
        "\"\"\"A helper class for managing pool of workers.\"\"\"",
        "\"\"\"Determine the number of jobs which are going to run in parallel\"\"\"",
        "\"\"\"Shutdown the process or thread pool\"\"\"",
        "\"\"\"Schedule a func to be run\"\"\"",
        "\"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"",
        "\"\"\"A helper class for automagically batching jobs.\"\"\"",
        "\"\"\"Determine the optimal batch size\"\"\"",
        "\"\"\"Callback indicate how long it took to run a batch\"\"\"",
        "\"\"\"A ParallelBackend which will use a thread pool to execute batches in.\n\n    This is a low-overhead backend but it suffers from the Python Global\n    Interpreter Lock if the called function relies a lot on Python objects.\n    Mostly useful when the execution bottleneck is a compiled extension that\n    explicitly releases the GIL (for instance a Cython loop wrapped in a\n    \"with nogil\" block or an expensive call to a library such as NumPy).\n    \"\"\"",
        "\"\"\"Build a process or thread pool and return the number of workers\"\"\"",
        "\"\"\"A ParallelBackend which will use a multiprocessing.Pool.\n\n    Will introduce some communication and memory overhead when exchanging\n    input and output data with the with the worker Python processes.\n    However, does not suffer from the Python Global Interpreter Lock.\n    \"\"\"",
        "\"\"\"Determine the number of jobs which are going to run in parallel.\n\n        This also checks if we are attempting to create a nested parallel\n        loop.\n        \"\"\"",
        "\"\"\"Build a process or thread pool and return the number of workers\"\"\"",
        "\"\"\"Shutdown the process or thread pool\"\"\"",
        "\"\"\"Wrapper that handles the serialization of exception tracebacks.\n\n    If an exception is triggered when calling the inner function, a copy of\n    the full traceback is captured to make it possible to serialize\n    it so that it can be rendered in a different Python process.\n    \"\"\"",
        "\"\"\"Raised when configuration should fallback to another backend\"\"\""
    ],
    "functions": [
        "effective_n_jobs",
        "apply_async",
        "configure",
        "terminate",
        "compute_batch_size",
        "batch_completed",
        "get_exceptions",
        "abort_everything",
        "effective_n_jobs",
        "apply_async",
        "effective_n_jobs",
        "terminate",
        "apply_async",
        "abort_everything",
        "compute_batch_size",
        "batch_completed",
        "configure",
        "effective_n_jobs",
        "configure",
        "terminate",
        "get",
        "__call__"
    ],
    "classes": [
        "ParallelBackendBase",
        "SequentialBackend",
        "PoolManagerMixin",
        "AutoBatchingMixin",
        "ThreadingBackend",
        "MultiprocessingBackend",
        "ImmediateResult",
        "SafeFunction",
        "FallbackToBackend"
    ]
}