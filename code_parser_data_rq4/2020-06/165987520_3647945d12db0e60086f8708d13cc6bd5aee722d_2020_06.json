{
    "identifiers": [
        "typing",
        "Dict",
        "Optional",
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "pytext",
        "config",
        "module_config",
        "Activation",
        "pytext",
        "contrib",
        "pytext_lib",
        "resources",
        "pytext",
        "contrib",
        "pytext_lib",
        "resources",
        "ROBERTA_BASE_TORCH",
        "XLMR_BASE",
        "XLMR_DUMMY",
        "pytext",
        "loss",
        "BinaryCrossEntropyLoss",
        "pytext",
        "models",
        "representations",
        "transformer",
        "MultiheadSelfAttention",
        "Transformer",
        "TransformerLayer",
        "pytext",
        "models",
        "representations",
        "transformer",
        "sentence_encoder",
        "translate_roberta_state_dict",
        "pytext",
        "optimizer",
        "get_activation",
        "pytext",
        "utils",
        "file_io",
        "PathManager",
        "torch",
        "serialization",
        "default_restore_location",
        "nn",
        "Module",
        "vocab_size",
        "embedding_dim",
        "num_attention_heads",
        "num_encoder_layers",
        "output_dropout",
        "model_path",
        "Optional",
        "Transformer",
        "vocab_size",
        "vocab_size",
        "embedding_dim",
        "embedding_dim",
        "layers",
        "TransformerLayer",
        "embedding_dim",
        "embedding_dim",
        "attention",
        "MultiheadSelfAttention",
        "embedding_dim",
        "num_attention_heads",
        "_",
        "num_encoder_layers",
        "nn",
        "Dropout",
        "output_dropout",
        "apply",
        "init_params",
        "model_path",
        "PathManager",
        "open",
        "model_path",
        "f",
        "torch",
        "load",
        "f",
        "map_location",
        "s",
        "l",
        "default_restore_location",
        "s",
        "roberta_state",
        "translate_roberta_state_dict",
        "roberta_state",
        "load_state_dict",
        "roberta_state",
        "tokens",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "transformer",
        "tokens",
        "all_layers",
        "transpose",
        "last_layer",
        "output_dropout",
        "sentence_rep",
        "isinstance",
        "torch",
        "nn",
        "Linear",
        "weight",
        "data",
        "normal_",
        "mean",
        "std",
        "bias",
        "bias",
        "data",
        "zero_",
        "isinstance",
        "torch",
        "nn",
        "Embedding",
        "weight",
        "data",
        "normal_",
        "mean",
        "std",
        "padding_idx",
        "weight",
        "data",
        "padding_idx",
        "zero_",
        "nn",
        "Module",
        "in_dim",
        "out_dim",
        "bias",
        "hidden_dims",
        "activation",
        "Activation",
        "Activation",
        "RELU",
        "dim",
        "hidden_dims",
        "layers",
        "append",
        "nn",
        "Linear",
        "in_dim",
        "dim",
        "bias",
        "layers",
        "append",
        "get_activation",
        "activation",
        "dim",
        "layers",
        "append",
        "nn",
        "Linear",
        "in_dim",
        "out_dim",
        "bias",
        "nn",
        "Sequential",
        "layers",
        "representation",
        "torch",
        "Tensor",
        "dense",
        "Optional",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "dense",
        "torch",
        "cat",
        "representation",
        "dense",
        "mlp",
        "representation",
        "nn",
        "Module",
        "encoder",
        "nn",
        "Module",
        "decoder",
        "nn",
        "Module",
        "encoder",
        "decoder",
        "inputs",
        "Dict",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "inputs",
        "Optional",
        "torch",
        "Tensor",
        "inputs",
        "inputs",
        "encoder",
        "tokens",
        "decoder",
        "representation",
        "dense",
        "dense",
        "nn",
        "Module",
        "label_weights",
        "Optional",
        "Dict",
        "loss",
        "loss",
        "BinaryCrossEntropyLoss",
        "BinaryCrossEntropyLoss",
        "Config",
        "logits",
        "torch",
        "max",
        "logits",
        "F",
        "logsigmoid",
        "logits",
        "preds",
        "scores",
        "logits",
        "targets",
        "reduce",
        "loss",
        "logits",
        "targets",
        "reduce",
        "reduce",
        "RobertaModel",
        "encoder",
        "nn",
        "Module",
        "decoder",
        "nn",
        "Module",
        "encoder",
        "decoder",
        "BinaryClassificationHead",
        "logits",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "head",
        "logits",
        "logits",
        "targets",
        "head",
        "get_loss",
        "logits",
        "targets",
        "model_path",
        "dense_dim",
        "embedding_dim",
        "out_dim",
        "vocab_size",
        "num_attention_heads",
        "num_encoder_layers",
        "output_dropout",
        "bias",
        "RoBERTaEncoder",
        "vocab_size",
        "vocab_size",
        "embedding_dim",
        "embedding_dim",
        "num_attention_heads",
        "num_attention_heads",
        "num_encoder_layers",
        "num_encoder_layers",
        "output_dropout",
        "output_dropout",
        "model_path",
        "model_path",
        "MLPDecoder",
        "in_dim",
        "embedding_dim",
        "dense_dim",
        "out_dim",
        "out_dim",
        "bias",
        "bias",
        "RobertaModelForBinaryDocClassification",
        "encoder",
        "encoder",
        "decoder",
        "decoder",
        "model",
        "pretrained",
        "resources",
        "models",
        "URL",
        "ROBERTA_BASE_TORCH",
        "pretrained",
        "build_model",
        "model_path",
        "model_path",
        "dense_dim",
        "embedding_dim",
        "out_dim",
        "vocab_size",
        "num_attention_heads",
        "num_encoder_layers",
        "output_dropout",
        "pretrained",
        "resources",
        "models",
        "URL",
        "XLMR_BASE",
        "pretrained",
        "build_model",
        "model_path",
        "model_path",
        "dense_dim",
        "embedding_dim",
        "out_dim",
        "vocab_size",
        "num_attention_heads",
        "num_encoder_layers",
        "output_dropout",
        "pretrained",
        "resources",
        "models",
        "URL",
        "XLMR_DUMMY",
        "pretrained",
        "build_model",
        "model_path",
        "model_path",
        "dense_dim",
        "embedding_dim",
        "out_dim",
        "vocab_size",
        "num_attention_heads",
        "num_encoder_layers",
        "output_dropout"
    ],
    "literals": [
        "\"rb\"",
        "\"cpu\"",
        "\"model\"",
        "\"model\"",
        "\"token_ids\"",
        "\"dense\"",
        "\"dense\""
    ],
    "variables": [
        "transformer",
        "output_dropout",
        "roberta_state",
        "roberta_state",
        "all_layers",
        "last_layer",
        "sentence_rep",
        "layers",
        "in_dim",
        "mlp",
        "representation",
        "encoder",
        "decoder",
        "tokens",
        "dense",
        "representation",
        "loss",
        "preds",
        "scores",
        "head",
        "encoder",
        "decoder",
        "model",
        "model_path",
        "model_path",
        "model_path"
    ],
    "comments": [
        "!/usr/bin/env python3",
        "Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved",
        "lists of T x B x C"
    ],
    "docstrings": [
        "\"\"\"Initialize the RoBERTa weights for pre-training from scratch.\"\"\""
    ],
    "functions": [
        "forward",
        "init_params",
        "forward",
        "forward",
        "forward",
        "get_loss",
        "get_pred",
        "get_loss",
        "build_model",
        "roberta_base_binary_doc_classifier",
        "xlmr_base_binary_doc_classifier",
        "xlmr_dummy_binary_doc_classifier"
    ],
    "classes": [
        "RoBERTaEncoder",
        "MLPDecoder",
        "RobertaModel",
        "BinaryClassificationHead",
        "RobertaModelForBinaryDocClassification"
    ]
}