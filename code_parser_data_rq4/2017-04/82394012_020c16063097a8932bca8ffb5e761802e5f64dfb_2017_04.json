{
    "identifiers": [
        "print_function",
        "keras",
        "models",
        "Sequential",
        "Model",
        "keras",
        "layers",
        "embeddings",
        "Embedding",
        "keras",
        "layers",
        "Input",
        "Activation",
        "Dense",
        "Permute",
        "Dropout",
        "add",
        "dot",
        "concatenate",
        "keras",
        "layers",
        "LSTM",
        "keras",
        "utils",
        "data_utils",
        "get_file",
        "keras",
        "preprocessing",
        "sequence",
        "pad_sequences",
        "functools",
        "reduce",
        "tarfile",
        "numpy",
        "np",
        "re",
        "sent",
        "x",
        "strip",
        "x",
        "re",
        "split",
        "sent",
        "x",
        "strip",
        "lines",
        "only_supporting",
        "line",
        "lines",
        "line",
        "decode",
        "strip",
        "line",
        "split",
        "nid",
        "nid",
        "line",
        "line",
        "split",
        "tokenize",
        "q",
        "only_supporting",
        "supporting",
        "split",
        "story",
        "i",
        "i",
        "supporting",
        "x",
        "x",
        "story",
        "x",
        "data",
        "append",
        "substory",
        "q",
        "a",
        "story",
        "append",
        "tokenize",
        "line",
        "story",
        "append",
        "sent",
        "data",
        "f",
        "only_supporting",
        "max_length",
        "parse_stories",
        "f",
        "readlines",
        "only_supporting",
        "only_supporting",
        "data",
        "reduce",
        "x",
        "y",
        "x",
        "y",
        "data",
        "flatten",
        "story",
        "q",
        "answer",
        "story",
        "q",
        "answer",
        "data",
        "max_length",
        "len",
        "flatten",
        "story",
        "max_length",
        "data",
        "data",
        "word_idx",
        "story_maxlen",
        "query_maxlen",
        "story",
        "query",
        "answer",
        "data",
        "word_idx",
        "w",
        "w",
        "story",
        "word_idx",
        "w",
        "w",
        "query",
        "np",
        "zeros",
        "len",
        "word_idx",
        "word_idx",
        "answer",
        "X",
        "append",
        "x",
        "Xq",
        "append",
        "xq",
        "Y",
        "append",
        "y",
        "pad_sequences",
        "X",
        "maxlen",
        "story_maxlen",
        "pad_sequences",
        "Xq",
        "maxlen",
        "query_maxlen",
        "np",
        "array",
        "Y",
        "get_file",
        "origin",
        "tarfile",
        "open",
        "path",
        "challenges",
        "challenge_type",
        "challenge_type",
        "get_stories",
        "tar",
        "extractfile",
        "challenge",
        "format",
        "get_stories",
        "tar",
        "extractfile",
        "challenge",
        "format",
        "story",
        "q",
        "answer",
        "train_stories",
        "test_stories",
        "vocab",
        "story",
        "q",
        "answer",
        "sorted",
        "vocab",
        "len",
        "vocab",
        "max",
        "len",
        "x",
        "x",
        "_",
        "_",
        "train_stories",
        "test_stories",
        "max",
        "len",
        "x",
        "_",
        "x",
        "_",
        "train_stories",
        "test_stories",
        "vocab_size",
        "story_maxlen",
        "query_maxlen",
        "len",
        "train_stories",
        "len",
        "test_stories",
        "train_stories",
        "c",
        "i",
        "i",
        "c",
        "vocab",
        "vectorize_stories",
        "train_stories",
        "word_idx",
        "story_maxlen",
        "query_maxlen",
        "vectorize_stories",
        "test_stories",
        "word_idx",
        "story_maxlen",
        "query_maxlen",
        "inputs_train",
        "shape",
        "inputs_test",
        "shape",
        "queries_train",
        "shape",
        "queries_test",
        "shape",
        "answers_train",
        "shape",
        "answers_test",
        "shape",
        "Input",
        "story_maxlen",
        "Input",
        "query_maxlen",
        "Sequential",
        "input_encoder_m",
        "add",
        "Embedding",
        "input_dim",
        "vocab_size",
        "output_dim",
        "input_encoder_m",
        "add",
        "Dropout",
        "Sequential",
        "input_encoder_c",
        "add",
        "Embedding",
        "input_dim",
        "vocab_size",
        "output_dim",
        "query_maxlen",
        "input_encoder_c",
        "add",
        "Dropout",
        "Sequential",
        "question_encoder",
        "add",
        "Embedding",
        "input_dim",
        "vocab_size",
        "output_dim",
        "input_length",
        "query_maxlen",
        "question_encoder",
        "add",
        "Dropout",
        "input_encoder_m",
        "input_sequence",
        "input_encoder_c",
        "input_sequence",
        "question_encoder",
        "question",
        "dot",
        "input_encoded_m",
        "question_encoded",
        "axes",
        "Activation",
        "match",
        "add",
        "match",
        "input_encoded_c",
        "Permute",
        "response",
        "concatenate",
        "response",
        "question_encoded",
        "LSTM",
        "answer",
        "Dropout",
        "answer",
        "Dense",
        "vocab_size",
        "answer",
        "Activation",
        "answer",
        "Model",
        "input_sequence",
        "question",
        "answer",
        "model",
        "compile",
        "optimizer",
        "loss",
        "metrics",
        "model",
        "fit",
        "inputs_train",
        "queries_train",
        "answers_train",
        "batch_size",
        "epochs",
        "validation_data",
        "inputs_test",
        "queries_test",
        "answers_test"
    ],
    "literals": [
        "'(\\W+)?'",
        "'utf-8'",
        "' '",
        "'\\t'",
        "'\\t'",
        "''",
        "'babi-tasks-v1-2.tar.gz'",
        "'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'",
        "'Error downloading dataset, please download it manually:\\n'",
        "'$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'",
        "'$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz'",
        "'single_supporting_fact_10k'",
        "'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'",
        "'two_supporting_facts_10k'",
        "'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'",
        "'single_supporting_fact_10k'",
        "'Extracting stories for the challenge:'",
        "'train'",
        "'test'",
        "'-'",
        "'Vocab size:'",
        "'unique words'",
        "'Story max length:'",
        "'words'",
        "'Query max length:'",
        "'words'",
        "'Number of training stories:'",
        "'Number of test stories:'",
        "'-'",
        "'Here\\'s what a \"story\" tuple looks like (input, query, answer):'",
        "'-'",
        "'Vectorizing the word sequences...'",
        "'-'",
        "'inputs: integer tensor of shape (samples, max_length)'",
        "'inputs_train shape:'",
        "'inputs_test shape:'",
        "'-'",
        "'queries: integer tensor of shape (samples, max_length)'",
        "'queries_train shape:'",
        "'queries_test shape:'",
        "'-'",
        "'answers: binary (1 or 0) tensor of shape (samples, vocab_size)'",
        "'answers_train shape:'",
        "'answers_test shape:'",
        "'-'",
        "'Compiling...'",
        "'softmax'",
        "'softmax'",
        "'rmsprop'",
        "'categorical_crossentropy'",
        "'accuracy'"
    ],
    "variables": [
        "data",
        "story",
        "line",
        "nid",
        "line",
        "nid",
        "story",
        "q",
        "a",
        "supporting",
        "q",
        "substory",
        "supporting",
        "substory",
        "substory",
        "sent",
        "data",
        "flatten",
        "data",
        "X",
        "Xq",
        "Y",
        "x",
        "xq",
        "y",
        "y",
        "path",
        "tar",
        "challenges",
        "challenge_type",
        "challenge",
        "train_stories",
        "test_stories",
        "vocab",
        "vocab",
        "vocab_size",
        "story_maxlen",
        "query_maxlen",
        "word_idx",
        "inputs_train",
        "queries_train",
        "answers_train",
        "inputs_test",
        "queries_test",
        "answers_test",
        "input_sequence",
        "question",
        "input_encoder_m",
        "input_encoder_c",
        "question_encoder",
        "input_encoded_m",
        "input_encoded_c",
        "question_encoded",
        "match",
        "match",
        "response",
        "response",
        "answer",
        "answer",
        "answer",
        "answer",
        "answer",
        "model"
    ],
    "comments": [
        "Only select the related substory",
        "Provide all the substories",
        "let's not forget that index 0 is reserved",
        "QA1 with 10,000 samples",
        "QA2 with 10,000 samples",
        "Reserve 0 for masking via pad_sequences",
        "placeholders",
        "encoders",
        "embed the input sequence into a sequence of vectors",
        "output: (samples, story_maxlen, embedding_dim)",
        "embed the input into a sequence of vectors of size query_maxlen",
        "output: (samples, story_maxlen, query_maxlen)",
        "embed the question into a sequence of vectors",
        "output: (samples, query_maxlen, embedding_dim)",
        "encode input sequence and questions (which are indices)",
        "to sequences of dense vectors",
        "compute a 'match' between the first input vector sequence",
        "and the question vector sequence",
        "shape: `(samples, story_maxlen, query_maxlen)`",
        "add the match matrix with the second input vector sequence",
        "(samples, story_maxlen, query_maxlen)",
        "(samples, query_maxlen, story_maxlen)",
        "concatenate the match matrix with the question vector sequence",
        "the original paper uses a matrix multiplication for this reduction step.",
        "we choose to use a RNN instead.",
        "(samples, 32)",
        "one regularization layer -- more would probably be needed.",
        "(samples, vocab_size)",
        "we output a probability distribution over the vocabulary",
        "build the final model",
        "train"
    ],
    "docstrings": [
        "'''Trains a memory network on the bAbI dataset.\nReferences:\n- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n  http://arxiv.org/abs/1502.05698\n- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n  \"End-To-End Memory Networks\",\n  http://arxiv.org/abs/1503.08895\nReaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\nTime per epoch: 3s on CPU (core i7).\n'''",
        "'''Return the tokens of a sentence including punctuation.\n    >>> tokenize('Bob dropped the apple. Where is the apple?')\n    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n    '''",
        "'''Parse stories provided in the bAbi tasks format\n    If only_supporting is true, only the sentences\n    that support the answer are kept.\n    '''",
        "'''Given a file name, read the file,\n    retrieve the stories,\n    and then convert the sentences into a single story.\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    '''"
    ],
    "functions": [
        "tokenize",
        "parse_stories",
        "get_stories",
        "vectorize_stories"
    ],
    "classes": []
}