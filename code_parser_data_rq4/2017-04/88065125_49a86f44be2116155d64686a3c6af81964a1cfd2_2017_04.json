{
    "identifiers": [
        "numpy",
        "np",
        "cPickle",
        "pickle",
        "gym",
        "resume",
        "pickle",
        "load",
        "open",
        "np",
        "random",
        "randn",
        "H",
        "D",
        "np",
        "sqrt",
        "D",
        "np",
        "random",
        "randn",
        "H",
        "np",
        "sqrt",
        "H",
        "k",
        "np",
        "zeros_like",
        "v",
        "k",
        "v",
        "model",
        "iteritems",
        "k",
        "np",
        "zeros_like",
        "v",
        "k",
        "v",
        "model",
        "iteritems",
        "x",
        "np",
        "exp",
        "x",
        "I",
        "I",
        "I",
        "I",
        "I",
        "I",
        "I",
        "astype",
        "np",
        "ravel",
        "r",
        "np",
        "zeros_like",
        "r",
        "t",
        "reversed",
        "xrange",
        "r",
        "size",
        "r",
        "t",
        "running_add",
        "gamma",
        "r",
        "t",
        "running_add",
        "discounted_r",
        "x",
        "np",
        "dot",
        "model",
        "x",
        "h",
        "np",
        "dot",
        "model",
        "h",
        "sigmoid",
        "logp",
        "p",
        "h",
        "eph",
        "epdlogp",
        "np",
        "dot",
        "eph",
        "T",
        "epdlogp",
        "ravel",
        "np",
        "outer",
        "epdlogp",
        "model",
        "eph",
        "np",
        "dot",
        "dh",
        "T",
        "epx",
        "dW1",
        "dW2",
        "gym",
        "make",
        "env",
        "reset",
        "prepro",
        "observation",
        "cur_x",
        "prev_x",
        "prev_x",
        "np",
        "zeros",
        "D",
        "cur_x",
        "policy_forward",
        "x",
        "np",
        "random",
        "uniform",
        "aprob",
        "xs",
        "append",
        "x",
        "hs",
        "append",
        "h",
        "action",
        "dlogps",
        "append",
        "y",
        "aprob",
        "env",
        "render",
        "env",
        "step",
        "action",
        "reward_sum",
        "reward",
        "drs",
        "append",
        "reward",
        "done",
        "episode_number",
        "np",
        "vstack",
        "xs",
        "np",
        "vstack",
        "hs",
        "np",
        "vstack",
        "dlogps",
        "np",
        "vstack",
        "drs",
        "discount_rewards",
        "epr",
        "discounted_epr",
        "np",
        "mean",
        "discounted_epr",
        "discounted_epr",
        "np",
        "std",
        "discounted_epr",
        "epdlogp",
        "discounted_epr",
        "policy_backward",
        "eph",
        "epdlogp",
        "k",
        "model",
        "grad_buffer",
        "k",
        "grad",
        "k",
        "episode_number",
        "batch_size",
        "k",
        "v",
        "model",
        "iteritems",
        "grad_buffer",
        "k",
        "decay_rate",
        "rmsprop_cache",
        "k",
        "decay_rate",
        "g",
        "model",
        "k",
        "learning_rate",
        "g",
        "np",
        "sqrt",
        "rmsprop_cache",
        "k",
        "np",
        "zeros_like",
        "v",
        "reward_sum",
        "running_reward",
        "running_reward",
        "reward_sum",
        "reward_sum",
        "running_reward",
        "episode_number",
        "pickle",
        "dump",
        "model",
        "open",
        "env",
        "reset",
        "reward",
        "episode_number",
        "reward",
        "reward"
    ],
    "literals": [
        "'save.p'",
        "'rb'",
        "'W1'",
        "'W2'",
        "'W1'",
        "'W2'",
        "'W2'",
        "'W1'",
        "'W2'",
        "\"Pong-v0\"",
        "'resetting env. episode reward total was %f. running mean: %f'",
        "'save.p'",
        "'wb'",
        "'ep %d: game finished, reward: %f'",
        "''",
        "' !!!!!!!!'"
    ],
    "variables": [
        "H",
        "batch_size",
        "learning_rate",
        "gamma",
        "decay_rate",
        "resume",
        "D",
        "model",
        "model",
        "model",
        "model",
        "grad_buffer",
        "rmsprop_cache",
        "I",
        "I",
        "I",
        "I",
        "I",
        "discounted_r",
        "running_add",
        "running_add",
        "running_add",
        "discounted_r",
        "t",
        "h",
        "h",
        "logp",
        "p",
        "dW2",
        "dh",
        "dh",
        "dW1",
        "env",
        "observation",
        "prev_x",
        "xs",
        "hs",
        "dlogps",
        "drs",
        "running_reward",
        "reward_sum",
        "episode_number",
        "cur_x",
        "x",
        "prev_x",
        "aprob",
        "h",
        "action",
        "y",
        "observation",
        "reward",
        "done",
        "info",
        "epx",
        "eph",
        "epdlogp",
        "epr",
        "xs",
        "hs",
        "dlogps",
        "drs",
        "discounted_epr",
        "grad",
        "g",
        "rmsprop_cache",
        "k",
        "grad_buffer",
        "k",
        "running_reward",
        "reward_sum",
        "observation",
        "prev_x"
    ],
    "comments": [
        "matrix math",
        "for loading and saving models. Pickling is the process whereby a Python object hierarchy is converted into a byte stream)",
        "OpenAI's library that provides environments to test RL algorithms in, Universe adds",
        "even more environments",
        "Pong",
        "1 - receive image frame",
        "2 - move paddle UP/DOWN? (binary)",
        "3 - Make action recieve reward (+1 if moves past AI, -1 if missed ball, 0 otherwise)",
        "lets forget about other pong details. focus on the algo.",
        "1 RL is a Branch of machine learning concerned with taking #sequences of actions",
        "Usually described in terms of agent interacting with a",
        "previously unknown environment, trying to maximize cumulative reward",
        "2 RL combined with other techniques is powerful (AlphaGo, DQN)",
        "Policy Gradients > DQN (by most ppl, incl. DQN Authors) https://www.youtube.com/watch?v=M8RfOCYIL8k",
        "3 we'll build a 2 layer fully connected neural network",
        "https://karpathy.github.io/assets/rl/policy.png",
        "recieves image pixels, outputs probability of moving UP (stochasticity!)",
        "Credit assignment problem - we just got +1 reward. what caused it? millions parameters and pixels...which frame?",
        "no labels!",
        "https://karpathy.github.io/assets/rl/rl.png",
        "forward pass > output probability distribution > sample from it to take an action > wait for reward > use reward as gradient",
        "to update network via backprop",
        "reward could be any positive number. doesnt matter. its about magnitude. thanks neural nets.",
        "Policy gradients has 3 key differences from supervised learning",
        "1) We don’t have the correct labels so as a “fake label” we substitute the action we happened to sample from the policy",
        "2) We modulate the loss for each example multiplicatively based on the eventual outcome,",
        "since we want to increase the log probability for actions that worked and decrease it for those that didn’t.",
        "3) runs on a continuously changing dataset (the episodes), scaled by the advantage, and we only want to do one (or very few)",
        "updates based on each sampled dataset.",
        "4 can be used to backprop when nodes are stochastic (!!, more on this later)",
        "PG vs Humans",
        "Policy Gradients have to actually experience a positive reward, and experience it very often in order to eventually and slowly shift the policy parameters towards",
        "repeating moves that give high rewards. With our abstract model, humans can figure out what is likely to give rewards without ever actually experiencing the",
        "rewarding or unrewarding transition. I don’t have to actually experience crashing my car into a wall a few hundred times before I slowly start avoiding to do so.",
        "Training time - 3 nights on a macbook, couple hours on GPU cluster in AWS",
        "hyperparameters",
        "number of hidden layer neurons",
        "every how many episodes to do a param update?",
        "for convergence (too low- slow to converge, too high,never converge)",
        "discount factor for reward (i.e later rewards are exponentially less important)",
        "decay factor for RMSProp leaky sum of grad^2",
        "resume from previous checkpoint?",
        "model initialization",
        "input dimensionality: 80x80 grid (the pong world)",
        "load from pickled checkpoint",
        "initialize model",
        "rand returns a sample (or samples) from the “standard normal” distribution",
        "xavier algo determines the scale of initialization based on the number of input and output neurons.",
        "Imagine that your weights are initially very close to 0. What happens is that the signals shrink as it goes through each",
        "layer until it becomes too tiny to be useful. Now if your weights are too big, the signals grow at each layer",
        "it passes through until it is too massive to be useful.",
        "By using Xavier initialization, we make sure that the weights are not too small but not too big to propagate accurately the signals.",
        "\"Xavier\" initialization",
        "zeros like returns an array of zeros with the same shape and type as a given array.",
        "we will update buffers that add up gradients over a batch",
        "rmsprop (gradient descent) memory used to update model",
        "activation function",
        "sigmoid \"squashing\" function to interval [0,1]",
        "takes a single game frame as input",
        "preprocesses before feeding into model",
        "crop",
        "downsample by factor of 2",
        "erase background (background type 1)",
        "erase background (background type 2)",
        "everything else (paddles, ball) just set to 1",
        "In practice the discount factor could be used to model the fact that the decision maker is uncertain",
        "about if in the next decision instant the world (e.g., environment / game / process ) is going to end.",
        "If the decision maker is a robot, the discount factor could be the probability that the robot is switched",
        "off in the next time instant (the world ends in the previous terminology). That is the reason why the robot",
        "is short sighted and does not optimize the sum reward but the discounted sum reward.",
        "initilize discount reward matrix as empty",
        "to store reward sums",
        "for each reward",
        "if reward at index t is nonzero, reset the sum, since this was a game boundary (pong specific!)",
        "increment the sum",
        "https://github.com/hunkim/ReinforcementZeroToAll/issues/1",
        "earlier rewards given more value over time",
        "assign the calculated sum to our discounted reward matrix",
        "forward propagation via numpy woot!",
        "matrix multiply input by the first set of weights to get hidden state",
        "will be able to detect various game scenarios (e.g. the ball is in the top, and our paddle is in the middle)",
        "apply an activation function to it",
        "f(x)=max(0,x) take max value, if less than 0, use 0",
        "ReLU nonlinearity",
        "repeat process once more",
        "will decide if in each case we should be going UP or DOWN.",
        "squash it with an activation (this time sigmoid to output probabilities)",
        "return probability of taking action 2, and hidden state",
        "recursively compute error derivatives for both layers, this is the chain rule",
        "epdlopgp modulates the gradient with advantage",
        "compute updated derivative with respect to weight 2. It's the parameter hidden states transpose * gradient w/ advantage (then flatten with ravel())",
        "Compute derivative hidden. It's the outer product of gradient w/ advatange and weight matrix 2 of 2",
        "apply activation",
        "backpro prelu",
        "compute derivative with respect to weight 1 using hidden states transpose and input observation",
        "return both derivatives to update weights",
        "environment",
        "Each timestep, the agent chooses an action, and the environment returns an observation and a reward.",
        "The process gets started by calling reset, which returns an initial observation",
        "used in computing the difference frame",
        "observation, hidden state, gradient, reward",
        "current reward",
        "sum rewards",
        "where are we?",
        "begin training!",
        "preprocess the observation, set input to network to be difference image",
        "Since we want our policy network to detect motion",
        "difference image = subtraction of current and last frame",
        "forward the policy network and sample an action from the returned probability",
        "this is the stochastic part",
        "since not apart of the model, model is easily differentiable",
        "if it was apart of the model, we'd have to use a reparametrization trick (a la variational autoencoders. so badass)",
        "roll the dice!",
        "record various intermediates (needed later for backprop)",
        "observation",
        "hidden state",
        "a \"fake label\"",
        "grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)",
        "step the environment and get new measurements",
        "record reward (has to be done after we call step() to get reward for previous action)",
        "an episode finished",
        "stack together all inputs, hidden states, action gradients, and rewards for this episode",
        "reset array memory",
        "the strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important",
        "compute the discounted reward backwards through time",
        "standardize the rewards to be unit normal (helps control the gradient estimator variance)",
        "modulate the gradient with advantage (PG magic happens right here.)",
        "accumulate grad over batch",
        "perform rmsprop parameter update every batch_size episodes",
        "gradient",
        "reset batch gradient buffer",
        "boring book-keeping",
        "reset env",
        "Pong has either +1 or -1 reward exactly when game ends."
    ],
    "docstrings": [
        "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"",
        "\"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"",
        "\"\"\" take 1D float array of rewards and compute discounted reward \"\"\"",
        "\"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\""
    ],
    "functions": [
        "sigmoid",
        "prepro",
        "discount_rewards",
        "policy_forward",
        "policy_backward"
    ],
    "classes": []
}