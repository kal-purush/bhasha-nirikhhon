{
    "identifiers": [
        "numpy",
        "np",
        "sys",
        "networkx",
        "nx",
        "sys",
        "path",
        "append",
        "payoff_matrix",
        "matplotlib",
        "matplotlib",
        "use",
        "matplotlib",
        "pyplot",
        "plt",
        "plt",
        "style",
        "use",
        "warnings",
        "tqdm",
        "tqdm",
        "warnings",
        "filterwarnings",
        "agent",
        "Q_Learning_Agent",
        "Q_Learning_Agent",
        "pandas",
        "pd",
        "n_agent",
        "n_round",
        "payoff_matrix",
        "n_agent",
        "n_round",
        "game_name",
        "action_name",
        "payoff_matrix",
        "payoff_matrix",
        "create_network",
        "n_agent",
        "pd",
        "DataFrame",
        "np",
        "zeros",
        "n_round",
        "n_agent",
        "pd",
        "DataFrame",
        "np",
        "zeros",
        "n_round",
        "n_agent",
        "f_name",
        "other",
        "open",
        "f_name",
        "f",
        "f",
        "write",
        "n_round",
        "f",
        "write",
        "n_agent",
        "f",
        "write",
        "n_edges",
        "f",
        "write",
        "G_alg",
        "f",
        "write",
        "game_name",
        "f",
        "write",
        "rl_alg",
        "other",
        "f",
        "write",
        "other",
        "n_agent",
        "G_alg",
        "G",
        "nx",
        "random_regular_graph",
        "d",
        "n",
        "n_agent",
        "n",
        "G",
        "nodes",
        "G",
        "neighbors",
        "n",
        "n_edges",
        "len",
        "neighbors",
        "eval",
        "rl_alg",
        "n",
        "sorted",
        "neighbors",
        "np",
        "arange",
        "len",
        "neighbors",
        "action_name",
        "G",
        "node",
        "n",
        "agent",
        "G",
        "node",
        "n",
        "G",
        "node",
        "n",
        "f_name",
        "w_degree",
        "G",
        "node",
        "n",
        "G",
        "node",
        "n",
        "n",
        "G",
        "nodes",
        "w_degree",
        "nx",
        "degree",
        "G",
        "nx",
        "draw",
        "G",
        "pos",
        "nx",
        "circular_layout",
        "G",
        "nodelist",
        "d",
        "keys",
        "width",
        "alpha",
        "node_color",
        "color",
        "node_size",
        "v",
        "v",
        "d",
        "values",
        "nx",
        "draw",
        "G",
        "pos",
        "nx",
        "circular_layout",
        "G",
        "width",
        "alpha",
        "node_color",
        "color",
        "f_name",
        "plt",
        "show",
        "plt",
        "savefig",
        "f_name",
        "i",
        "n_round",
        "i",
        "i",
        "i",
        "len",
        "payoff_matrix",
        "n",
        "G",
        "nodes",
        "G",
        "node",
        "n",
        "G",
        "node",
        "n",
        "act",
        "G",
        "node",
        "n",
        "random",
        "rand",
        "reduction",
        "agent_action_table",
        "n",
        "G",
        "node",
        "n",
        "n",
        "G",
        "nodes",
        "G",
        "node",
        "n",
        "get_neighbors",
        "G",
        "node",
        "n",
        "ne",
        "neighbors",
        "G",
        "node",
        "ne",
        "ne_action",
        "ne_action",
        "n_signal",
        "n_reward",
        "payoff_matrix",
        "n_action",
        "ne_action",
        "G",
        "node",
        "n",
        "n_reward",
        "agent_payoff_table",
        "n",
        "n_reward",
        "len",
        "neighbors",
        "G",
        "node",
        "n",
        "update_q",
        "G",
        "node",
        "n",
        "n_reward",
        "G",
        "node",
        "n",
        "n_signal",
        "f_name",
        "agent_payoff_table",
        "apply",
        "x",
        "np",
        "mean",
        "x",
        "axis",
        "to_csv",
        "f_name",
        "header",
        "index",
        "f_name",
        "agent_action_table",
        "apply",
        "x",
        "len",
        "x",
        "x",
        "len",
        "x",
        "axis",
        "to_csv",
        "f_name",
        "header",
        "index",
        "n",
        "all_",
        "n",
        "RESULT_DIR",
        "n",
        "synchro_world_signal",
        "eval",
        "n",
        "W",
        "run",
        "W",
        "save_average_reward",
        "RESULT_NAME",
        "W",
        "save_coop_per",
        "RESULT_NAME",
        "W",
        "draw_network",
        "f_name",
        "RESULT_NAME",
        "w_degree",
        "W",
        "report_meta_info",
        "f_name",
        "RESULT_NAME",
        "other"
    ],
    "literals": [
        "\"../\"",
        "\"Agg\"",
        "'ggplot'",
        "'ignore'",
        "\"w\"",
        "\"繰り返し回数 : \"",
        "\"\\n\"",
        "\"エージェント数 : \"",
        "\"\\n\"",
        "\"エッジ数 : \"",
        "\"\\n\"",
        "\"ネットワーク種類 : \"",
        "\"\\n\"",
        "\"利得行列 : \"",
        "\"\\n\"",
        "\"強化学習アルゴリズム : \"",
        "\"\\n\"",
        "\"その他の条件 : \"",
        "\"random\"",
        "\"Q_Learning_Agent\"",
        "\"agent\"",
        "\"action\"",
        "\"n_signal\"",
        "\"r\"",
        "\"action\"",
        "\"b\"",
        "\"action\"",
        "\"g\"",
        "'iter : '",
        "\"action\"",
        "\"agent\"",
        "\"n_signal\"",
        "\"action\"",
        "\"agent\"",
        "\"action\"",
        "\"action\"",
        "\"reward\"",
        "\"agent\"",
        "\"n_signal\"",
        "\"n_signal\"",
        "\"prisoners_dilemma_sig\"",
        "\"__main__\"",
        "\"../../../results/50/\"",
        "'_q_reduc_signal_random_50'",
        "\"_ave.csv\"",
        "\"_per.csv\"",
        "\"_fig.png\"",
        "\"_meta.txt\"",
        "\"シグナル\"",
        "'save done!!'"
    ],
    "variables": [
        "n_agent",
        "n_round",
        "agent_action_table",
        "agent_payoff_table",
        "n_edges",
        "rl_alg",
        "neighbors",
        "agent",
        "color",
        "d",
        "rand",
        "rand",
        "i",
        "neighbors",
        "n_action",
        "n_reward",
        "n_signal",
        "ne_action",
        "i",
        "all_",
        "RESULT_DIR",
        "RESULT_NAME",
        "W"
    ],
    "comments": [
        "-*- coding: utf-8 -*-",
        "author : Takuro Yamazaki",
        "last update : 07/06/2017",
        "description : エッジの数によるsignal意思決定ゲームの学習差を検証する",
        "データに記述すべきメタデータ",
        "- 繰り返し回数",
        "- エージェント数",
        "- ネットワークの種類",
        "- エージェントの種類",
        "- エッジ数",
        "- 利得行列の種類",
        "- その他の条件(初期行動制約など)",
        "prevent no display error",
        "warning表示なし",
        "from agent.Satisficing_Agent import Satisficing_Agent",
        "from agent.WoLF_PHC_Agent import WoLF_PHC_Agent",
        "rl alg name",
        "エージェントの定義を変えるのはここ",
        "状態の初期値は0",
        "行動によって色分けしてるけど別ので分類するなら変えよう",
        "file名指定がなければただ表示",
        "全エージェントが同期的に行動選択",
        "初期のランダム行動",
        "reduction=Trueで減衰",
        "報酬計算&Q値更新",
        "write(f_name,\"a\")で追記",
        "妥当なエージェント数はいくつか"
    ],
    "docstrings": [
        "\"\"\"\n        @description ネットワークモデルの定義\n        @param n_agent エージェントの数\n        \"\"\"",
        "\"\"\"\n        @description Graphの可視化\n        @param f_name save file name\n        @param w_degree draw graph dependent on the node degree\n        \"\"\""
    ],
    "functions": [
        "report_meta_info",
        "create_network",
        "draw_network",
        "run",
        "save_average_reward",
        "save_coop_per"
    ],
    "classes": [
        "synchro_world_signal"
    ]
}