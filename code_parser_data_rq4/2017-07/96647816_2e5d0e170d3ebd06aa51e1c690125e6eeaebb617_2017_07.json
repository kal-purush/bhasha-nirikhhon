{
    "identifiers": [
        "nltk",
        "tokenize",
        "sent_tokenize",
        "open",
        "fhandler",
        "fhandler",
        "read",
        "sent_tokenize",
        "str_input",
        "format",
        "num",
        "len",
        "sent_result",
        "nltk",
        "tokenize",
        "word_tokenize",
        "word_tokenize",
        "sent_result",
        "token_result",
        "nltk",
        "pos_tag",
        "pos_tag",
        "token_result",
        "pos_crf",
        "nltk",
        "help",
        "upenn_tagset",
        "upenn_tagset"
    ],
    "literals": [
        "'darksouls_training.txt'",
        "'r'",
        "'There are {num} sentences in the training data.'",
        "'JJ'"
    ],
    "variables": [
        "str_input",
        "sent_result",
        "token_result",
        "pos_crf"
    ],
    "comments": [
        "I. tokenization",
        "generally speaking there are 2 types of tokenization: at the setence level",
        "and at the word level. We are interested in the word level one here. But I",
        "will demostrate both here.",
        "1. sentence_tokenize",
        "2 word_tokenize",
        "II. POS",
        "after breaking down the sentence into tokens, we want to call a POS-tagger",
        "to determin what POS each token/word is. There are many different taggers",
        "to choose from. The in nltk, the state-of-art tagger should be the",
        "CRFTagger. We can train our own tagger if we have enough labeled data. For",
        "now I will use the default pos_tag method",
        "if you don't know what each symbol mean, you can call the following function to help. Let's check JJ for example",
        "3. syntactic parsing.",
        "TODO: write a demo"
    ],
    "docstrings": [
        "\"\"\"PURPOSE: this file provides some basic use cases for the nltk package.\nThe text analysis process can generally be divided into 2 steps. I'll\nname them the preprocessing step and the processing step. The Preprocessing\nstep refers to the tokenization of the input text sequence. In short, we\nneed to break down the whole sequence into meaningful units (words) for\nfurther processing. The processing step includes the word-level part of\nspeech tagging, the syntactic level parsing, and the understanding of the\nsemantics.\n\nDATA: I scraped the wiki page of Dark Souls on 2017-07-09. This data is\nsplit into 2 parts (darksouls_training.txt and darksouls_test.txt). I use\ndark_souls_training.txt for this demo.\n\"\"\""
    ],
    "functions": [],
    "classes": []
}