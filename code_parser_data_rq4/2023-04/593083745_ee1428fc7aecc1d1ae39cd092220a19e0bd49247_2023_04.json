{
    "identifiers": [
        "partiallib",
        "nside2indexes",
        "ntrain",
        "EXP_NAME",
        "order",
        "Nside",
        "architecture",
        "verbose",
        "EXP_NAME",
        "n_classes",
        "Nside",
        "Nside",
        "Nside",
        "Nside",
        "Nside",
        "Nside",
        "Nside",
        "nsides",
        "nside2indexes",
        "nsides",
        "order",
        "architecture",
        "order",
        "ntrain",
        "n_evaluations",
        "verbose",
        "format",
        "nsides",
        "format",
        "nside",
        "order",
        "nside",
        "nsides",
        "format",
        "format",
        "Nside",
        "order",
        "format",
        "ntrain",
        "Nside",
        "order"
    ],
    "literals": [
        "\"FCN\"",
        "'dir_name'",
        "'conv'",
        "'chebyshev5'",
        "'pool'",
        "'max'",
        "'activation'",
        "'relu'",
        "'statistics'",
        "'mean'",
        "'F'",
        "'K'",
        "'batch_norm'",
        "'M'",
        "'nsides'",
        "'indexes'",
        "'regularization'",
        "'2d'",
        "'regularization'",
        "'dropout'",
        "'num_epochs'",
        "'batch_size'",
        "'eval_frequency'",
        "'num_epochs'",
        "'batch_size'",
        "'#sides: {}'",
        "'#pixels: {}'",
        "'#samples per batch: {}'",
        "'batch_size'",
        "'=> #pixels per batch (input): {:,}'",
        "'batch_size'",
        "'=> #pixels for training (input): {:,}'",
        "'num_epochs'"
    ],
    "variables": [
        "n_classes",
        "nsides",
        "n_evaluations"
    ],
    "comments": [
        "Types of layers.",
        "Graph convolution: chebyshev5 or monomials.",
        "Pooling: max or average.",
        "Non-linearity: relu, elu, leaky_relu, softmax, tanh, etc.",
        "Statistics (for invariance): None, mean, var, meanvar, hist.",
        "Architecture.",
        "Graph convolutional layers: number of feature maps.",
        "Polynomial orders.",
        "Batch normalization.",
        "Fully connected layers: output dimensionalities.",
        "Pooling.",
        "Regularization (to prevent over-fitting).",
        "Amount of L2 regularization over the weights (will be divided by the number of weights).",
        "elif architecture == 'FNN':",
        "print('Use regularization new')",
        "params['regularization'] = 10  # Amount of L2 regularization over the weights (will be divided by the number of weights).",
        "params['dropout'] = 1  # Percentage of neurons to keep.",
        "Percentage of neurons to keep.",
        "Training.",
        "Number of passes through the training data.",
        "Constant quantity of information (#pixels) per step (invariant to sample size).",
        "Optimization: learning rate schedule and optimizer.",
        "params['scheduler'] = lambda step: tf.train.exponential_decay(2e-4, step, decay_steps=1, decay_rate=0.999)",
        "params['optimizer'] = lambda lr: tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.999, epsilon=1e-8)",
        "Number of model evaluations during training (influence training time).",
        "Number of pixels on the full sphere: 12 * nsides**2.",
        "n_steps = params['num_epochs'] * ntrain // params['batch_size']",
        "lr = [params['scheduler'](step).eval(session=tf.Session()) for step in [0, n_steps]]",
        "print('Learning rate will start at {:.1e} and finish at {:.1e}.'.format(*lr))"
    ],
    "docstrings": [
        "\"\"\"Parameters for the cgcnn and cnn2d defined in deepsphere/models.py\"\"\""
    ],
    "functions": [
        "get_params"
    ],
    "classes": []
}