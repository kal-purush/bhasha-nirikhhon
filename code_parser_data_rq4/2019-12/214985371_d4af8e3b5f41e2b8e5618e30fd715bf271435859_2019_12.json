{
    "identifiers": [
        "numpy",
        "np",
        "random",
        "copy",
        "collections",
        "namedtuple",
        "deque",
        "model",
        "Actor",
        "Critic",
        "torch",
        "torch",
        "nn",
        "functional",
        "F",
        "torch",
        "optim",
        "optim",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "state_size",
        "action_size",
        "random_seed",
        "state_size",
        "action_size",
        "random",
        "seed",
        "random_seed",
        "EPSILON",
        "Actor",
        "state_size",
        "action_size",
        "random_seed",
        "to",
        "device",
        "Actor",
        "state_size",
        "action_size",
        "random_seed",
        "to",
        "device",
        "optim",
        "Adam",
        "actor_local",
        "parameters",
        "lr",
        "LR_ACTOR",
        "Critic",
        "state_size",
        "action_size",
        "random_seed",
        "to",
        "device",
        "Critic",
        "state_size",
        "action_size",
        "random_seed",
        "to",
        "device",
        "optim",
        "Adam",
        "critic_local",
        "parameters",
        "lr",
        "LR_CRITIC",
        "weight_decay",
        "WEIGHT_DECAY",
        "OUNoise",
        "action_size",
        "random_seed",
        "ReplayBuffer",
        "action_size",
        "BUFFER_SIZE",
        "BATCH_SIZE",
        "random_seed",
        "state",
        "action",
        "reward",
        "next_state",
        "done",
        "timestep",
        "memory",
        "add",
        "state",
        "action",
        "reward",
        "next_state",
        "done",
        "len",
        "memory",
        "BATCH_SIZE",
        "timestep",
        "LEARN_EVERY",
        "_",
        "LEARN_NUM",
        "memory",
        "sample",
        "learn",
        "experiences",
        "GAMMA",
        "state",
        "add_noise",
        "torch",
        "from_numpy",
        "state",
        "to",
        "device",
        "actor_local",
        "eval",
        "torch",
        "no_grad",
        "actor_local",
        "state",
        "cpu",
        "data",
        "numpy",
        "actor_local",
        "train",
        "add_noise",
        "action",
        "epsilon",
        "noise",
        "sample",
        "np",
        "clip",
        "action",
        "noise",
        "reset",
        "experiences",
        "gamma",
        "experiences",
        "actor_target",
        "next_states",
        "critic_target",
        "next_states",
        "actions_next",
        "rewards",
        "gamma",
        "Q_targets_next",
        "dones",
        "critic_local",
        "states",
        "actions",
        "F",
        "mse_loss",
        "Q_expected",
        "Q_targets",
        "critic_optimizer",
        "zero_grad",
        "critic_loss",
        "backward",
        "GRAD_CLIPPING",
        "torch",
        "nn",
        "utils",
        "clip_grad_norm_",
        "critic_local",
        "parameters",
        "GRAD_CLIPPING",
        "critic_optimizer",
        "step",
        "actor_local",
        "states",
        "critic_local",
        "states",
        "actions_pred",
        "mean",
        "actor_optimizer",
        "zero_grad",
        "actor_loss",
        "backward",
        "actor_optimizer",
        "step",
        "soft_update",
        "critic_local",
        "critic_target",
        "TAU",
        "soft_update",
        "actor_local",
        "actor_target",
        "TAU",
        "EPSILON_DECAY",
        "epsilon",
        "EPSILON_DECAY",
        "noise",
        "reset",
        "local_model",
        "target_model",
        "tau",
        "target_param",
        "local_param",
        "target_model",
        "parameters",
        "local_model",
        "parameters",
        "target_param",
        "data",
        "copy_",
        "tau",
        "local_param",
        "data",
        "tau",
        "target_param",
        "data",
        "size",
        "seed",
        "mu",
        "theta",
        "OU_THETA",
        "sigma",
        "OU_SIGMA",
        "mu",
        "np",
        "ones",
        "size",
        "theta",
        "sigma",
        "random",
        "seed",
        "seed",
        "reset",
        "copy",
        "copy",
        "mu",
        "state",
        "theta",
        "mu",
        "x",
        "sigma",
        "np",
        "array",
        "random",
        "random",
        "i",
        "len",
        "x",
        "x",
        "dx",
        "state",
        "action_size",
        "buffer_size",
        "batch_size",
        "seed",
        "action_size",
        "deque",
        "maxlen",
        "buffer_size",
        "batch_size",
        "namedtuple",
        "field_names",
        "random",
        "seed",
        "seed",
        "state",
        "action",
        "reward",
        "next_state",
        "done",
        "experience",
        "state",
        "action",
        "reward",
        "next_state",
        "done",
        "memory",
        "append",
        "e",
        "random",
        "sample",
        "memory",
        "k",
        "batch_size",
        "torch",
        "from_numpy",
        "np",
        "vstack",
        "e",
        "state",
        "e",
        "experiences",
        "e",
        "to",
        "device",
        "torch",
        "from_numpy",
        "np",
        "vstack",
        "e",
        "action",
        "e",
        "experiences",
        "e",
        "to",
        "device",
        "torch",
        "from_numpy",
        "np",
        "vstack",
        "e",
        "reward",
        "e",
        "experiences",
        "e",
        "to",
        "device",
        "torch",
        "from_numpy",
        "np",
        "vstack",
        "e",
        "next_state",
        "e",
        "experiences",
        "e",
        "to",
        "device",
        "torch",
        "from_numpy",
        "np",
        "vstack",
        "e",
        "done",
        "e",
        "experiences",
        "e",
        "astype",
        "np",
        "uint8",
        "to",
        "device",
        "states",
        "actions",
        "rewards",
        "next_states",
        "dones",
        "len",
        "memory"
    ],
    "literals": [
        "\"cuda:0\"",
        "\"cpu\"",
        "\"Experience\"",
        "\"state\"",
        "\"action\"",
        "\"reward\"",
        "\"next_state\"",
        "\"done\""
    ],
    "variables": [
        "BUFFER_SIZE",
        "BATCH_SIZE",
        "GAMMA",
        "TAU",
        "LR_ACTOR",
        "LR_CRITIC",
        "WEIGHT_DECAY",
        "LEARN_EVERY",
        "LEARN_NUM",
        "GRAD_CLIPPING",
        "OU_SIGMA",
        "OU_THETA",
        "EPSILON",
        "EPSILON_DECAY",
        "device",
        "state_size",
        "action_size",
        "seed",
        "epsilon",
        "actor_local",
        "actor_target",
        "actor_optimizer",
        "critic_local",
        "critic_target",
        "critic_optimizer",
        "noise",
        "memory",
        "experiences",
        "state",
        "action",
        "states",
        "actions",
        "rewards",
        "next_states",
        "dones",
        "actions_next",
        "Q_targets_next",
        "Q_targets",
        "Q_expected",
        "critic_loss",
        "actions_pred",
        "actor_loss",
        "mu",
        "theta",
        "sigma",
        "seed",
        "state",
        "x",
        "dx",
        "state",
        "action_size",
        "memory",
        "batch_size",
        "experience",
        "seed",
        "e",
        "experiences",
        "states",
        "actions",
        "rewards",
        "next_states",
        "dones"
    ],
    "comments": [
        "replay buffer size",
        "minibatch size",
        "discount factor",
        "for soft update of target parameters",
        "learning rate of the actor",
        "learning rate of the critic",
        "L2 weight decay",
        "Suggested on slack:",
        "learning timestep interval",
        "number of learning passes",
        "Gradient Clipping",
        "Ornstein-Uhlenbeck noise parameters",
        "",
        "for epsilon in the noise process (act step)",
        "",
        "Actor Network (w/ Target Network)",
        "Critic Network (w/ Target Network)",
        "Noise process",
        "Replay memory",
        "Save experience / reward",
        "If updating in batches, then add the last memory of the agents (e.g. 20 agents) to a buffer",
        "and if we've met batch size only push to learn in multiples of whatever LEARN_NUM specifies (e.g.10)",
        "Learn, if enough samples are available in memory",
        "---------------------------- update critic ---------------------------- #",
        "Get predicted next-state actions and Q values from target models",
        "Compute Q targets for current states (y_i)",
        "Compute critic loss",
        "Minimize the loss",
        "gradient clipping for critic",
        "---------------------------- update actor ---------------------------- #",
        "Compute actor loss",
        "Minimize the loss",
        "----------------------- update target networks ----------------------- #",
        "--------------------- and update epsilon decay ----------------------- #",
        "internal memory (deque)"
    ],
    "docstrings": [
        "\"\"\"Interacts with and learns from the environment.\"\"\"",
        "\"\"\"Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            random_seed (int): random seed\n        \"\"\"",
        "\"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"",
        "\"\"\"Returns actions for given state as per current policy.\"\"\"",
        "\"\"\"Update policy and value parameters using given batch of experience tuples.\n        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n        where:\n            actor_target(state) -> action\n            critic_target(state, action) -> Q-value\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n            gamma (float): discount factor\n        \"\"\"",
        "\"\"\"Soft update model parameters.\n        θ_target = τ*θ_local + (1 - τ)*θ_target\n\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        \"\"\"",
        "\"\"\"Ornstein-Uhlenbeck process.\"\"\"",
        "\"\"\"Initialize parameters and noise process.\"\"\"",
        "\"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"",
        "\"\"\"Update internal state and return it as a noise sample.\"\"\"",
        "\"\"\"Fixed-size buffer to store experience tuples.\"\"\"",
        "\"\"\"Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n        \"\"\"",
        "\"\"\"Add a new experience to memory.\"\"\"",
        "\"\"\"Randomly sample a batch of experiences from memory.\"\"\"",
        "\"\"\"Return the current size of internal memory.\"\"\""
    ],
    "functions": [
        "step",
        "act",
        "reset",
        "learn",
        "soft_update",
        "reset",
        "sample",
        "add",
        "sample",
        "__len__"
    ],
    "classes": [
        "Agent",
        "OUNoise",
        "ReplayBuffer"
    ]
}