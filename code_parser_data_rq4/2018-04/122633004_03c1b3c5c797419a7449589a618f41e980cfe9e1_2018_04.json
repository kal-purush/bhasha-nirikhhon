{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "autograd",
        "Variable",
        "pytorch_lm",
        "dropout",
        "StatefulDropout",
        "pytorch_lm",
        "utils",
        "config",
        "create_object",
        "nn",
        "Module",
        "input_size",
        "hidden_size",
        "num_layers",
        "dropout",
        "Rhn",
        "input_size",
        "hidden_size",
        "num_layers",
        "nn",
        "Parameter",
        "torch",
        "Tensor",
        "input_size",
        "hidden_size",
        "nn",
        "Parameter",
        "torch",
        "Tensor",
        "input_size",
        "hidden_size",
        "nn",
        "Parameter",
        "torch",
        "Tensor",
        "input_size",
        "hidden_size",
        "nn",
        "Linear",
        "hidden_size",
        "hidden_size",
        "l",
        "num_layers",
        "nn",
        "Linear",
        "hidden_size",
        "hidden_size",
        "l",
        "num_layers",
        "nn",
        "Linear",
        "hidden_size",
        "hidden_size",
        "l",
        "num_layers",
        "letter",
        "lst",
        "r_h",
        "r_t",
        "r_c",
        "l",
        "p",
        "lst",
        "add_module",
        "format",
        "letter",
        "l",
        "p",
        "reset_parameters",
        "input",
        "s",
        "input",
        "s",
        "input_t",
        "torch",
        "squeeze",
        "input",
        "chunk",
        "input",
        "size",
        "dim",
        "input_t",
        "l",
        "num_layers",
        "l",
        "input_t",
        "matmul",
        "w_h",
        "l",
        "input_t",
        "matmul",
        "w_t",
        "l",
        "input_t",
        "matmul",
        "w_c",
        "l",
        "whx",
        "wtx",
        "wcx",
        "torch",
        "tanh",
        "whx",
        "r_h",
        "l",
        "s",
        "torch",
        "sigmoid",
        "wtx",
        "r_t",
        "l",
        "s",
        "torch",
        "sigmoid",
        "wcx",
        "r_c",
        "l",
        "s",
        "h",
        "t",
        "c",
        "h",
        "t",
        "s",
        "c",
        "s",
        "outputs",
        "append",
        "s",
        "torch",
        "stack",
        "outputs",
        "s",
        "initrange",
        "weight",
        "parameters",
        "weight",
        "data",
        "uniform_",
        "initrange",
        "initrange",
        "batch_size",
        "Variable",
        "torch",
        "Tensor",
        "batch_size",
        "hidden_size",
        "zero_",
        "w_h"
    ],
    "literals": [
        "'H'",
        "'T'",
        "'C'",
        "'Rb_{}_{}'",
        "'INPUT'",
        "'S-1'",
        "'INPUT_T'",
        "'L'",
        "'WHX'",
        "'WTX'",
        "'WCX'",
        "'H'",
        "'T'",
        "'C'",
        "'S'",
        "'OUTPUT'"
    ],
    "variables": [
        "input_size",
        "hidden_size",
        "num_layers",
        "w_h",
        "w_t",
        "w_c",
        "r_h",
        "r_t",
        "r_c",
        "outputs",
        "whx",
        "wtx",
        "wcx",
        "h",
        "t",
        "c",
        "s"
    ],
    "comments": [
        "!/usr/bin/env python3",
        "vim: set fileencoding=utf-8 :",
        "chunk() cuts batch_size x 1 x input_size chunks from input",
        "The input is processed only by the first layer",
        "The gates (and the state)",
        "The new state",
        "Here the output is the current s"
    ],
    "docstrings": [
        "\"\"\"Implements Recurrent Highway Networks from Zilly et al. (2017).\"\"\"",
        "\"\"\"Implements Recurrent Highway Networks from Zilly et al. (2017).\"\"\"",
        "\"\"\"Initializes the parameters uniformly to between -/+ initrange.\"\"\"",
        "\"\"\"\n        Returns a :class:`Variable` for the hidden state. As I understand, we\n        only need one of these (as opposed to LSTM).\n        \"\"\""
    ],
    "functions": [
        "forward",
        "reset_parameters",
        "init_hidden"
    ],
    "classes": [
        "Rhn"
    ]
}