{
    "identifiers": [
        "gym",
        "numpy",
        "np",
        "torch",
        "torch",
        "nn",
        "nn",
        "gym",
        "make",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "device",
        "nn",
        "Module",
        "policy_estimator",
        "env",
        "observation_space",
        "shape",
        "env",
        "action_space",
        "n",
        "nn",
        "Sequential",
        "nn",
        "Linear",
        "n_inputs",
        "nn",
        "ReLU",
        "nn",
        "Linear",
        "n_outputs",
        "nn",
        "Softmax",
        "dim",
        "state",
        "network",
        "torch",
        "FloatTensor",
        "state",
        "to",
        "device",
        "device",
        "action_probs",
        "torch",
        "load",
        "model",
        "to",
        "device",
        "device",
        "model",
        "eval",
        "action_space_n",
        "probs",
        "np",
        "random",
        "random",
        "i",
        "action_space_n",
        "p",
        "probs",
        "i",
        "item",
        "ran_num",
        "p",
        "i",
        "env",
        "reset",
        "_",
        "env",
        "render",
        "model",
        "predict",
        "observation",
        "sample_action",
        "env",
        "action_space",
        "n",
        "action_probs",
        "env",
        "step",
        "action",
        "done",
        "env",
        "reset",
        "env",
        "close"
    ],
    "literals": [
        "\"CartPole-v1\"",
        "'cuda'",
        "'cpu'",
        "'cartPole_model.pt'"
    ],
    "variables": [
        "env",
        "device",
        "n_inputs",
        "n_outputs",
        "network",
        "action_probs",
        "model",
        "ran_num",
        "p",
        "observation",
        "action_probs",
        "action",
        "observation",
        "reward",
        "done",
        "info",
        "observation"
    ],
    "comments": [
        "Set device",
        "Load trained model ##############",
        "Define network",
        "",
        "action = env.action_space.sample() # your agent here (this takes random actions)"
    ],
    "docstrings": [],
    "functions": [
        "predict",
        "sample_action"
    ],
    "classes": [
        "policy_estimator"
    ]
}