{
    "identifiers": [
        "logging",
        "ray",
        "ray",
        "rllib",
        "optimizers",
        "policy_optimizer",
        "PolicyOptimizer",
        "ray",
        "rllib",
        "utils",
        "annotations",
        "ray",
        "rllib",
        "utils",
        "timer",
        "TimerStat",
        "logging",
        "getLogger",
        "PolicyOptimizer",
        "workers",
        "num_sgd_iter",
        "train_batch_size",
        "sgd_minibatch_size",
        "standardize_fields",
        "keep_local_weights_in_sync",
        "backend",
        "PolicyOptimizer",
        "workers",
        "num_sgd_iter",
        "train_batch_size",
        "sgd_minibatch_size",
        "standardize_fields",
        "keep_local_weights_in_sync",
        "TimerStat",
        "TimerStat",
        "workers",
        "remote_workers",
        "ValueError",
        "ray",
        "get",
        "workers",
        "remote_workers",
        "get_node_ip",
        "remote",
        "ray",
        "get",
        "workers",
        "remote_workers",
        "find_free_port",
        "remote",
        "format",
        "ip",
        "ip",
        "port",
        "port",
        "logger",
        "info",
        "format",
        "address",
        "ray",
        "get",
        "worker",
        "setup_torch_data_parallel",
        "remote",
        "address",
        "i",
        "len",
        "workers",
        "remote_workers",
        "backend",
        "i",
        "worker",
        "workers",
        "remote_workers",
        "logger",
        "info",
        "PolicyOptimizer",
        "keep_local_weights_in_sync",
        "update_weights_timer",
        "ray",
        "put",
        "workers",
        "local_worker",
        "get_weights",
        "e",
        "workers",
        "remote_workers",
        "e",
        "set_weights",
        "remote",
        "weights",
        "learn_timer",
        "ray",
        "get",
        "w",
        "sample_and_learn",
        "remote",
        "train_batch_size",
        "num_sgd_iter",
        "sgd_minibatch_size",
        "standardize_fields",
        "w",
        "workers",
        "remote_workers",
        "info",
        "count",
        "results",
        "num_steps_sampled",
        "count",
        "num_steps_trained",
        "count",
        "results",
        "logger",
        "isEnabledFor",
        "logging",
        "DEBUG",
        "ray",
        "get",
        "w",
        "get_weights",
        "remote",
        "w",
        "workers",
        "remote_workers",
        "w",
        "weights",
        "p",
        "w",
        "values",
        "k",
        "v",
        "p",
        "items",
        "acc",
        "v",
        "sum",
        "sums",
        "append",
        "acc",
        "logger",
        "debug",
        "format",
        "sums",
        "len",
        "sums",
        "sums",
        "keep_local_weights_in_sync",
        "workers",
        "local_worker",
        "set_weights",
        "ray",
        "get",
        "workers",
        "remote_workers",
        "get_weights",
        "remote",
        "learner_stats",
        "PolicyOptimizer",
        "PolicyOptimizer",
        "stats",
        "round",
        "update_weights_timer",
        "mean",
        "round",
        "learn_timer",
        "mean",
        "learner_stats"
    ],
    "literals": [
        "\"gloo\"",
        "\"This optimizer requires >0 remote workers.\"",
        "\"tcp://{ip}:{port}\"",
        "\"Creating torch process group with leader {}\"",
        "\"Torch process group init completed\"",
        "\"The worker weight sums are {}\"",
        "\"update_weights_time_ms\"",
        "\"learn_time_ms\"",
        "\"learner\""
    ],
    "variables": [
        "logger",
        "learner_stats",
        "num_sgd_iter",
        "train_batch_size",
        "sgd_minibatch_size",
        "standardize_fields",
        "keep_local_weights_in_sync",
        "update_weights_timer",
        "learn_timer",
        "ip",
        "port",
        "address",
        "weights",
        "results",
        "learner_stats",
        "weights",
        "sums",
        "acc"
    ],
    "comments": [
        "Setup the distributed processes.",
        "Get setup tasks in order to throw errors on failure.",
        "Sync up the weights. In principle we don't need this, but it doesn't",
        "add too much overhead and handles the case where the user manually",
        "updates the local weights.",
        "In debug mode, check the allreduce successfully synced the weights.",
        "Sync down the weights. As with the sync up, this is not really",
        "needed unless the user is reading the local weights."
    ],
    "docstrings": [
        "\"\"\"EXPERIMENTAL: torch distributed multi-node SGD.\"\"\""
    ],
    "functions": [
        "step",
        "stats"
    ],
    "classes": [
        "TorchDistributedDataParallelOptimizer"
    ]
}