{
    "identifiers": [
        "csv",
        "re",
        "nltk",
        "nltk",
        "pos_tag",
        "nltk",
        "FreqDist",
        "nltk",
        "corpus",
        "stopwords",
        "sklearn",
        "svm",
        "nltk",
        "tokenize",
        "word_tokenize",
        "RegexpTokenizer",
        "nltk",
        "stem",
        "PorterStemmer",
        "SnowballStemmer",
        "sklearn",
        "feature_extraction",
        "text",
        "TfidfVectorizer",
        "sklearn",
        "naive_bayes",
        "MultinomialNB",
        "GaussianNB",
        "sklearn",
        "linear_model",
        "LogisticRegression",
        "LR",
        "nltk",
        "stem",
        "WordNetLemmatizer",
        "pickle",
        "common",
        "common",
        "numpy",
        "open",
        "file",
        "csv",
        "reader",
        "file",
        "row",
        "reader",
        "trainingData",
        "append",
        "row",
        "open",
        "file",
        "csv",
        "reader",
        "file",
        "row",
        "reader",
        "testData",
        "append",
        "row",
        "read_data",
        "song",
        "trainingData",
        "trainingSong",
        "append",
        "emotionDictionary",
        "song",
        "trainingSong",
        "append",
        "song",
        "song",
        "testData",
        "testSong",
        "append",
        "emotionDictionary",
        "song",
        "testSong",
        "append",
        "song",
        "prepare_dataset",
        "TfidfVectorizer",
        "tokenizer",
        "common",
        "tokenize_text",
        "min_df",
        "ngram_range",
        "sublinear_tf",
        "stop_words",
        "vectorizer",
        "fit_transform",
        "trainingSong",
        "vectorizer",
        "transform",
        "testSong",
        "LR",
        "multi_class",
        "solver",
        "model",
        "fit",
        "train_x",
        "trainingSong",
        "model",
        "score",
        "test_x",
        "testSong",
        "pickle",
        "dump",
        "model",
        "open",
        "model_filename",
        "pickle",
        "dump",
        "vectorizer",
        "open",
        "dictionary"
    ],
    "literals": [
        "\"DataSet/Training.csv\"",
        "\"r\"",
        "\"DataSet/Test.csv\"",
        "\"r\"",
        "\"angry\"",
        "\"happy\"",
        "\"sad\"",
        "\"relaxed\"",
        "\"Training over dataset (Emotion, lyrics):\"",
        "\"english\"",
        "'multinomial'",
        "'newton-cg'",
        "\"Accuracy by applying Logistic Regression: \"",
        "'finalized_model.sav'",
        "'wb'",
        "'dictionary.pkl'",
        "'wb'",
        "\"Model saved as finalized_model.asv and vectorizer as dictionary.pkl\""
    ],
    "variables": [
        "trainingData",
        "testData",
        "reader",
        "reader",
        "emotionDictionary",
        "trainingSong",
        "testSong",
        "vectorizer",
        "train_x",
        "test_x",
        "model",
        "model_filename",
        "dictionary"
    ],
    "comments": [
        "Adding necessary imports",
        "Arrays to store data after reading files",
        "read training data to train model from csv file",
        "read test data to determine Accuracy and test model",
        "dictionary to enumerate emotion",
        "Preparing datatset for model-training as 2D-array containing lyrics - emotion pair",
        "5th element in each song record is emotion",
        "6th element in each song record is lyrics",
        "Convert a collection of raw documents to a matrix of TF-IDF features.",
        "apply tf-idf vectorizer on training and test data",
        "Logistic Regression model",
        "training model over training data",
        "Saving the trained model using pickle"
    ],
    "docstrings": [
        "\"\"\"\nSentiment analysis of text\n\"\"\""
    ],
    "functions": [
        "read_data",
        "prepare_dataset"
    ],
    "classes": []
}