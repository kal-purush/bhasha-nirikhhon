{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "torch",
        "nn",
        "CrossEntropyLoss",
        "pytorch_transformers",
        "modeling_bert",
        "BertForPreTraining",
        "BertPredictionHeadTransform",
        "BertEmbeddings",
        "BertPreTrainingHeads",
        "BertLayerNorm",
        "BertModel",
        "BertEncoder",
        "BertPooler",
        "utils",
        "data_utils",
        "sequence_mask",
        "BertEmbeddings",
        "config",
        "BertEmbeddingsDialog",
        "config",
        "nn",
        "Embedding",
        "config",
        "vocab_size",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "max_position_embeddings",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "type_vocab_size",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "hidden_size",
        "nn",
        "Embedding",
        "config",
        "hidden_size",
        "BertLayerNorm",
        "config",
        "hidden_size",
        "eps",
        "nn",
        "Dropout",
        "config",
        "hidden_dropout_prob",
        "config",
        "input_ids",
        "sep_indices",
        "sep_len",
        "token_type_ids",
        "input_ids",
        "size",
        "torch",
        "arange",
        "seq_length",
        "dtype",
        "torch",
        "device",
        "input_ids",
        "device",
        "position_ids",
        "unsqueeze",
        "expand_as",
        "input_ids",
        "token_type_ids",
        "torch",
        "zeros_like",
        "input_ids",
        "word_embeddings",
        "input_ids",
        "position_embeddings",
        "position_ids",
        "token_type_ids",
        "config",
        "type_vocab_size",
        "token_type_ids_extension",
        "token_type_ids_extension",
        "token_type_ids_extension_mask",
        "token_type_ids",
        "config",
        "type_vocab_size",
        "torch",
        "sum",
        "token_type_ids_extension_mask",
        "token_type_ids_mask",
        "torch",
        "numel",
        "token_type_ids",
        "torch",
        "numel",
        "token_type_ids_mask",
        "token_type_ids",
        "token_type_ids_mask",
        "token_type_embeddings",
        "token_type_ids",
        "token_type_embeddings_extension",
        "token_type_ids_extension",
        "token_type_embeddings",
        "token_type_ids_mask",
        "unsqueeze",
        "token_type_embeddings_extension",
        "token_type_ids_extension_mask",
        "unsqueeze",
        "words_embeddings",
        "position_embeddings",
        "token_type_embeddings",
        "LayerNorm",
        "embeddings",
        "dropout",
        "embeddings",
        "embeddings",
        "BertModel",
        "config",
        "BertModelDialog",
        "config",
        "BertEmbeddingsDialog",
        "config",
        "BertEncoder",
        "config",
        "BertPooler",
        "config",
        "init_weights",
        "new_num_tokens",
        "embeddings",
        "word_embeddings",
        "_get_resized_embeddings",
        "old_embeddings",
        "new_num_tokens",
        "embeddings",
        "new_embeddings",
        "embeddings",
        "word_embeddings",
        "heads_to_prune",
        "layer",
        "heads",
        "heads_to_prune",
        "items",
        "encoder",
        "layer",
        "layer",
        "attention",
        "prune_heads",
        "heads",
        "input_ids",
        "sep_indices",
        "sep_len",
        "token_type_ids",
        "attention_mask",
        "position_ids",
        "head_mask",
        "attention_mask",
        "torch",
        "ones_like",
        "input_ids",
        "token_type_ids",
        "torch",
        "zeros_like",
        "input_ids",
        "attention_mask",
        "unsqueeze",
        "unsqueeze",
        "extended_attention_mask",
        "to",
        "dtype",
        "next",
        "parameters",
        "dtype",
        "extended_attention_mask",
        "head_mask",
        "head_mask",
        "dim",
        "head_mask",
        "unsqueeze",
        "unsqueeze",
        "unsqueeze",
        "unsqueeze",
        "head_mask",
        "expand",
        "config",
        "num_hidden_layers",
        "head_mask",
        "dim",
        "head_mask",
        "unsqueeze",
        "unsqueeze",
        "unsqueeze",
        "head_mask",
        "to",
        "dtype",
        "next",
        "parameters",
        "dtype",
        "config",
        "num_hidden_layers",
        "embeddings",
        "input_ids",
        "sep_indices",
        "sep_indices",
        "sep_len",
        "sep_len",
        "token_type_ids",
        "token_type_ids",
        "encoder",
        "embedding_output",
        "extended_attention_mask",
        "head_mask",
        "head_mask",
        "encoder_outputs",
        "pooler",
        "sequence_output",
        "sequence_output",
        "pooled_output",
        "encoder_outputs",
        "outputs",
        "BertForPreTraining",
        "config",
        "BertForPretrainingDialog",
        "config",
        "BertModelDialog",
        "config",
        "BertPreTrainingHeads",
        "config",
        "init_weights",
        "tie_weights",
        "_tie_or_clone_weights",
        "cls",
        "predictions",
        "decoder",
        "bert",
        "embeddings",
        "word_embeddings",
        "input_ids",
        "sep_indices",
        "sep_len",
        "token_type_ids",
        "attention_mask",
        "masked_lm_labels",
        "next_sentence_label",
        "position_ids",
        "head_mask",
        "bert",
        "input_ids",
        "sep_indices",
        "sep_indices",
        "sep_len",
        "sep_len",
        "token_type_ids",
        "token_type_ids",
        "attention_mask",
        "attention_mask",
        "outputs",
        "cls",
        "sequence_output",
        "pooled_output",
        "prediction_scores",
        "seq_relationship_score",
        "outputs",
        "masked_lm_labels",
        "next_sentence_label",
        "CrossEntropyLoss",
        "ignore_index",
        "loss_fct",
        "prediction_scores",
        "view",
        "config",
        "vocab_size",
        "masked_lm_labels",
        "view",
        "loss_fct",
        "seq_relationship_score",
        "view",
        "next_sentence_label",
        "view",
        "masked_lm_loss",
        "next_sentence_loss",
        "total_loss",
        "outputs",
        "outputs"
    ],
    "literals": [],
    "variables": [
        "word_embeddings",
        "position_embeddings",
        "token_type_embeddings",
        "token_type_embeddings_extension",
        "sep_embeddings",
        "LayerNorm",
        "dropout",
        "config",
        "seq_length",
        "position_ids",
        "position_ids",
        "token_type_ids",
        "words_embeddings",
        "position_embeddings",
        "token_type_ids_extension",
        "token_type_ids_extension_mask",
        "token_type_ids_extension",
        "token_type_ids_mask",
        "token_type_ids",
        "token_type_embeddings",
        "token_type_embeddings_extension",
        "token_type_embeddings",
        "embeddings",
        "embeddings",
        "embeddings",
        "embeddings",
        "encoder",
        "pooler",
        "old_embeddings",
        "new_embeddings",
        "word_embeddings",
        "attention_mask",
        "token_type_ids",
        "extended_attention_mask",
        "extended_attention_mask",
        "extended_attention_mask",
        "head_mask",
        "head_mask",
        "head_mask",
        "head_mask",
        "head_mask",
        "embedding_output",
        "encoder_outputs",
        "sequence_output",
        "pooled_output",
        "outputs",
        "bert",
        "cls",
        "outputs",
        "sequence_output",
        "pooled_output",
        "prediction_scores",
        "seq_relationship_score",
        "outputs",
        "loss_fct",
        "masked_lm_loss",
        "next_sentence_loss",
        "total_loss",
        "outputs"
    ],
    "comments": [
        "add support for additional segment embeddings. Supporting 10 additional embedding as of now",
        "adding specialized embeddings for sep tokens",
        "self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load",
        "any TensorFlow checkpoint file",
        "We create a 3D attention mask from a 2D tensor mask.",
        "Sizes are [batch_size, 1, 1, to_seq_length]",
        "So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",
        "this attention mask is more simple than the triangular masking of causal attention",
        "used in OpenAI GPT, we just need to prepare the broadcast dimension here.",
        "Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
        "masked positions, this operation will create a tensor which is 0.0 for",
        "positions we want to attend and -10000.0 for masked positions.",
        "Since we are adding it to the raw scores before the softmax, this is",
        "effectively the same as removing these entirely.",
        "fp16 compatibility",
        "Prepare head mask if needed",
        "1.0 in head_mask indicate we keep the head",
        "attention_probs has shape bsz x n_heads x N x N",
        "input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]",
        "and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]",
        "We can specify head_mask for each layer",
        "switch to fload if need + fp16 compatibility",
        "add hidden_states and attentions if they are here",
        "sequence_output, pooled_output, (hidden_states), (attentions)",
        "add hidden states and attention if they are here",
        "(loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)"
    ],
    "docstrings": [
        "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"",
        "\"\"\" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n        \"\"\""
    ],
    "functions": [
        "forward",
        "_resize_token_embeddings",
        "_prune_heads",
        "forward",
        "tie_weights",
        "forward"
    ],
    "classes": [
        "BertEmbeddingsDialog",
        "BertModelDialog",
        "BertForPretrainingDialog"
    ]
}