{
    "identifiers": [
        "interact",
        "ArgumentParser",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "torch",
        "cuda",
        "is_available",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "parse_args",
        "logging",
        "basicConfig",
        "level",
        "logging",
        "INFO",
        "logging",
        "getLogger",
        "logger",
        "info",
        "pformat",
        "args",
        "args",
        "model_checkpoint",
        "logging",
        "error",
        "random",
        "seed",
        "args",
        "seed",
        "torch",
        "random",
        "manual_seed",
        "args",
        "seed",
        "torch",
        "cuda",
        "manual_seed",
        "args",
        "seed",
        "logger",
        "info",
        "BertTokenizer",
        "OpenAIGPTLMHeadModel",
        "args",
        "gpt2",
        "GPT2LMHeadModel",
        "tokenizer_class",
        "from_pretrained",
        "args",
        "model_checkpoint",
        "do_lower_case",
        "model_class",
        "from_pretrained",
        "args",
        "model_checkpoint",
        "model",
        "to",
        "args",
        "device",
        "model",
        "eval",
        "model",
        "tokenizer",
        "args",
        "eva_model",
        "raw_text",
        "history",
        "obj",
        "isinstance",
        "obj",
        "tokenizer",
        "convert_tokens_to_ids",
        "tokenizer",
        "tokenize",
        "obj",
        "isinstance",
        "obj",
        "n",
        "tokenize",
        "o",
        "n",
        "o",
        "obj",
        "items",
        "tokenize",
        "o",
        "o",
        "obj",
        "join",
        "raw_text",
        "replace",
        "history",
        "append",
        "tokenize",
        "raw_text",
        "torch",
        "no_grad",
        "sample_sequence",
        "history",
        "tokenizer",
        "model",
        "args",
        "history",
        "append",
        "out_ids",
        "history",
        "args",
        "max_history",
        "tokenizer",
        "decode",
        "out_ids",
        "skip_special_tokens",
        "out_text"
    ],
    "literals": [
        "'--gpt2'",
        "'store_true'",
        "\"use gpt2\"",
        "\"--model_checkpoint\"",
        "\"./models/\"",
        "\"Path, url or short name of the model\"",
        "\"--max_history\"",
        "\"Number of previous utterances to keep in history\"",
        "\"--device\"",
        "\"cuda\"",
        "\"cpu\"",
        "\"Device (cuda or cpu)\"",
        "\"--no_sample\"",
        "'store_true'",
        "\"Set to use greedy decoding instead of sampling\"",
        "\"--max_length\"",
        "\"Maximum length of the output utterances\"",
        "\"--min_length\"",
        "\"Minimum length of the output utterances\"",
        "\"--seed\"",
        "\"Seed\"",
        "\"--temperature\"",
        "\"Sampling softmax temperature\"",
        "\"--top_k\"",
        "\"Filter top-k tokens before sampling (<=0: no filtering)\"",
        "\"--top_p\"",
        "\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\"",
        "\"\"",
        "\"Checkpoint needed!\"",
        "\"Get pretrained model and tokenizer\"",
        "\" \"",
        "\" \"",
        "\"\""
    ],
    "variables": [
        "parser",
        "args",
        "logger",
        "tokenizer_class",
        "model_class",
        "tokenizer",
        "model",
        "history",
        "model",
        "tokenizer",
        "args",
        "raw_text",
        "out_ids",
        "history",
        "out_text"
    ],
    "comments": [
        "print(out_text)"
    ],
    "docstrings": [],
    "functions": [
        "eva_model",
        "chat_response",
        "tokenize"
    ],
    "classes": []
}