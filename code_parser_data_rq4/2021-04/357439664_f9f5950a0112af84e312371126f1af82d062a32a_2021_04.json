{
    "identifiers": [
        "numpy",
        "np",
        "random",
        "pandas",
        "pd",
        "matplotlib",
        "pyplot",
        "plt",
        "file_path",
        "np",
        "array",
        "pd",
        "read_csv",
        "file_path",
        "header",
        "data",
        "shape",
        "data",
        "shape",
        "i",
        "i",
        "len",
        "data",
        "random",
        "shuffle",
        "index",
        "data",
        "index",
        "data",
        "num_features",
        "reshape",
        "num_features",
        "training_examples",
        "data",
        "reshape",
        "training_examples",
        "input_X",
        "output_Y",
        "x",
        "y",
        "np",
        "array",
        "x",
        "T",
        "y",
        "T",
        "data",
        "transpose",
        "np",
        "random",
        "shuffle",
        "data",
        "np",
        "array",
        "data",
        "transpose",
        "np",
        "array",
        "data",
        "transpose",
        "x",
        "y",
        "x",
        "np",
        "mean",
        "x",
        "axis",
        "np",
        "std",
        "x",
        "axis",
        "x",
        "avg",
        "std",
        "x",
        "input_X",
        "np",
        "random",
        "rand",
        "input_X",
        "shape",
        "np",
        "zeros",
        "input_X",
        "W",
        "B",
        "np",
        "dot",
        "W",
        "input_X",
        "B",
        "Y_hat",
        "output_Y",
        "Y_hat",
        "np",
        "mean",
        "Y_hat",
        "output_Y",
        "total_loss",
        "W",
        "B",
        "Y_hat",
        "output_Y",
        "input_X",
        "np",
        "dot",
        "Y_hat",
        "output_Y",
        "input_X",
        "T",
        "output_Y",
        "shape",
        "np",
        "sum",
        "Y_hat",
        "output_Y",
        "keepdims",
        "output_Y",
        "shape",
        "W_gradient",
        "B_gradient",
        "W",
        "B",
        "W_gradient",
        "B_gradient",
        "learning_rate",
        "W",
        "learning_rate",
        "W_gradient",
        "B",
        "learning_rate",
        "B_gradient",
        "W_update",
        "B_update",
        "read",
        "normalization",
        "input_X",
        "output_Y",
        "input_X",
        "shape",
        "input_X",
        "shape",
        "Weight_initialization",
        "input_X",
        "Bias_initialazation",
        "i",
        "iteration",
        "i",
        "batch_size",
        "input_X",
        "shape",
        "min",
        "start",
        "batch_size",
        "output_Y",
        "shape",
        "X",
        "start",
        "end",
        "Y",
        "start",
        "end",
        "linear_forward",
        "X_train",
        "W",
        "B",
        "i",
        "compute_loss",
        "Y_train",
        "Y_hat",
        "i",
        "loss",
        "compute_gradient",
        "W",
        "B",
        "Y_hat",
        "Y_train",
        "X_train",
        "apply_gradient",
        "W",
        "B",
        "W_gradient",
        "B_gradient",
        "learning_rate",
        "np",
        "squeeze",
        "W",
        "np",
        "squeeze",
        "B",
        "np",
        "mean",
        "w",
        "X",
        "b",
        "Y",
        "iteration",
        "w",
        "b",
        "error_1",
        "np",
        "ones",
        "training_examples",
        "np",
        "insert",
        "input_X",
        "num_features",
        "b_add",
        "axis",
        "X_train_2",
        "T",
        "output_Y",
        "T",
        "np",
        "array",
        "np",
        "dot",
        "np",
        "dot",
        "np",
        "linalg",
        "inv",
        "np",
        "dot",
        "X_train_2",
        "T",
        "X_train_2",
        "X_train_2",
        "T",
        "Y_train_2",
        "np",
        "mean",
        "np",
        "dot",
        "theta",
        "T",
        "X_train_2",
        "T",
        "Y_train_2",
        "T",
        "theta",
        "theta",
        "theta_w",
        "theta_b",
        "error_2",
        "input_X",
        "flatten",
        "output_Y",
        "flatten",
        "X",
        "flatten",
        "Y",
        "flatten",
        "plt",
        "figure",
        "fig",
        "suptitle",
        "fontsize",
        "plt",
        "subplot",
        "plt",
        "scatter",
        "x_1",
        "y",
        "plt",
        "plot",
        "x_1",
        "w",
        "x_1",
        "b",
        "c",
        "plt",
        "title",
        "fontsize",
        "plt",
        "subplot",
        "plt",
        "scatter",
        "x",
        "y",
        "plt",
        "plot",
        "x",
        "theta_w",
        "x",
        "theta_b",
        "c",
        "plt",
        "title",
        "fontsize",
        "plt",
        "savefig",
        "plt",
        "show"
    ],
    "literals": [
        "\"__main__\"",
        "\"data.csv\"",
        "\"After %d iterations, the error is %g\"",
        "\"The result of Gradient Descent\\n After %d iterations, Weight: %g Bias: %g Error: %g\"",
        "\"The result of Normal Equation\\n Weight: %g Bias: %g Error: %g\"",
        "\"Linear Regression\"",
        "'r'",
        "\"Linear Regression \\nwith Gradient Descent\"",
        "'r'",
        "\"Linear Regression \\nwith Normal Equation\"",
        "\"Linear_Regression.png\""
    ],
    "variables": [
        "data",
        "num_features",
        "training_examples",
        "index",
        "data",
        "input_X",
        "output_Y",
        "data",
        "data",
        "x",
        "y",
        "avg",
        "std",
        "x",
        "Y_hat",
        "total_loss",
        "W_gradient",
        "B_gradient",
        "W_update",
        "B_update",
        "input_X",
        "output_Y",
        "X",
        "Y",
        "training_examples",
        "num_features",
        "batch_size",
        "start",
        "W",
        "B",
        "iteration",
        "start",
        "end",
        "X_train",
        "Y_train",
        "Y_hat",
        "loss",
        "W_gradient",
        "B_gradient",
        "W",
        "B",
        "w",
        "b",
        "error_1",
        "b_add",
        "X_train_2",
        "X_train_2",
        "Y_train_2",
        "theta",
        "error_2",
        "theta_w",
        "theta_b",
        "x",
        "y",
        "x_1",
        "y_1",
        "fig"
    ],
    "comments": [
        "-*- encoding: utf-8 -*-",
        "shuffle the data set",
        "Method 1",
        "shape: (n,m)",
        "shape: (1,m)",
        "shuffle data set",
        "Method 2",
        "x.T shape = (m,n)",
        "y.T shape = (m,1)",
        "data.shape = (2,m,)",
        "data.shape = (m,2,)",
        "shuffle the m training examples",
        "extract input_x and output_y: x.shape = (m,n) y.shape = (m,1)",
        "feature scaling",
        "output_Y.shape[1] = number of training examples",
        "Gradient Clipping (for solving gradient explosion)",
        "You can use either this or AdamOptimizer (see below) to prevent gradient explosion",
        "I choose the threshold as 0.5",
        "threshold_norm = 0.5",
        "if np.linalg.norm(W_gradient)>threshold_norm:",
        "W_gradient = threshold_norm*W_gradient/np.linalg.norm(W_gradient)",
        "if np.linalg.norm(B_gradient)>threshold_norm:",
        "B_gradient = threshold_norm*B_gradient/np.linalg.norm(B_gradient)",
        "apply gradient descent (update parameters)",
        "Method 1: gradient descent",
        "use Batch-training",
        "forward-prop",
        "compute loss",
        "back-prop with Adam Optimizer",
        "compute original gradient",
        "# Momentum and RMSprop",
        "if i == 0:",
        "v_w,v_b = 0.1*W_gradient,0.1*B_gradient",
        "s_w,s_b = 0.001*W_gradient**2,0.001*B_gradient**2",
        "else:",
        "v_w = 0.9*v_w+0.1*W_gradient",
        "v_b = 0.9*v_b+0.1*B_gradient",
        "s_w = 0.999*s_w +0.001*W_gradient**2",
        "s_b = 0.999*s_b + 0.001*B_gradient**2",
        "W_gradient = v_w/(np.sqrt(s_w)+pow(10,-8))",
        "B_gradient = v_b/(np.sqrt(s_b)+pow(10,-8))",
        "gradient descent",
        "Method 2: Normal Equation",
        "add a col for X to use intercept for normal equation",
        "shape: (100,2)",
        "shape: (100,1)",
        "Visualization"
    ],
    "docstrings": [
        "'''\n@File    :   Linear_Regression_Model.py\n@Author  :   Yixing Lu\n@Time    :   2021/04/12 14:47:10\n@Software : VS Code\n'''"
    ],
    "functions": [
        "read",
        "shuffle_data",
        "normalization",
        "Weight_initialization",
        "Bias_initialazation",
        "linear_forward",
        "compute_loss",
        "compute_gradient",
        "apply_gradient"
    ],
    "classes": []
}