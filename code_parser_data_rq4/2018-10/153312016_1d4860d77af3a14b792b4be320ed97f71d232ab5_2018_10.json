{
    "identifiers": [
        "inputs",
        "size",
        "dim",
        "name",
        "np",
        "sqrt",
        "dim",
        "tf",
        "Variable",
        "tf",
        "random_uniform",
        "size",
        "dim",
        "std",
        "std",
        "tf",
        "nn",
        "embedding_lookup",
        "emb",
        "inputs",
        "name",
        "name",
        "lookup",
        "x",
        "out_len",
        "name_w",
        "name_b",
        "x",
        "shape",
        "tf",
        "Variable",
        "tf",
        "truncated_normal",
        "tot_nodes",
        "out_len",
        "stddev",
        "name",
        "name_w",
        "tf",
        "Variable",
        "tf",
        "constant",
        "shape",
        "out_len",
        "name",
        "name_b",
        "tf",
        "matmul",
        "x",
        "W_dense",
        "b_dense",
        "layer",
        "dropout_keep_prob",
        "tf",
        "nn",
        "dropout",
        "layer",
        "dropout_keep_prob",
        "layer",
        "tf",
        "nn",
        "relu",
        "layer",
        "layer",
        "out_len",
        "layer",
        "dense_NN",
        "layer",
        "out_len",
        "relu_layer",
        "layer_",
        "dense_NN",
        "layer_",
        "out_len",
        "relu_layer",
        "layer_",
        "dense_NN",
        "shortcut",
        "out_len",
        "tf",
        "add",
        "layer_",
        "shortcut",
        "relu_layer",
        "added_",
        "added_",
        "x",
        "W_shape",
        "b_shape",
        "pad_length",
        "name_w",
        "name_b",
        "tf",
        "Variable",
        "tf",
        "truncated_normal",
        "W_shape",
        "stddev",
        "name",
        "name_w",
        "tf",
        "Variable",
        "tf",
        "constant",
        "shape",
        "b_shape",
        "name",
        "name_b",
        "tf",
        "nn",
        "conv2d",
        "x",
        "W1",
        "strides",
        "padding",
        "name",
        "tf",
        "nn",
        "relu",
        "tf",
        "nn",
        "bias_add",
        "conv",
        "b1",
        "name",
        "tf",
        "nn",
        "max_pool",
        "h",
        "ksize",
        "pad_length",
        "strides",
        "padding",
        "name",
        "tf",
        "reshape",
        "pooled",
        "out_nodes",
        "pool_flat"
    ],
    "literals": [
        "'res_w'",
        "'res_b'",
        "'res_w2'",
        "'res_b2'",
        "'shortcut_w'",
        "'shortcut_b'",
        "\"VALID\"",
        "\"conv\"",
        "\"relu\"",
        "'VALID'",
        "\"pool\""
    ],
    "variables": [
        "std",
        "emb",
        "lookup",
        "tot_nodes",
        "W_dense",
        "b_dense",
        "shortcut",
        "layer_",
        "layer_",
        "layer_",
        "layer_",
        "shortcut",
        "added_",
        "added_",
        "W1",
        "b1",
        "conv",
        "h",
        "pooled",
        "pool_flat"
    ],
    "comments": [
        "generate an embedding layer that can be trained on (emblayer) ###",
        "inputs is a list of indices",
        "size is the number of unique indices (look for max index to achieve this if ordered)",
        "dim is the number of embedded numbers",
        "print(lookup.shape)",
        "input_x_cat1 = input_x[:,(input_name_len + input_itemdesc_len)] # just a row of categories. can be multiple indices",
        "cat1_emb = embed([i for i in range(dict_cat1_len)],dict_cat1_len,cat1_emb_size, name= 'cat1_emb')",
        "cat1_emb_lookup = tf.nn.embedding_lookup(cat1_emb,input_x_cat1)",
        "cat1_emb_lookup is the matrix that can be used as input for NN now",
        "General Neural Network (DNN) ###",
        "\"W2\"",
        "\"b2\"",
        "Dropout layer (dropNN) ###",
        "ReLU layer (relu) ###",
        "ResNet Simple Model (resnet) ###",
        "Convolutional Neural Network (CNN) ###",
        "x is the expanded lookup tables that will be trained",
        "\"W1\"",
        "\"b1\"",
        "tf.layers.conv2d is also used, with  more parameters. Probably a slightly higher API because of that.",
        "print('shape of CNN output:' + str(conv.shape))",
        "print('shape after ReLU: ' + str(h.shape))",
        "print('shape after max pooling: ' + str(pooled.shape))",
        "print(\"shape after flattening:\" + str(pool_flat.shape))",
        "h_drop = tf.nn.dropout(pool_flat, dropout_keep_prob)",
        "print('shape after dropout: ' + str(h_drop.shape))",
        "name_emb_lookup_expand = tf.expand_dims(name_emb_lookup,-1)",
        "W_shape_name = [1,name_emb_size,1,out_nodes]",
        "b_shape_name = out_nodes # same as last dimension in W",
        "layers_name = CNN(name_emb_lookup_expand,W_shape_name,b_shape_name,name_pad_size,\"W_name\", \"b_name\")"
    ],
    "docstrings": [],
    "functions": [
        "embed",
        "dense_NN",
        "dropout_layer",
        "relu_layer",
        "residual_block",
        "CNN"
    ],
    "classes": []
}