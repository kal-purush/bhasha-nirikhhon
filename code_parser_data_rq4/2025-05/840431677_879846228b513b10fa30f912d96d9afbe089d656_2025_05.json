{
    "identifiers": [
        "automation",
        "pipelines",
        "LLMCompressorLMEvalPipeline",
        "automation",
        "hpo",
        "BaseHPO",
        "clearml",
        "automation",
        "UniformParameterRange",
        "UniformIntegerParameterRange",
        "dataset_name",
        "vision_samples",
        "text_samples",
        "max_seq_len",
        "processor",
        "automation",
        "datasets",
        "load_vlm_messages",
        "automation",
        "datasets",
        "calibration",
        "DATASET_PATH",
        "TEXT_SUBSET",
        "VISION_SUBSET",
        "messages",
        "processor",
        "message",
        "messages",
        "content",
        "message",
        "content",
        "content_",
        "append",
        "content",
        "content_",
        "append",
        "content",
        "messages_",
        "append",
        "message",
        "content_",
        "processor",
        "apply_chat_template",
        "messages_",
        "add_generation_prompt",
        "images",
        "processor",
        "input",
        "max_length",
        "max_seq_len",
        "truncation",
        "tokenized_input",
        "get",
        "tokenized_input",
        "get",
        "k",
        "v",
        "tolist",
        "hasattr",
        "v",
        "v",
        "k",
        "v",
        "tokenized_input",
        "items",
        "tokenized_input",
        "load_vlm_messages",
        "DATASET_PATH",
        "VISION_SUBSET",
        "TEXT_SUBSET",
        "num_samples",
        "vision_samples",
        "text_samples",
        "processor",
        "processor",
        "message_processor",
        "message_processor",
        "batch",
        "torch",
        "len",
        "batch",
        "k",
        "v",
        "batch",
        "items",
        "v",
        "k",
        "torch",
        "LongTensor",
        "v",
        "k",
        "torch",
        "tensor",
        "v",
        "dtype",
        "torch",
        "bfloat16",
        "torch",
        "tensor",
        "v",
        "collated",
        "task",
        "task",
        "get_reported_scalars",
        "results",
        "results",
        "results",
        "results",
        "results",
        "results",
        "results",
        "results",
        "gsm8k_score",
        "mmlu_score",
        "mmmu_score",
        "arc_score",
        "task",
        "get_logger",
        "report_scalar",
        "title",
        "series",
        "iteration",
        "value",
        "average_score",
        "LLMCompressorLMEvalPipeline",
        "project_name",
        "pipeline_name",
        "model_id",
        "execution_queues",
        "job_end_callback",
        "average_scores",
        "parameters",
        "llmcompressor_kwargs",
        "recipe",
        "dataset_loader",
        "data_collator",
        "lmeval_kwargs",
        "pipeline",
        "create_pipeline",
        "BaseHPO",
        "project_name",
        "task_name",
        "report_period_min",
        "optimizer",
        "optuna_sampler",
        "optuna_sampler_kwargs",
        "objective_metric_title",
        "objective_metric_series",
        "objective_metric_sign",
        "total_max_jobs",
        "max_number_of_concurrent_tasks",
        "pool_period_min",
        "max_iteration_per_job",
        "spawn_project",
        "base_task_id",
        "pipeline",
        "id",
        "hpo_task",
        "add_parameter",
        "UniformParameterRange",
        "min_value",
        "max_value",
        "hpo_task",
        "add_parameter",
        "UniformIntegerParameterRange",
        "min_value",
        "max_value",
        "step_size",
        "hpo_task",
        "add_parameter",
        "UniformIntegerParameterRange",
        "min_value",
        "max_value",
        "step_size",
        "hpo_task",
        "execute_remotely"
    ],
    "literals": [
        "\"content\"",
        "\"type\"",
        "\"image\"",
        "\"type\"",
        "\"image\"",
        "\"image\"",
        "\"role\"",
        "\"role\"",
        "\"content\"",
        "\"text\"",
        "\"images\"",
        "\"pixel_values\"",
        "\"pixel_values\"",
        "\"image_sizes\"",
        "\"image_sizes\"",
        "\"tolist\"",
        "\"input_ids\"",
        "\"pixel_values\"",
        "\"gsm8k\"",
        "\"gsm8k\"",
        "\"exact_match,strict-match\"",
        "\"y\"",
        "\"mmlu\"",
        "\"mmlu\"",
        "\"acc,none\"",
        "\"y\"",
        "\"mmmu_val\"",
        "\"mmmu_val\"",
        "\"acc,none\"",
        "\"y\"",
        "\"arc_challenge\"",
        "\"arc_challenge\"",
        "\"acc,none\"",
        "\"y\"",
        "\"score\"",
        "\"average\"",
        "\"Mistral/Mistral-Small-3.1-24B-Instruct\"",
        "\"Mistral-Small-3_1-24B-Instruct-2503/W4A16/Pipeline\"",
        "\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"",
        "\"oneshot-a100x2\"",
        "\"oneshot-a100x2\"",
        "\"oneshot-a100x2\"",
        "\"oneshot-a100x2\"",
        "\"oneshot-a100x2\"",
        "\"dampening_frac\"",
        "\"default\"",
        "\"param_type\"",
        "\"float\"",
        "\"recipe_arg\"",
        "\"text_samples\"",
        "\"default\"",
        "\"param_type\"",
        "\"int\"",
        "\"recipe_arg\"",
        "\"vision_samples\"",
        "\"default\"",
        "\"param_type\"",
        "\"int\"",
        "\"recipe_arg\"",
        "\"recipe\"",
        "\"dataset_loader\"",
        "\"data_collator\"",
        "\"tracing_class\"",
        "\"TraceableMistral3ForConditionalGeneration\"",
        "\"model_class\"",
        "\"AutoModelForImageTextToText\"",
        "\"max_seq_len\"",
        "\"max_memory_per_gpu\"",
        "\"auto\"",
        "\"evaluation_mmlu\"",
        "\"tasks\"",
        "\"mmlu\"",
        "\"monitor_metrics\"",
        "\"mmlu\"",
        "\"acc,none\"",
        "\"model_args\"",
        "\"gpu_memory_utilization=0.5,enable_chunked_prefill=True,max_model_len=8192\"",
        "\"apply_chat_template\"",
        "\"fewshot_as_multiturn\"",
        "\"num_fewshot\"",
        "\"batch_size\"",
        "\"auto\"",
        "\"packages\"",
        "\"numpy==2.1\"",
        "\"evaluation_gsm8k\"",
        "\"tasks\"",
        "\"gsm8k\"",
        "\"monitor_metrics\"",
        "\"gsm8k\"",
        "\"exact_match,strict-match\"",
        "\"model_args\"",
        "\"gpu_memory_utilization=0.9,enable_chunked_prefill=True,max_model_len=8192\"",
        "\"apply_chat_template\"",
        "\"fewshot_as_multiturn\"",
        "\"num_fewshot\"",
        "\"batch_size\"",
        "\"auto\"",
        "\"packages\"",
        "\"numpy==2.1\"",
        "\"evaluation_arc\"",
        "\"tasks\"",
        "\"arc_challenge\"",
        "\"monitor_metrics\"",
        "\"arc_challenge\"",
        "\"acc,none\"",
        "\"model_args\"",
        "\"gpu_memory_utilization=0.5,enable_chunked_prefill=True,max_model_len=8192\"",
        "\"apply_chat_template\"",
        "\"fewshot_as_multiturn\"",
        "\"num_fewshot\"",
        "\"batch_size\"",
        "\"auto\"",
        "\"packages\"",
        "\"numpy==2.1\"",
        "\"evaluation_mmmu\"",
        "\"model\"",
        "\"vllm-vlm\"",
        "\"tasks\"",
        "\"mmmu_val\"",
        "\"monitor_metrics\"",
        "\"mmmu_val\"",
        "\"acc,none\"",
        "\"model_args\"",
        "\"gpu_memory_utilization=0.5,enable_chunked_prefill=True,max_model_len=8192,max_images=8\"",
        "\"apply_chat_template\"",
        "\"batch_size\"",
        "\"auto\"",
        "\"packages\"",
        "\"numpy==2.1\"",
        "\"Mistral/Mistral-Small-3.1-24B-Instruct\"",
        "\"Mistral-Small-3_1-24B-Instruct-2503/W4A16/HPO\"",
        "\"Optuna\"",
        "\"TPESampler\"",
        "\"n_startup_trials\"",
        "\"score\"",
        "\"average\"",
        "\"max\"",
        "\"Mistral/Mistral-Small-3.1-24B-Instruct/W4A16_hpo\"",
        "\"Args/dampening_frac\"",
        "\"Args/text_samples\"",
        "\"Args/vision_samples\""
    ],
    "variables": [
        "recipe",
        "messages_",
        "images",
        "content_",
        "images",
        "input",
        "tokenized_input",
        "tokenized_input",
        "tokenized_input",
        "tokenized_input",
        "collated",
        "collated",
        "k",
        "collated",
        "k",
        "collated",
        "k",
        "results",
        "gsm8k_score",
        "gsm8k_score",
        "mmlu_score",
        "mmlu_score",
        "mmmu_score",
        "mmmu_score",
        "arc_score",
        "arc_score",
        "average_score",
        "pipeline",
        "hpo_task"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\"\nquant_stage:\n  quant_modifiers:\n    GPTQModifier:\n      ignore: [\"language_model.lm_head\", \"re:vision_tower.*\", \"re:multi_modal_projector.*\"]\n      sequential_targets: [\"MistralDecoderLayer\"]\n      dampening_frac: $dampening_frac\n      config_groups:\n        group0:\n          targets: [\"Linear\"]\n          weights:\n            num_bits: 4\n            type: \"int\"\n            strategy: \"group\"\n            group_size: 128\n            symmetric: true\n            actorder: \"weight\"\n            observer: \"mse\"\n\"\"\""
    ],
    "functions": [
        "dataset_loader",
        "message_processor",
        "data_collator",
        "average_scores"
    ],
    "classes": []
}