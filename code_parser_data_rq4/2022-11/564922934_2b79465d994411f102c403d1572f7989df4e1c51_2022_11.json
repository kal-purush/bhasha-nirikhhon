{
    "identifiers": [
        "os",
        "path",
        "sys",
        "cv2",
        "numpy",
        "numpy",
        "np",
        "torch",
        "PIL",
        "Image",
        "clip",
        "clip",
        "transformers",
        "CLIPProcessor",
        "CLIPModel",
        "pipeline",
        "modules",
        "paths",
        "modules",
        "shared",
        "modelloader",
        "repositories",
        "CodeFormer",
        "facelib",
        "detection",
        "yolov5face",
        "utils",
        "general",
        "xyxy2xywh",
        "xywh2xyxy",
        "boxes",
        "shape",
        "isinstance",
        "boxes",
        "torch",
        "Tensor",
        "boxes",
        "clamp_",
        "shape",
        "boxes",
        "clamp_",
        "shape",
        "boxes",
        "clamp_",
        "shape",
        "boxes",
        "clamp_",
        "shape",
        "boxes",
        "clip",
        "shape",
        "boxes",
        "clip",
        "shape",
        "parent",
        "Image",
        "child",
        "Image",
        "child",
        "width",
        "child",
        "height",
        "cv2",
        "matchTemplate",
        "np",
        "array",
        "parent",
        "np",
        "array",
        "child",
        "cv2",
        "TM_CCOEFF_NORMED",
        "cv2",
        "minMaxLoc",
        "res",
        "max_loc",
        "top_left",
        "w",
        "top_left",
        "h",
        "center_x",
        "center_y",
        "os",
        "path",
        "join",
        "modules",
        "paths",
        "models_path",
        "modelloader",
        "load_models",
        "model_dir",
        "model_url",
        "model_name",
        "torch",
        "hub",
        "load",
        "model_path",
        "sys",
        "modules",
        "image",
        "Image",
        "prompt",
        "model",
        "image",
        "results",
        "crop",
        "crop",
        "cropped",
        "l",
        "append",
        "Image",
        "fromarray",
        "crop",
        "len",
        "l",
        "image",
        "shared",
        "device",
        "clip",
        "load",
        "device",
        "device",
        "torch",
        "stack",
        "preprocess",
        "im",
        "im",
        "l",
        "to",
        "device",
        "torch",
        "no_grad",
        "model",
        "encode_image",
        "images",
        "image_features",
        "image_features",
        "norm",
        "dim",
        "keepdim",
        "image_features",
        "cpu",
        "numpy",
        "torch",
        "tensor",
        "cuda",
        "torch",
        "tensor",
        "cuda",
        "preprocess",
        "im",
        "im",
        "l",
        "torch",
        "tensor",
        "np",
        "stack",
        "images",
        "cuda",
        "image_input",
        "image_mean",
        "image_input",
        "image_std",
        "torch",
        "no_grad",
        "model",
        "encode_image",
        "image_input",
        "image_features",
        "image_features",
        "norm",
        "dim",
        "keepdim",
        "similarity_list",
        "N",
        "len",
        "similarity_list",
        "similarity_list",
        "sorted",
        "results",
        "key",
        "x",
        "x",
        "reverse",
        "index",
        "score",
        "results",
        "N",
        "scores",
        "append",
        "score",
        "top_images",
        "append",
        "l",
        "index",
        "scores",
        "top_images",
        "torch",
        "no_grad",
        "model",
        "encode_text",
        "clip",
        "tokenize",
        "prompt",
        "to",
        "device",
        "text_encoded",
        "text_encoded",
        "norm",
        "dim",
        "keepdim",
        "text_encoded",
        "cpu",
        "numpy",
        "image_features",
        "cpu",
        "numpy",
        "T",
        "similarity",
        "similarity_top",
        "similarity",
        "N",
        "imgs",
        "cv2",
        "matchTemplate",
        "numpy",
        "array",
        "image",
        "numpy",
        "array",
        "cv2",
        "TM_SQDIFF",
        "cv2",
        "minMaxLoc",
        "res",
        "min_loc",
        "top_left",
        "width",
        "top_left",
        "height",
        "top_left",
        "bottom_right",
        "top_left",
        "bottom_right"
    ],
    "literals": [
        "'yolov5m6.pt'",
        "'https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m6.pt'",
        "\"yolo\"",
        "'.pt'",
        "'ultralytics/yolov5'",
        "'custom'",
        "'models'",
        "\"im\"",
        "\"ViT-B/32\""
    ],
    "variables": [
        "boxes",
        "boxes",
        "w",
        "h",
        "res",
        "min_val",
        "max_val",
        "min_loc",
        "max_loc",
        "top_left",
        "center_x",
        "center_y",
        "model_name",
        "model_url",
        "model_dir",
        "model_path",
        "model",
        "results",
        "cropped",
        "l",
        "l",
        "device",
        "model",
        "preprocess",
        "images",
        "image_features",
        "image_mean",
        "image_std",
        "images",
        "image_input",
        "image_features",
        "results",
        "results",
        "top_images",
        "scores",
        "text_encoded",
        "similarity",
        "similarity",
        "scores",
        "imgs",
        "res",
        "min_val",
        "max_val",
        "min_loc",
        "max_loc",
        "top_left",
        "bottom_right"
    ],
    "comments": [
        "Original project: https://github.com/Vishnunkumar/clipcrop/blob/main/clipcrop/clipcrop.py",
        "Clip boxes (xyxy) to image shape (height, width)",
        "faster individually",
        "x1",
        "y1",
        "x2",
        "y2",
        "np.array (faster grouped)",
        "x1, x2",
        "y1, y2",
        "If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum",
        "Model",
        "Prevent BLIP crossfire breakage",
        "Load image into YOLO parser",
        "includes NMS",
        "Crop each image result to an array",
        "Take out cropped YOLO images, and get the features?",
        "@title Crop",
        "Encode and normalize the description using CLIP",
        "Retrieve the description vector and the photo vectors",
        "If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum"
    ],
    "docstrings": [],
    "functions": [
        "clip_boxes",
        "find_position",
        "get_center",
        "similarity_top"
    ],
    "classes": [
        "CropClip"
    ]
}