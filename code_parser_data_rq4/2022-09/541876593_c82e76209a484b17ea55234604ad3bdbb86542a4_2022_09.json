{
    "identifiers": [
        "re",
        "json",
        "syllapy",
        "syllables",
        "pandas",
        "pd",
        "numpy",
        "np",
        "tqdm",
        "tqdm",
        "nltk",
        "corpus",
        "cmudict",
        "haiku_df",
        "cmudict",
        "syllapy",
        "WORD_DICT",
        "json",
        "load",
        "open",
        "load_script",
        "haiku_df",
        "error_bad_lines",
        "speaking_only",
        "tqdm",
        "pandas",
        "pd",
        "read_csv",
        "file_path",
        "error_bad_lines",
        "error_bad_lines",
        "dtype",
        "script_lines",
        "replace",
        "speaking_only",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "isna",
        "script_lines",
        "fillna",
        "script_lines",
        "fillna",
        "script_lines",
        "fillna",
        "script_lines",
        "replace",
        "replace",
        "replace",
        "split",
        "script_lines",
        "apply",
        "x",
        "i",
        "i",
        "len",
        "x",
        "script_lines",
        "explode",
        "script_lines",
        "sort_values",
        "script_lines",
        "spoken_words_split",
        "progress_apply",
        "count_syllables_line",
        "script_lines",
        "script_lines",
        "n_syllables",
        "script_lines",
        "word",
        "strip_list",
        "word",
        "replace",
        "word",
        "lower",
        "word",
        "simpson_dict",
        "simpson_dict",
        "word",
        "word",
        "nltk_dict",
        "keys",
        "len",
        "y",
        "y",
        "x",
        "y",
        "isdigit",
        "x",
        "nltk_dict",
        "word",
        "word",
        "word",
        "nltk_dict",
        "keys",
        "len",
        "y",
        "y",
        "x",
        "y",
        "isdigit",
        "x",
        "nltk_dict",
        "word",
        "word",
        "syllapy_dict",
        "syllapy_dict",
        "word",
        "syllables",
        "estimate",
        "word",
        "max",
        "n_syl",
        "line",
        "return_list",
        "line",
        "lower",
        "replace",
        "replace",
        "split",
        "word",
        "words",
        "strip_list",
        "word",
        "replace",
        "word",
        "num_syllables",
        "word",
        "count",
        "n_syllables",
        "count_list",
        "append",
        "count",
        "return_list",
        "count_list",
        "count",
        "save",
        "i",
        "tqdm",
        "len",
        "script",
        "j",
        "i",
        "min",
        "i",
        "len",
        "script",
        "script",
        "n_syllables",
        "iloc",
        "i",
        "j",
        "sum",
        "script",
        "iloc",
        "i",
        "j",
        "groupby",
        "agg",
        "aggregated_df",
        "shape",
        "join",
        "aggregated_df",
        "spoken_words_split",
        "values",
        "haiku_list",
        "append",
        "aggregated_df",
        "pd",
        "concat",
        "haiku_list",
        "haiku_df",
        "haiku_df",
        "spoken_words_split",
        "progress_apply",
        "is_parsable_as_haiku",
        "haiku_df",
        "save",
        "haiku_df",
        "to_csv",
        "haiku_df",
        "word_string",
        "count_syllables_line",
        "word_string",
        "return_list",
        "count_list",
        "count_list",
        "count_list",
        "return_list",
        "haiku_df",
        "haiku_df",
        "generate_haiku_df",
        "save",
        "haiku_df",
        "sample",
        "spoken_words_split",
        "values",
        "haiku",
        "replace",
        "replace",
        "split",
        "word",
        "words",
        "strip_list",
        "word",
        "replace",
        "word",
        "num_syllables",
        "word",
        "count",
        "n_syllables",
        "count",
        "syllable_list",
        "i",
        "i",
        "haiku_list",
        "i",
        "word",
        "line",
        "strip",
        "line",
        "haiku_list",
        "return_list",
        "haiku_list",
        "join",
        "haiku_list"
    ],
    "literals": [
        "'dataset/simpsons_script_lines.csv'",
        "','",
        "'.'",
        "'?'",
        "'!'",
        "':'",
        "'\\\\'",
        "'\"'",
        "'simpson_lect.json'",
        "'speaking_lines'",
        "'speaking_line'",
        "'speaking_line'",
        "'true'",
        "'false'",
        "'speaking_line'",
        "'normalized_text'",
        "'location_id'",
        "'character_id'",
        "'location_id'",
        "'character_id'",
        "'raw_character_text'",
        "'raw_character_text'",
        "'Missing Character'",
        "'raw_location_text'",
        "'raw_location_text'",
        "'Missing Location'",
        "'spoken_words_split'",
        "'spoken_words'",
        "'?'",
        "'.'",
        "'!'",
        "'.'",
        "'/'",
        "'.'",
        "'.'",
        "'number_in_line'",
        "'spoken_words_split'",
        "'spoken_words_split'",
        "'number_in_line'",
        "'episode_id'",
        "'number'",
        "'number_in_line'",
        "'n_syllables'",
        "''",
        "'s'",
        "'-'",
        "' '",
        "'/'",
        "' '",
        "' '",
        "''",
        "'episode_id'",
        "' '",
        "'haiku_df.csv'",
        "'-'",
        "' '",
        "'/'",
        "' '",
        "' '",
        "''",
        "''",
        "''",
        "''",
        "' '",
        "'\\n'",
        "'__main__'"
    ],
    "variables": [
        "file_path",
        "strip_list",
        "nltk_dict",
        "syllapy_dict",
        "simpson_dict",
        "script",
        "haiku_df",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "script_lines",
        "word",
        "word",
        "n_syl",
        "n_syl",
        "n_syl",
        "n_syl",
        "n_syl",
        "words",
        "count",
        "count_list",
        "word",
        "n_syllables",
        "n_syllables",
        "haiku_list",
        "aggregated_df",
        "aggregated_df",
        "spoken_words_split",
        "haiku_df",
        "haiku_df",
        "haiku_df",
        "count_list",
        "haiku_df",
        "haiku_df",
        "haiku",
        "words",
        "count",
        "haiku_list",
        "syllable_list",
        "i",
        "word",
        "n_syllables",
        "n_syllables",
        "haiku_list"
    ],
    "comments": [
        "Data cleaning on speaking lines",
        "Cleaning missing values",
        "Split longer lines of dialogue based on delimiters and explode to longer format",
        "split on '.!?/', but could extend to ':;,'",
        "TODO Regex version instead? Keep (multiple) elimiters? Exclude Mr. etc.",
        "script_lines['spoken_words_split'] = script_lines['spoken_words'].apply(lambda x: re.split('[?!/]', x))  # split on '.!?/', but could extend to ':;,'",
        "Order by episode then line sequence",
        "non-explode version",
        "script_lines['spoken_words_split'] = script_lines['spoken_words']",
        "script_lines = script_lines.sort_values(['episode_id', 'number'])",
        "Resort to estimation",
        ", s_dict):",
        "Brute force-ish, could be optimized",
        "Select 17-syllable sequences from same episode",
        "Check for parsability, requiring no word-breaks to confirm to 5-7-5 structure",
        "def generate_dictionary(self, script_lines, save=True):",
        "\"\"\"Generate dictionary of words from Simpsons corpus, together with syllable count.\"\"\"",
        "tqdm.pandas()",
        "# String all dialogue into one line and split into list of strings",
        "corpus = script_lines['spoken_words'].str.cat(sep=' ')",
        "for char in self.strip_list:",
        "corpus = corpus.replace(char, '')",
        "corpus_list = corpus.lower().replace('-', ' ').replace('/', ' ').split(' ')",
        "corpus_df = pd.DataFrame({'word' : corpus_list})",
        "simpsons_count = corpus_df.value_counts().reset_index(name='counts')",
        "simpsons_dict = {}",
        "for word in tqdm(simpsons_count['word']):",
        "for char in self.strip_list:",
        "word = word.replace(char, '')",
        "word = word.lower()",
        "if word:",
        "n_syllables = self.num_syllables(word)",
        "if word not in simpsons_dict.keys():",
        "simpsons_dict[word] = n_syllables",
        "self.simpsons_dict = simpsons_dict",
        "if save:",
        "pass  # Do a save of syllable dict to json",
        "# return simpsons_dict",
        "def get_haiku_lines(self, save=False):",
        "\"\"\"Find lines of dialogue that are already self-contained haikus.\"\"\"",
        "script_lines=self.script",
        "tqdm.pandas()",
        "script_lines['syllables'] = script_lines['normalized_text'].progress_apply(self.count_syllables_line)",
        "ready_haikus = script_lines[script_lines['syllables'] == 17]",
        "if save:",
        "ready_haikus.to_csv('readymade_haikus.csv')",
        "return ready_haikus",
        "def generate_haiku(script_lines):",
        "\"\"\"Identify existing haikus from script, starting at random point in corpus.",
        "\"\"\"",
        "line_1, line_2, line_3 = \"\", \"\", \"\"",
        "syllable_count = 0",
        "# String all dialogue into one line",
        "generate_dictionary(script_lines)",
        "corpus = script_lines['normalized_text'].str.cat(sep=' ')",
        "corpus_list = corpus.split(' ')",
        "# Select index of starting word at random",
        "random_int = np.random.randint(len(corpus_list))",
        "random_element = corpus_list[random_int] + ' '",
        "line_1 += random_element",
        "syllable_count += syllables.estimate(random_element)",
        "for i in range(random_int + 1, random_int + 17):",
        "element = corpus_list[i] + ' '",
        "element_syllables = syllables.estimate(element)",
        "if syllable_count + element_syllables <= 5:",
        "line_1 += element",
        "# syllable_count += syllables.estimate(element)",
        "elif syllable_count + element_syllables <= 12:",
        "line_2 += element",
        "# syllable_count += syllables.estimate(element)",
        "elif syllable_count + element_syllables <= 17:",
        "line_3 += element",
        "else:",
        "break",
        "syllable_count += element_syllables",
        "print(line_1)",
        "print(line_2)",
        "print(line_3)",
        "print(syllable_count)",
        "print(count_syllables_line('leased your camry from christian brothers auto'))",
        "script = load_script()",
        "generate_haiku(script)",
        "print(count_syllables_line(\"you're\"))",
        "print(count_syllables_line(\"moe youre always moe homer look your house is on tv you\"))"
    ],
    "docstrings": [
        "\"\"\"Load Simpsons script into pandas DataFrame.\"\"\"",
        "\"\"\"Number of syllables using NLTK. Props to user hoju (!) at `https://stackoverflow.com/a/4103234`.\"\"\"",
        "\"\"\"\"Count number of syllables in a line. Return either the final count or a list of cumulative counts from \n        constituent words.\n        \"\"\"",
        "\"\"\"Generate DataFrame of haikus from corpus.\"\"\"",
        "\"\"\"Return True if string can be parsed as a haiku.\"\"\"",
        "\"\"\"Using an either an exisiting haiku DataFrame or generating one, sample \n        a haiku, parsed in the 5-7-5 line format. Either return as a list or as a\n        string delimited with newline characters.\n        \"\"\""
    ],
    "functions": [
        "load_script",
        "num_syllables",
        "count_syllables_line",
        "generate_haiku_df",
        "is_parsable_as_haiku",
        "generate_haiku"
    ],
    "classes": [
        "SimpsonsHaiku"
    ]
}