{
    "identifiers": [
        "argparse",
        "logging",
        "os",
        "sys",
        "io",
        "numpy",
        "np",
        "torch",
        "fairseq",
        "models",
        "roberta",
        "RobertaModel",
        "mxnet",
        "mx",
        "gluonnlp",
        "nlp",
        "gluonnlp",
        "model",
        "BERTEncoder",
        "BERTModel",
        "gluonnlp",
        "model",
        "bert",
        "bert_hparams",
        "gluonnlp",
        "data",
        "utils",
        "_load_pretrained_vocab",
        "utils",
        "get_hash",
        "load_text_vocab",
        "tf_vocab_to_gluon_vocab",
        "argparse",
        "ArgumentParser",
        "description",
        "formatter_class",
        "argparse",
        "ArgumentDefaultsHelpFormatter",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "choices",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "parse_args",
        "os",
        "path",
        "expanduser",
        "args",
        "ckpt_dir",
        "torch",
        "load",
        "os",
        "path",
        "join",
        "ckpt_dir",
        "ckpt",
        "args",
        "verbose",
        "ckpt",
        "k",
        "v",
        "pytorch_params",
        "items",
        "k",
        "v",
        "shape",
        "RobertaModel",
        "from_pretrained",
        "ckpt_dir",
        "roberta",
        "eval",
        "torch_vocab",
        "len",
        "torch_vocab",
        "torch_vocab",
        "bos",
        "torch_vocab",
        "pad",
        "torch_vocab",
        "eos",
        "torch_vocab",
        "unk",
        "torch_vocab",
        "symbols",
        "bos_idx",
        "torch_vocab",
        "symbols",
        "pad_idx",
        "torch_vocab",
        "symbols",
        "eos_idx",
        "torch_vocab",
        "symbols",
        "unk_idx",
        "bos_idx",
        "pad_idx",
        "eos_idx",
        "unk_idx",
        "_load_pretrained_vocab",
        "io",
        "open",
        "os",
        "path",
        "join",
        "ckpt_dir",
        "encoding",
        "f",
        "i",
        "line",
        "f",
        "line",
        "split",
        "token",
        "i",
        "len",
        "specials",
        "ValueError",
        "i",
        "len",
        "specials",
        "token",
        "idx",
        "token",
        "openai_vocab",
        "idx_to_token",
        "idx",
        "openai_to_roberta",
        "openai_to_roberta",
        "idx",
        "token",
        "token",
        "token",
        "torch_vocab",
        "index",
        "torch_vocab",
        "mask_idx",
        "index_to_words",
        "idx",
        "token",
        "index_to_words",
        "idx",
        "nlp",
        "vocab",
        "Vocab",
        "word2idx",
        "token_to_idx",
        "word2idx",
        "unknown_token",
        "index_to_words",
        "unk_idx",
        "padding_token",
        "index_to_words",
        "pad_idx",
        "bos_token",
        "index_to_words",
        "bos_idx",
        "eos_token",
        "index_to_words",
        "eos_idx",
        "mask_token",
        "vocab",
        "fairseq_vocab_to_gluon_vocab",
        "roberta",
        "task",
        "dictionary",
        "bert_hparams",
        "args",
        "model",
        "BERTEncoder",
        "attention_cell",
        "predefined_args",
        "num_layers",
        "predefined_args",
        "units",
        "predefined_args",
        "hidden_size",
        "predefined_args",
        "max_length",
        "predefined_args",
        "num_heads",
        "predefined_args",
        "scaled",
        "predefined_args",
        "dropout",
        "predefined_args",
        "use_residual",
        "predefined_args",
        "layer_norm_eps",
        "predefined_args",
        "BERTModel",
        "encoder",
        "len",
        "vocab",
        "units",
        "predefined_args",
        "embed_size",
        "predefined_args",
        "embed_dropout",
        "predefined_args",
        "word_embed",
        "predefined_args",
        "use_pooler",
        "use_token_type_embed",
        "use_classifier",
        "bert",
        "initialize",
        "init",
        "mx",
        "init",
        "Normal",
        "mx",
        "nd",
        "ones",
        "bert",
        "ones",
        "mx",
        "nd",
        "array",
        "mx",
        "nd",
        "array",
        "bert",
        "_collect_params_with_prefix",
        "i",
        "format",
        "i",
        "format",
        "i",
        "format",
        "i",
        "format",
        "i",
        "name",
        "name",
        "source",
        "dest",
        "mapping",
        "items",
        "pytorch_name",
        "replace",
        "source",
        "dest",
        "pytorch_name",
        "pytorch_params",
        "keys",
        "pytorch_name",
        "name",
        "pytorch_params",
        "pytorch_name",
        "cpu",
        "pytorch_name",
        "torch_arr",
        "mx",
        "nd",
        "array",
        "torch_arr",
        "name",
        "arr",
        "split",
        "num_outputs",
        "axis",
        "i",
        "p",
        "unfused",
        "p",
        "name",
        "arrs",
        "i",
        "arr",
        "shape",
        "name",
        "shape",
        "arr",
        "shape",
        "name",
        "shape",
        "name",
        "pytorch_name",
        "name",
        "set_data",
        "arr",
        "len",
        "len",
        "loaded_params",
        "len",
        "visited_pytorch_params",
        "len",
        "pytorch_params",
        "len",
        "visited_pytorch_params",
        "len",
        "pytorch_params",
        "roberta",
        "encode",
        "texts",
        "roberta",
        "extract_features",
        "torch_tokens",
        "torch_features",
        "detach",
        "numpy",
        "nlp",
        "data",
        "GPT2BPETokenizer",
        "vocab",
        "bos_token",
        "mx_tokenizer",
        "texts",
        "vocab",
        "eos_token",
        "vocab",
        "mx_tokens",
        "mx_tokens",
        "vocab",
        "mx_tokens",
        "torch_tokens",
        "mx_data",
        "torch_tokens",
        "tolist",
        "bert",
        "mx",
        "nd",
        "array",
        "mx_data",
        "np",
        "std",
        "mx_out",
        "asnumpy",
        "pytorch_out",
        "mx",
        "test_utils",
        "assert_almost_equal",
        "mx_out",
        "asnumpy",
        "pytorch_out",
        "atol",
        "rtol",
        "mx",
        "test_utils",
        "assert_almost_equal",
        "mx_out",
        "asnumpy",
        "pytorch_out",
        "atol",
        "rtol",
        "bert",
        "save_parameters",
        "os",
        "path",
        "join",
        "ckpt_dir",
        "args",
        "model",
        "io",
        "open",
        "os",
        "path",
        "join",
        "ckpt_dir",
        "args",
        "model",
        "encoding",
        "f",
        "f",
        "write",
        "vocab",
        "to_json"
    ],
    "literals": [
        "'Conversion script for Fairseq RoBERTa model'",
        "'--ckpt_dir'",
        "'Full path to the roberta folder'",
        "'/home/ubuntu/roberta/roberta.base'",
        "'--model'",
        "'Model type. '",
        "'roberta_12_768_12'",
        "'roberta_24_1024_16'",
        "'roberta_12_768_12'",
        "'--verbose'",
        "'store_true'",
        "'Verbose logging'",
        "'model.pt'",
        "'model'",
        "'args'",
        "'openai_webtext'",
        "'.'",
        "'dict.txt'",
        "'utf-8'",
        "' '",
        "u'<mask>'",
        "u'<mask>'",
        "u'<mask>'",
        "'attention_cell'",
        "'num_layers'",
        "'units'",
        "'hidden_size'",
        "'max_length'",
        "'num_heads'",
        "'scaled'",
        "'dropout'",
        "'use_residual'",
        "'layer_norm_eps'",
        "'units'",
        "'embed_size'",
        "'embed_dropout'",
        "'word_embed'",
        "'decoder.2'",
        "'decoder.lm_head.layer_norm'",
        "'decoder.0'",
        "'decoder.lm_head.dense'",
        "'decoder.3'",
        "'decoder.lm_head'",
        "'encoder.layer_norm'",
        "'decoder.sentence_encoder.emb_layer_norm'",
        "'encoder.position_weight'",
        "'decoder.sentence_encoder.embed_positions.weight'",
        "'encoder.transformer_cells'",
        "'decoder.sentence_encoder.layers'",
        "'attention_cell.proj_key.'",
        "'self_attn.in_proj_'",
        "'attention_cell.proj_value.'",
        "'self_attn.in_proj_'",
        "'attention_cell.proj_query.'",
        "'self_attn.in_proj_'",
        "'ffn.ffn_1'",
        "'fc1'",
        "'ffn.ffn_2'",
        "'fc2'",
        "'layer_norm.gamma'",
        "'layer_norm.weight'",
        "'layer_norm.beta'",
        "'layer_norm.bias'",
        "'ffn.layer_norm'",
        "'final_layer_norm'",
        "'word_embed.0.weight'",
        "'decoder.sentence_encoder.embed_tokens.weight'",
        "'{}.layer_norm'",
        "'{}.self_attn_layer_norm'",
        "'{}.proj'",
        "'{}.self_attn.out_proj'",
        "'Key '",
        "' for '",
        "' not found.'",
        "'decoder.sentence_encoder.embed_positions.weight'",
        "'attention_cell.proj'",
        "'query'",
        "'key'",
        "'value'",
        "\"Gluon model does not match PyTorch model. \"",
        "\"Please fix the BERTModel hyperparameters\\n\"",
        "' v.s. '",
        "'Hello world. abc, def and 中文!'",
        "'stdev = '",
        "'.params'",
        "'.vocab'",
        "'w'",
        "'utf-8'"
    ],
    "variables": [
        "parser",
        "args",
        "ckpt_dir",
        "ckpt",
        "pytorch_params",
        "roberta",
        "index_to_words",
        "bos_idx",
        "pad_idx",
        "eos_idx",
        "unk_idx",
        "index_to_words",
        "bos_idx",
        "index_to_words",
        "pad_idx",
        "index_to_words",
        "eos_idx",
        "index_to_words",
        "unk_idx",
        "specials",
        "openai_to_roberta",
        "openai_vocab",
        "token",
        "count",
        "fake_token",
        "openai_to_roberta",
        "token",
        "index_to_words",
        "index_to_words",
        "mask_idx",
        "index_to_words",
        "mask_idx",
        "word2idx",
        "word2idx",
        "token",
        "vocab",
        "vocab",
        "predefined_args",
        "encoder",
        "bert",
        "ones",
        "mapping",
        "mapping",
        "mapping",
        "loaded_params",
        "visited_pytorch_params",
        "pytorch_name",
        "pytorch_name",
        "torch_arr",
        "torch_arr",
        "arr",
        "unfused",
        "arrs",
        "arr",
        "loaded_params",
        "name",
        "visited_pytorch_params",
        "pytorch_name",
        "texts",
        "torch_tokens",
        "torch_features",
        "pytorch_out",
        "mx_tokenizer",
        "mx_tokens",
        "mx_data",
        "mx_out"
    ],
    "comments": [
        "coding: utf-8",
        "Licensed to the Apache Software Foundation (ASF) under one",
        "or more contributor license agreements.  See the NOTICE file",
        "distributed with this work for additional information",
        "regarding copyright ownership.  The ASF licenses this file",
        "to you under the Apache License, Version 2.0 (the",
        "'License'); you may not use this file except in compliance",
        "with the License.  You may obtain a copy of the License at",
        "",
        "http://www.apache.org/licenses/LICENSE-2.0",
        "",
        "Unless required by applicable law or agreed to in writing,",
        "software distributed under the License is distributed on an",
        "'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
        "KIND, either express or implied.  See the License for the",
        "specific language governing permissions and limitations",
        "under the License.",
        "pylint:disable=redefined-outer-name,logging-format-interpolation",
        "Load the model in fairseq",
        "BERT encoder",
        "BERT model",
        "set parameter data",
        "fairseq positional embedding starts with index 2"
    ],
    "docstrings": [
        "\"\"\" Script for converting Fairseq Roberta Model to Gluon. \"\"\""
    ],
    "functions": [
        "fairseq_vocab_to_gluon_vocab"
    ],
    "classes": []
}