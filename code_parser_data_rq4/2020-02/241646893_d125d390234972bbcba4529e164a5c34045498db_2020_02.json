{
    "identifiers": [
        "re",
        "pandas",
        "pd",
        "numpy",
        "np",
        "nltk",
        "tokenize",
        "RegexpTokenizer",
        "gensim",
        "models",
        "Word2Vec",
        "multiprocessing",
        "time",
        "time",
        "whoosh",
        "analysis",
        "sklearn",
        "cluster",
        "KMeans",
        "spacy",
        "collections",
        "Counter",
        "spacy",
        "load",
        "disable",
        "pd",
        "read_csv",
        "delimiter",
        "names",
        "df",
        "sample",
        "frac",
        "reset_index",
        "drop",
        "df",
        "drop",
        "axis",
        "inplace",
        "preprocessing",
        "np",
        "random",
        "randn",
        "len",
        "df_cleaned",
        "df_cleaned",
        "msk",
        "df_cleaned",
        "msk",
        "train_data",
        "drop",
        "axis",
        "inplace",
        "train_data",
        "tolist",
        "pd",
        "DataFrame",
        "test",
        "pd",
        "DataFrame",
        "test",
        "Word2Vec",
        "min_count",
        "size",
        "sample",
        "alpha",
        "min_alpha",
        "negative",
        "iter",
        "workers",
        "multiprocessing",
        "cpu_count",
        "df",
        "isnull",
        "sum",
        "df",
        "isnull",
        "sum",
        "df",
        "dropna",
        "reset_index",
        "drop",
        "time",
        "pd",
        "DataFrame",
        "columns",
        "index",
        "row",
        "df",
        "iterrows",
        "nlp",
        "row",
        "token",
        "lemma_",
        "token",
        "doc",
        "token",
        "lemma_",
        "s",
        "lower",
        "s",
        "result",
        "s",
        "isalpha",
        "len",
        "s",
        "result",
        "df_local",
        "loc",
        "result",
        "row",
        "index",
        "format",
        "round",
        "time",
        "t",
        "df_local",
        "doc",
        "token",
        "lemma_",
        "token",
        "doc",
        "token",
        "is_stop",
        "len",
        "txt",
        "join",
        "txt",
        "time",
        "w2v_model",
        "build_vocab",
        "train_data",
        "progress_per",
        "format",
        "round",
        "time",
        "t",
        "time",
        "w2v_model",
        "train",
        "train_data",
        "total_examples",
        "w2v_model",
        "corpus_count",
        "epochs",
        "report_delay",
        "format",
        "round",
        "time",
        "t",
        "w2v_model",
        "save",
        "w2v_model",
        "wv",
        "KMeans",
        "n_clusters",
        "max_iter",
        "random_state",
        "n_init",
        "fit",
        "X",
        "word_vec",
        "vectors",
        "word_vec",
        "similar_by_vector",
        "model",
        "cluster_centers_",
        "topn",
        "restrict_vocab",
        "model",
        "cluster_centers_",
        "argsort",
        "pd",
        "DataFrame",
        "word_vec",
        "vocab",
        "keys",
        "words",
        "words",
        "words",
        "words",
        "apply",
        "x",
        "word_vec",
        "wv",
        "x",
        "words",
        "words",
        "vectors",
        "apply",
        "x",
        "model",
        "predict",
        "np",
        "array",
        "x",
        "Word2Vec",
        "min_count",
        "size",
        "sample",
        "alpha",
        "min_alpha",
        "negative",
        "iter",
        "workers",
        "multiprocessing",
        "cpu_count",
        "test_data",
        "tolist",
        "w2v_model_new",
        "build_vocab",
        "test_ls",
        "progress_per",
        "w2v_model_new",
        "train",
        "test_ls",
        "total_examples",
        "w2v_model",
        "corpus_count",
        "epochs",
        "report_delay",
        "w2v_model_new",
        "wv",
        "pd",
        "DataFrame",
        "wordVec",
        "vocab",
        "keys",
        "words",
        "words",
        "apply",
        "x",
        "wordVec",
        "wv",
        "x",
        "words",
        "vectors",
        "apply",
        "x",
        "model",
        "predict",
        "np",
        "array",
        "x",
        "index",
        "row",
        "test_data",
        "iterrows",
        "pd",
        "Index",
        "words",
        "word",
        "row",
        "word",
        "words",
        "tolist",
        "uniquq_indexes",
        "get_loc",
        "word",
        "cluster_res",
        "append",
        "words",
        "word_indx",
        "cluster_res",
        "Counter",
        "cluster_res",
        "occ_counter",
        "most_common",
        "most_com",
        "pred",
        "test_label",
        "index",
        "pred",
        "row",
        "correct",
        "non_correct",
        "format",
        "correct",
        "non_correct",
        "UnsupervisedLearning",
        "Unsupervised_obj",
        "train",
        "Unsupervised_obj",
        "cluster",
        "Unsupervised_obj",
        "retrain_model"
    ],
    "literals": [
        "'en'",
        "'parser'",
        "'ner'",
        "\"training.1600000.processed.noemoticon.csv\"",
        "','",
        "'target'",
        "'id'",
        "'date'",
        "'flag'",
        "'user'",
        "'text'",
        "\"id\"",
        "\"date\"",
        "\"flag\"",
        "\"user\"",
        "\"target\"",
        "\"text\"",
        "'text'",
        "'target'",
        "\"text\"",
        "\"target\"",
        "'text'",
        "'target'",
        "'text'",
        "'-PRON-'",
        "\"target\"",
        "'Time for pre-processing: {} mins'",
        "' '",
        "'Time to build vocab: {} mins'",
        "\"Time to train : {} mins\"",
        "\"w2v_model_old\"",
        "\"Clustering hahahaha\"",
        "'words'",
        "'vectors'",
        "f'{x}'",
        "'cluster'",
        "'text'",
        "\"Okaaay, now we finished updating the model\"",
        "'words'",
        "'vectors'",
        "f'{x}'",
        "'cluster'",
        "'words'",
        "'text'",
        "'words'",
        "'cluster'",
        "'0'",
        "'0'",
        "'4'",
        "'target'",
        "\"Was Classified correctly as : \"",
        "\"That was the text : \"",
        "'text'",
        "\"Classification was wrong\"",
        "\"We got {} of correct guesses, and {} of incorrect guesses\"",
        "\"__main__\"",
        "\"This is it!\""
    ],
    "variables": [
        "nlp",
        "df",
        "df",
        "df_cleaned",
        "msk",
        "train_data",
        "test",
        "train_data",
        "test_data",
        "test_label",
        "w2v_model",
        "t",
        "df_local",
        "doc",
        "result",
        "result",
        "result",
        "index",
        "txt",
        "t",
        "t",
        "word_vec",
        "model",
        "order_centroids",
        "words",
        "columns",
        "w2v_model_new",
        "test_ls",
        "wordVec",
        "words",
        "words",
        "columns",
        "words",
        "words",
        "cluster_res",
        "correct",
        "non_correct",
        "uniquq_indexes",
        "word_indx",
        "occ_counter",
        "most_com",
        "cluster_res",
        "pred",
        "pred",
        "Unsupervised_obj"
    ],
    "comments": [
        "Initialize a spacy obj.",
        "Shuffle all rows in the dataframe.",
        "Specifying drop=True prevents reset_index() from creating a column containing the old index entries.",
        "Dropping all column except \"text\", and \"target\".",
        "Solution 2 : Uses custom cleaning and pre-processing.",
        "Pre-processing.",
        "Setting a mask to split the data to train and test.",
        "Splitting the data.",
        "Dropping the target column for the training data.",
        "Because it's an unsupervised learning, so we don't use the target.",
        "Convert the trainging data to a list.",
        "Split the texts and labels to different data frames.",
        "Initializing  a word to vector model.",
        "Removing all null records.",
        "Spacy version to lemmatize.",
        "removing redundant values from \"result\".",
        "Add \"result\" to the local dataframe.",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in self.df_masked['text'])",
        "t = time()",
        "",
        "txt = [self.cleaning(doc) for doc in self.nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]",
        "",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))",
        "return txt",
        "Lemmatizes and removes stopwords",
        "doc needs to be a spacy Doc object",
        "Word2Vec uses context words to learn the vector representation of a target word,",
        "if a sentence is only one or two words long,",
        "the benefit for the training is very small",
        "Build the vocab.",
        "Train the model.",
        "cluster_res = []",
        "for index, row in self.test_data.iterrows():",
        "local_text = ' '.join(row['text'])",
        "doc = self.nlp(local_text)",
        "result = [token.lemma_ for token in doc if token.lemma_ != '-PORUN-']",
        "result = [s.lower() for s in result if s.isalpha() and len(s) >= 2]",
        "for word in result:",
        "if word in self.words['words'].tolist():",
        "cluster_res.append(str(self.words['cluster'][index][0]))",
        "# Returns count of each element in the list",
        "if cluster_res:",
        "occ_counter = Counter(cluster_res)",
        "most_com = occ_counter.most_common(1)[0][0]",
        "if most_com == 0:",
        "pred = 4",
        "else:",
        "pred = 1",
        "if pred == self.test_label['target'][index]:",
        "print(\"Was Classified correctly as : \", self.test_label['target'][index])",
        "print(\"That was the text : \", row['text'])",
        "else:",
        "print(\"Classification was wrong\")",
        "load the saved model.",
        "w2v_model_new = Word2Vec.load(\"w2v_model_old\")",
        "local_text = ' '.join(row['text'])",
        "doc = self.nlp(local_text)",
        "result = [token.lemma_ for token in doc if token.lemma_ != '-PORUN-']",
        "result = [s.lower() for s in result if s.isalpha() and len(s) >= 2]",
        "Saving the indexes of the dataframe \"words\" in a list.",
        "retrieve the \"word\" index form \"unique_indexes\" list.",
        "Returns count of each element in the list",
        "Unsupervised_obj.w2v_model.vocabulary()",
        "Unsupervised_obj.w2v_model.wv.most_similar(positive=[\"mad\"])"
    ],
    "docstrings": [
        "\"\"\"\n        Solution 1 : Using Spacy for pre-processing and cleaning.\n        \n        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])  # disabling Named Entity Recognition for speed\n        \n        Creating a mask to split the data into train and test.\n        self.msk = np.random.randn(len(self.df)) < 2\n        self.df_masked = self.df[~self.msk]\n        # Clean the data.\n        txt = self.preprocessing()\n\n        # Create a dataframe to save the cleaned tweets.\n        self.df_clean = pd.DataFrame({'clean': txt})\n\n        # Adding the labels to df_clean dataframe.\n        # First Match the indexes.\n        self.df_masked.index = self.df_clean.index\n        self.df_clean['target'] = self.df_masked['target']\n\n        # Create another mask.\n        self.msk_1 = np.random.rand(len(self.df_clean)) < 0.8\n\n        # Splitting to train and test datasets.\n        self.train_all = self.df_clean[self.msk_1]\n        self.test = self.df_clean[~self.msk_1]\n\n        # Since this is an unsupervised learning, we will drop the target column for the train data.\n        self.train_data = pd.DataFrame({'clean': self.train_all[\"clean\"]})\n        self.train_data[\"clean\"] = self.train_data[\"clean\"].str.split(\" \")\n\n        # Convert train_data to a list of strings.\n        self.train_data = self.train_data.values.tolist()\n        self.train_data = [x for x in self.train_data if x]\n\n\n        # For the test data, we assign the text and target column to test_data, and test_label respectively.\n        self.test_data, self.test_label = self.test[\"clean\"], self.test[\"target\"]\n        \"\"\"",
        "\"\"\"\n        # Remove Punctuation, and numbers from texts.\n        tokenizer = RegexpTokenizer(r'\\w+')\n        df_local = pd.DataFrame(columns=['text', 'target'])\n        for index, row in self.df.iterrows():\n            row['text'] = re.sub(r'\\d+', '', row['text'])\n            result = tokenizer.tokenize(row[\"text\"])\n            result = [token.lower() for token in result]\n            for word in result:\n                if len(word) < 2:\n                    result.remove(word)\n            df_local.loc[index] = [result, str(row[\"target\"])]\n        \"\"\"",
        "\"\"\"\n        -Gets the vocab from the word vector model.\n        -Add each of the vocab's vector representation as a column in the data frame.\n        -Add the cluster each of the vocab belongs to as a column in the data frame.\n\n        * By the end of there three stages we get a data frame with the following columns:\n        1- vocab, 2- vector representation, 3- cluster number.\n\n        -Then it iterates over the test data frame.\n        -iterate over each word of each sentence.\n        -checks if that word belongs to one of the clusters.\n        -Till it build a vector with cluster numbers, that each corresponding word in a sentence belongs to.\n        -e.g. sentence_1 = [\"hate\", \"to\", \"leave\", \"you\", \"happy\"]\n        -Cluster 0 \"Negative\" has \"hate\", \"leave\"\n        -Cluster 1 \"Positive\" has \"happy\"\n        - \"to\", and \"you\" doesn't belong to either of the clusters\n        -The resulting vector would be [0, 0, 1]\n        -The prediction would be that sentence_1 belongs to cluster 0.\n        -Means that sentence_1 is most probably a negative sentence.\n        :return:\n        \"\"\""
    ],
    "functions": [
        "preprocessing",
        "cleaning",
        "train",
        "cluster",
        "test_model",
        "retrain_model"
    ],
    "classes": [
        "UnsupervisedLearning"
    ]
}