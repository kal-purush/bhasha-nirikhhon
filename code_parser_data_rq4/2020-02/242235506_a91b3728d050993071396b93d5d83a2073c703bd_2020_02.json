{
    "identifiers": [
        "typing",
        "Union",
        "dataclasses",
        "dataclass",
        "numpy",
        "np",
        "os",
        "shutil",
        "pickle",
        "urllib",
        "request",
        "urlopen",
        "logging",
        "tensorflow",
        "tf",
        "tokenization",
        "modeling",
        "logging",
        "getLogger",
        "dataclass",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "dataclass",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "Union",
        "tf",
        "Tensor",
        "np",
        "array",
        "ERNIE4US_VERSION",
        "url",
        "destination_path",
        "os",
        "path",
        "exists",
        "destination_path",
        "_logger",
        "info",
        "os",
        "path",
        "basename",
        "url",
        "url",
        "urlopen",
        "url",
        "content",
        "destination_path",
        "os",
        "makedirs",
        "os",
        "path",
        "dirname",
        "destination_path",
        "exist_ok",
        "open",
        "temp_path",
        "content",
        "read",
        "total",
        "len",
        "stuff",
        "len",
        "stuff",
        "write",
        "stuff",
        "shutil",
        "move",
        "temp_path",
        "destination_path",
        "e",
        "os",
        "path",
        "exists",
        "temp_path",
        "os",
        "remove",
        "temp_path",
        "e",
        "_logger",
        "info",
        "total",
        "destination_path",
        "model_name",
        "path",
        "model_binary_repository_url",
        "os",
        "path",
        "join",
        "path",
        "model_name",
        "model_binary_repository_url",
        "DEFAULT_MODEL_BINARY_REPO_URL_",
        "os",
        "path",
        "join",
        "path",
        "model_name",
        "os",
        "path",
        "join",
        "path",
        "model_name",
        "os",
        "path",
        "join",
        "path",
        "model_name",
        "file_path",
        "config_path",
        "param_path",
        "vocab_path",
        "_download_file",
        "model_binary_repository_url",
        "os",
        "path",
        "basename",
        "file_path",
        "file_path",
        "path",
        "config_path",
        "vocab_path",
        "param_path",
        "is_training_tensor_name",
        "tf",
        "name_scope",
        "tf",
        "get_default_graph",
        "get_tensor_by_name",
        "is_training_tensor_name",
        "tf",
        "get_default_graph",
        "get_tensor_by_name",
        "is_training_tensor_name",
        "tf_is_training",
        "tf_is_training_float",
        "KeyError",
        "tf",
        "placeholder_with_default",
        "np",
        "array",
        "name",
        "is_training_tensor_name",
        "tf",
        "cast",
        "tf_is_training",
        "tf",
        "float32",
        "name",
        "is_training_tensor_name",
        "tf_is_training",
        "tf_is_training_float",
        "dropout_rate",
        "is_training_tensor_name",
        "get_is_training_tensors",
        "is_training_tensor_name",
        "tf",
        "multiply",
        "tf_is_training_float",
        "dropout_rate",
        "vocaburary_path",
        "do_lower_case",
        "max_seq_len",
        "max_seq_len",
        "tokenization",
        "FullTokenizer",
        "vocaburary_path",
        "do_lower_case",
        "do_lower_case",
        "tokenizer",
        "vocab",
        "vocab",
        "property",
        "_max_seq_len",
        "input_seq",
        "pad_value",
        "dtype",
        "np",
        "int64",
        "input_seq",
        "pad_value",
        "max_seq_len",
        "len",
        "input_seq",
        "np",
        "array",
        "padded",
        "dtype",
        "dtype",
        "sentences",
        "max_seq_len",
        "len",
        "sentences",
        "sentence",
        "sentences",
        "len",
        "sentence",
        "total_length",
        "l",
        "l",
        "max_tokens",
        "l",
        "sentence",
        "total_length",
        "max_length",
        "longest",
        "longest",
        "pop",
        "text_a",
        "text_b",
        "task_id",
        "text_a",
        "sentence",
        "text_a",
        "text_b",
        "sentence",
        "tokenization",
        "convert_to_unicode",
        "sentence",
        "tokenizer",
        "tokenize",
        "sentence",
        "sentences",
        "append",
        "sentence",
        "truncate_sentences",
        "sentences",
        "i",
        "sentence_tokens",
        "sentences",
        "tokens",
        "extend",
        "sentence_tokens",
        "sentence_ids",
        "extend",
        "i",
        "len",
        "sentence_tokens",
        "tokens",
        "append",
        "sentence_ids",
        "append",
        "i",
        "pad_input_seq",
        "tokenizer",
        "convert_tokens_to_ids",
        "tokens",
        "pad_id",
        "dtype",
        "np",
        "int64",
        "pad_input_seq",
        "len",
        "tokens",
        "pad_id",
        "dtype",
        "np",
        "int64",
        "pad_input_seq",
        "sentence_ids",
        "pad_id",
        "dtype",
        "np",
        "int64",
        "pad_input_seq",
        "len",
        "tokens",
        "np",
        "float32",
        "np",
        "full_like",
        "token_ids",
        "task_id",
        "np",
        "reshape",
        "token_ids",
        "np",
        "reshape",
        "sentence_ids",
        "np",
        "reshape",
        "position_ids",
        "np",
        "reshape",
        "input_mask",
        "np",
        "reshape",
        "task_ids",
        "Ernie2Input",
        "token_ids",
        "sentence_ids",
        "position_ids",
        "task_ids",
        "input_mask",
        "model_name",
        "ernie_config_path",
        "ernie_vocab_path",
        "ernie_param_path",
        "max_seq_len",
        "do_lower_case",
        "modeling",
        "ErnieConfig",
        "from_json_file",
        "ernie_config_path",
        "model_name",
        "ERNIE_LARGE_EN",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "max_seq_len",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "max_seq_len",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "max_seq_len",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "max_seq_len",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "max_seq_len",
        "name",
        "open",
        "ernie_param_path",
        "f",
        "pickle",
        "load",
        "f",
        "modeling",
        "ErnieModel",
        "config",
        "ernie_config",
        "input_ids",
        "src_ids",
        "input_mask",
        "input_mask",
        "token_type_ids",
        "segment_ids",
        "task_type_ids",
        "task_ids",
        "ernie_params",
        "ernie_params",
        "use_one_hot_embeddings",
        "len",
        "ernie_params",
        "Ernie2Input",
        "src_ids",
        "segment_ids",
        "pos_ids",
        "task_ids",
        "input_mask",
        "Ernie2Output",
        "model",
        "get_sequence_output",
        "model",
        "get_pooled_output",
        "Ernie2InputBuilder",
        "ernie_vocab_path",
        "do_lower_case",
        "do_lower_case",
        "max_seq_len",
        "max_seq_len",
        "input_builder",
        "ernie_tf_inputs",
        "ernie_tf_outputs",
        "model_name",
        "model_path",
        "max_seq_len",
        "do_lower_case",
        "model_binary_repository_url",
        "download_model_files",
        "model_name",
        "model_path",
        "model_binary_repository_url",
        "model_binary_repository_url",
        "create_ernie_model",
        "model_name",
        "config_path",
        "vocab_path",
        "param_path",
        "max_seq_len",
        "max_seq_len",
        "do_lower_case",
        "do_lower_case"
    ],
    "literals": [
        "'0.90'",
        "'ERNIE_Base_en_stable-2.0.0'",
        "'ERNIE_Large_en_stable-2.0.0'",
        "'https://github.com/winston-zillow/dummy_sandbox/releases/download/%s/'",
        "'Downloading model artifact %s from %s'",
        "'.tmp'",
        "'wb'",
        "'Downloaded model artifact (%d bytes) to %s'",
        "\"%s_persistables.pkl\"",
        "\"%s_vocab.txt\"",
        "\"%s_config.json\"",
        "'is_training'",
        "\"\"",
        "'%s:0'",
        "'%s_float:0'",
        "'%s_float'",
        "'is_training'",
        "\"[PAD]\"",
        "'[CLS]'",
        "'[SEP]'",
        "'src_ids'",
        "'sent_ids'",
        "'input_mask'",
        "'task_ids'",
        "'pos_ids'",
        "'rb'"
    ],
    "variables": [
        "_logger",
        "ERNIE4US_VERSION",
        "ERNIE_BASE_EN",
        "ERNIE_LARGE_EN",
        "token_ids",
        "sentence_ids",
        "position_ids",
        "task_ids",
        "input_mask",
        "sequence_features",
        "classification_features",
        "DEFAULT_MODEL_BINARY_REPO_URL_",
        "temp_path",
        "total",
        "stuff",
        "path",
        "model_binary_repository_url",
        "param_path",
        "vocab_path",
        "config_path",
        "tf_is_training",
        "tf_is_training_float",
        "tf_is_training",
        "tf_is_training_float",
        "_",
        "tf_is_training_float",
        "_max_seq_len",
        "tokenizer",
        "vocab",
        "pad_id",
        "padded",
        "max_length",
        "total_length",
        "max_tokens",
        "longest",
        "l",
        "max_tokens",
        "longest",
        "sentences",
        "sentence",
        "sentence",
        "tokens",
        "sentence_ids",
        "token_ids",
        "position_ids",
        "sentence_ids",
        "input_mask",
        "task_ids",
        "token_ids",
        "sentence_ids",
        "position_ids",
        "input_mask",
        "task_ids",
        "ernie_config",
        "ernie_config",
        "intermediate_size",
        "src_ids",
        "segment_ids",
        "input_mask",
        "task_ids",
        "pos_ids",
        "ernie_params",
        "model",
        "ernie_tf_inputs",
        "ernie_tf_outputs",
        "input_builder",
        "ernie_tf_checkpoint_path",
        "config_path",
        "vocab_path",
        "param_path"
    ],
    "comments": [
        "Copyright (c) 2020 Zillow Groups. All Rights Reserved.",
        "",
        "Licensed under the Apache License, Version 2.0 (the \"License\");",
        "you may not use this file except in compliance with the License.",
        "You may obtain a copy of the License at",
        "",
        "http://www.apache.org/licenses/LICENSE-2.0",
        "",
        "Unless required by applicable law or agreed to in writing, software",
        "distributed under the License is distributed on an \"AS IS\" BASIS,",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
        "See the License for the specific language governing permissions and",
        "limitations under the License.",
        "model variants",
        "credit to ERNIE codes",
        "This is a simple heuristic which will always truncate the longer sequence",
        "one token at a time. This makes more sense than truncating an equal percent",
        "of tokens from each, since if one sequence is very short then each token",
        "that's truncated likely contains more information than a longer sequence.",
        "these two are not used by the original BERT code, but have it to be compatible with ERNIE inputs"
    ],
    "docstrings": [
        "\"\"\"Truncates token sequences of sentences in place to the maximum length.\n        Param:\n            sentences: array of token arrays\n        \"\"\""
    ],
    "functions": [
        "_download_file",
        "download_model_files",
        "get_is_training_tensors",
        "get_dropout_rate_tensor",
        "max_seq_len",
        "pad_input_seq",
        "truncate_sentences",
        "build",
        "create_ernie_model",
        "load_ernie_model"
    ],
    "classes": [
        "Ernie2Input",
        "Ernie2Output",
        "Ernie2InputBuilder"
    ]
}