{
    "identifiers": [
        "torch",
        "nn",
        "nn",
        "pathes",
        "load_data",
        "load_ontonotes4ner",
        "equip_chinese_ner_with_skip",
        "load_yangjie_rich_pretrain_word_list",
        "load_resume_ner",
        "fastNLP",
        "embeddings",
        "StaticEmbedding",
        "models",
        "LatticeLSTM_SeqLabel",
        "LSTM_SeqLabel",
        "LatticeLSTM_SeqLabel_V1",
        "fastNLP",
        "CrossEntropyLoss",
        "SpanFPreRecMetric",
        "Trainer",
        "AccuracyMetric",
        "LossInForward",
        "torch",
        "optim",
        "optim",
        "argparse",
        "torch",
        "sys",
        "utils_",
        "LatticeLexiconPadder",
        "SpanFPreRecMetric_YJ",
        "fastNLP",
        "Tester",
        "fitlog",
        "fastNLP",
        "core",
        "callback",
        "FitlogCallback",
        "utils",
        "set_seed",
        "os",
        "fastNLP",
        "LRScheduler",
        "torch",
        "optim",
        "lr_scheduler",
        "LambdaLR",
        "argparse",
        "ArgumentParser",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "parser",
        "parse_args",
        "set_seed",
        "args",
        "seed",
        "args",
        "model",
        "args",
        "bi",
        "args",
        "batch",
        "args",
        "model",
        "fit_msg_list",
        "append",
        "args",
        "skip_before_head",
        "join",
        "fit_msg_list",
        "torch",
        "device",
        "args",
        "device",
        "k",
        "v",
        "args",
        "__dict__",
        "items",
        "k",
        "v",
        "args",
        "dataset",
        "load_ontonotes4ner",
        "ontonote4ner_cn_path",
        "yangjie_rich_pretrain_unigram_path",
        "yangjie_rich_pretrain_bigram_path",
        "_refresh",
        "refresh_data",
        "index_token",
        "args",
        "dataset",
        "load_resume_ner",
        "resume_ner_path",
        "yangjie_rich_pretrain_unigram_path",
        "yangjie_rich_pretrain_bigram_path",
        "_refresh",
        "refresh_data",
        "index_token",
        "load_yangjie_rich_pretrain_word_list",
        "yangjie_rich_pretrain_word_path",
        "_refresh",
        "refresh_data",
        "os",
        "path",
        "join",
        "args",
        "dataset",
        "equip_chinese_ner_with_skip",
        "datasets",
        "vocabs",
        "embeddings",
        "w_list",
        "yangjie_rich_pretrain_word_path",
        "_refresh",
        "refresh_data",
        "_cache_fp",
        "cache_name",
        "format",
        "embeddings",
        "format",
        "embeddings",
        "word_dropout",
        "datasets",
        "k",
        "v",
        "vocabs",
        "items",
        "format",
        "k",
        "len",
        "v",
        "k",
        "v",
        "datasets",
        "items",
        "args",
        "model",
        "v",
        "set_ignore_type",
        "args",
        "skip_before_head",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "pad_val_dynamic",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "pad_val_dynamic",
        "dynamic_offset",
        "args",
        "bi",
        "v",
        "set_input",
        "use_1st_ins_infer_dim_type",
        "v",
        "set_input",
        "use_1st_ins_infer_dim_type",
        "v",
        "set_target",
        "v",
        "set_pad_val",
        "args",
        "model",
        "v",
        "set_ignore_type",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_padder",
        "LatticeLexiconPadder",
        "v",
        "set_input",
        "use_1st_ins_infer_dim_type",
        "v",
        "set_target",
        "v",
        "set_pad_val",
        "datasets",
        "args",
        "model",
        "LatticeLSTM_SeqLabel_V1",
        "embeddings",
        "embeddings",
        "embeddings",
        "hidden_size",
        "args",
        "hidden",
        "label_size",
        "len",
        "vocabs",
        "device",
        "args",
        "device",
        "embed_dropout",
        "args",
        "embed_dropout",
        "output_dropout",
        "args",
        "output_dropout",
        "skip_batch_first",
        "bidirectional",
        "args",
        "bi",
        "debug",
        "args",
        "debug",
        "skip_before_head",
        "args",
        "skip_before_head",
        "use_bigram",
        "args",
        "use_bigram",
        "vocabs",
        "vocabs",
        "args",
        "model",
        "LSTM_SeqLabel",
        "embeddings",
        "embeddings",
        "embeddings",
        "hidden_size",
        "args",
        "hidden",
        "label_size",
        "len",
        "vocabs",
        "device",
        "args",
        "device",
        "bidirectional",
        "args",
        "bi",
        "embed_dropout",
        "args",
        "embed_dropout",
        "output_dropout",
        "args",
        "output_dropout",
        "use_bigram",
        "args",
        "use_bigram",
        "k",
        "v",
        "model",
        "state_dict",
        "items",
        "format",
        "k",
        "v",
        "size",
        "torch",
        "load",
        "open",
        "model",
        "model",
        "named_parameters",
        "len",
        "param_list",
        "i",
        "len",
        "param_list",
        "param_list",
        "i",
        "param_list",
        "i",
        "param_dict",
        "target",
        "source_dict",
        "state_dict_param",
        "target",
        "torch",
        "no_grad",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "t",
        "set_",
        "source_dict",
        "k",
        "v",
        "t",
        "items",
        "format",
        "k",
        "v",
        "copy_yangjie_lattice_weight",
        "model",
        "weight_dict",
        "LossInForward",
        "SpanFPreRecMetric",
        "vocabs",
        "pred",
        "target",
        "seq_len",
        "encoding_type",
        "SpanFPreRecMetric_YJ",
        "vocabs",
        "pred",
        "target",
        "seq_len",
        "encoding_type",
        "AccuracyMetric",
        "pred",
        "target",
        "seq_len",
        "f1_metric",
        "f1_metric_yj",
        "acc_metric",
        "args",
        "optim",
        "optim",
        "Adam",
        "model",
        "parameters",
        "lr",
        "args",
        "lr",
        "args",
        "optim",
        "optim",
        "SGD",
        "model",
        "parameters",
        "lr",
        "args",
        "lr",
        "momentum",
        "args",
        "momentum",
        "LRScheduler",
        "lr_scheduler",
        "LambdaLR",
        "optimizer",
        "ep",
        "ep",
        "datasets",
        "vocabs",
        "to_index",
        "fastNLP",
        "SequentialSampler",
        "Trainer",
        "datasets",
        "model",
        "optimizer",
        "optimizer",
        "loss",
        "loss",
        "metrics",
        "metrics",
        "dev_data",
        "datasets",
        "device",
        "device",
        "batch_size",
        "args",
        "batch",
        "n_epochs",
        "args",
        "epoch",
        "dev_batch_size",
        "args",
        "test_batch",
        "callbacks",
        "callbacks",
        "check_code_level",
        "sampler",
        "SequentialSampler",
        "trainer",
        "train"
    ],
    "literals": [
        "'--device'",
        "'cpu'",
        "'--debug'",
        "'--batch'",
        "'--test_batch'",
        "'--optim'",
        "'sgd'",
        "'adam|sgd'",
        "'--lr'",
        "'--model'",
        "'lattice'",
        "'lattice|lstm'",
        "'--skip_before_head'",
        "'--hidden'",
        "'--momentum'",
        "'--bi'",
        "'--dataset'",
        "'ontonote'",
        "'resume|ontonote|weibo|msra'",
        "'--use_bigram'",
        "'--embed_dropout'",
        "'--output_dropout'",
        "'--epoch'",
        "'--seed'",
        "'bi'",
        "'uni'",
        "'lattice'",
        "' '",
        "'ontonote'",
        "'resume'",
        "'cache'",
        "'_lattice'",
        "'中:embedding:{}'",
        "'char'",
        "'embed lookup dropout:{}'",
        "'word'",
        "'train'",
        "'vocab info:'",
        "'{}:{}'",
        "'lattice'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'skips_r2l_word'",
        "'skips_r2l_source'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'skips_r2l_word'",
        "'skips_r2l_source'",
        "'skips_l2r_word'",
        "'skips_r2l_word'",
        "'skips_l2r_source'",
        "'skips_r2l_source'",
        "'chars'",
        "'bigrams'",
        "'seq_len'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'lexicon_count'",
        "'skips_r2l_word'",
        "'skips_r2l_source'",
        "'lexicon_count_back'",
        "'target'",
        "'chars'",
        "'bigrams'",
        "'seq_len'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'lexicon_count'",
        "'target'",
        "'target'",
        "'seq_len'",
        "'target'",
        "'lstm'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'skips_l2r_word'",
        "'skips_l2r_source'",
        "'chars'",
        "'bigrams'",
        "'seq_len'",
        "'target'",
        "'target'",
        "'seq_len'",
        "'target'",
        "'dev'",
        "'skips_l2r_word'",
        "'lattice'",
        "'char'",
        "'bigram'",
        "'word'",
        "'label'",
        "'lstm'",
        "'char'",
        "'bigram'",
        "'word'",
        "'label'",
        "'{}:{}'",
        "'/remote-home/xnli/weight_debug/lattice_yangjie.pkl'",
        "'rb'",
        "'encoder.char_cell.weight_ih'",
        "'lstm.forward_lstm.rnn.weight_ih'",
        "'encoder.char_cell.weight_hh'",
        "'lstm.forward_lstm.rnn.weight_hh'",
        "'encoder.char_cell.alpha_weight_ih'",
        "'lstm.forward_lstm.rnn.alpha_weight_ih'",
        "'encoder.char_cell.alpha_weight_hh'",
        "'lstm.forward_lstm.rnn.alpha_weight_hh'",
        "'encoder.char_cell.bias'",
        "'lstm.forward_lstm.rnn.bias'",
        "'encoder.char_cell.alpha_bias'",
        "'lstm.forward_lstm.rnn.alpha_bias'",
        "'encoder.word_cell.weight_ih'",
        "'lstm.forward_lstm.word_rnn.weight_ih'",
        "'encoder.word_cell.weight_hh'",
        "'lstm.forward_lstm.word_rnn.weight_hh'",
        "'encoder.word_cell.bias'",
        "'lstm.forward_lstm.word_rnn.bias'",
        "'encoder_back.char_cell.weight_ih'",
        "'lstm.backward_lstm.rnn.weight_ih'",
        "'encoder_back.char_cell.weight_hh'",
        "'lstm.backward_lstm.rnn.weight_hh'",
        "'encoder_back.char_cell.alpha_weight_ih'",
        "'lstm.backward_lstm.rnn.alpha_weight_ih'",
        "'encoder_back.char_cell.alpha_weight_hh'",
        "'lstm.backward_lstm.rnn.alpha_weight_hh'",
        "'encoder_back.char_cell.bias'",
        "'lstm.backward_lstm.rnn.bias'",
        "'encoder_back.char_cell.alpha_bias'",
        "'lstm.backward_lstm.rnn.alpha_bias'",
        "'encoder_back.word_cell.weight_ih'",
        "'lstm.backward_lstm.word_rnn.weight_ih'",
        "'encoder_back.word_cell.weight_hh'",
        "'lstm.backward_lstm.word_rnn.weight_hh'",
        "'encoder_back.word_cell.bias'",
        "'lstm.backward_lstm.word_rnn.bias'",
        "'{}:{}'",
        "'label'",
        "'pred'",
        "'target'",
        "'seq_len'",
        "'bmeso'",
        "'label'",
        "'pred'",
        "'target'",
        "'seq_len'",
        "'bmesoyj'",
        "'pred'",
        "'target'",
        "'seq_len'",
        "'adam'",
        "'sgd'",
        "'train'",
        "'char'",
        "'：'",
        "'train'",
        "'dev'"
    ],
    "variables": [
        "parser",
        "args",
        "fit_msg_list",
        "fit_msg",
        "device",
        "refresh_data",
        "datasets",
        "vocabs",
        "embeddings",
        "datasets",
        "vocabs",
        "embeddings",
        "w_list",
        "cache_name",
        "datasets",
        "vocabs",
        "embeddings",
        "model",
        "model",
        "weight_dict",
        "param_list",
        "param_dict",
        "param_dict",
        "t",
        "loss",
        "f1_metric",
        "f1_metric_yj",
        "acc_metric",
        "metrics",
        "optimizer",
        "optimizer",
        "callbacks",
        "trainer"
    ],
    "comments": [
        "sys.path.append('.')",
        "sys.path.append('..')",
        "for p in sys.path:",
        "print(p)",
        "fitlog.add_hyper_in_file (__file__) # record your hyperparameters",
        "hyper",
        "hyper",
        "in paper it's false",
        "fitlog.commit(__file__,fit_msg=fit_msg)",
        "fitlog.add_hyper(args)",
        "exit()",
        "for k, v in datasets.items():",
        "#     v.apply_field(lambda x: list(map(len, x)), 'skips_l2r_word', 'lexicon_count')",
        "#     v.apply_field(lambda x:",
        "#                   list(map(lambda y:",
        "#                            list(map(lambda z: vocabs['word'].to_index(z), y)), x)),",
        "#                   'skips_l2r_word')",
        "print(datasets['dev'][0])",
        "print(datasets['test'][0])",
        "print(datasets['train'].get_all_fields().keys())",
        "exit()",
        "print(weight_dict.keys())",
        "for k,v in weight_dict.items():",
        "print('{}:{}'.format(k,v.size()))",
        "print(vocabs['label'].word2idx.keys())",
        "tester = Tester(datasets['dev'],model,metrics=metrics,batch_size=args.test_batch,device=device)",
        "test_result = tester.test()",
        "print(test_result)",
        "StaticEmbedding",
        "datasets['train'] = datasets['train'][1:]"
    ],
    "docstrings": [],
    "functions": [
        "state_dict_param",
        "copy_yangjie_lattice_weight"
    ],
    "classes": []
}