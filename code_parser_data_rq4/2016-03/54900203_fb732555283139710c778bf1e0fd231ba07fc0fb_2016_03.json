{
    "identifiers": [
        "c1",
        "c2",
        "c",
        "xrange",
        "ord",
        "c1",
        "ord",
        "c2",
        "l",
        "append",
        "chr",
        "c",
        "l",
        "feature",
        "voca_f",
        "len",
        "voca_f",
        "feature",
        "feature",
        "voca_f",
        "ValueError",
        "feature",
        "value",
        "value",
        "voca_f",
        "line",
        "voca",
        "line",
        "idx",
        "f",
        "line",
        "len",
        "voca",
        "idx",
        "translate_feature",
        "f",
        "voca",
        "idx",
        "idx",
        "offset",
        "len",
        "new_columns",
        "line",
        "idx",
        "line",
        "idx",
        "offset",
        "idx",
        "offset",
        "len",
        "new_columns",
        "new_columns",
        "offset",
        "len",
        "new_columns",
        "res",
        "lines",
        "voca",
        "translate_line",
        "l",
        "voca",
        "l",
        "lines",
        "lines",
        "voca",
        "throw_id",
        "delim",
        "l",
        "split",
        "delim",
        "l",
        "lines",
        "l",
        "len",
        "l",
        "l",
        "lines",
        "translate_lines",
        "lines",
        "voca",
        "lines",
        "feature",
        "l0",
        "feature",
        "l0",
        "l",
        "lines",
        "indx",
        "f",
        "l",
        "mean_l",
        "indx",
        "f",
        "sum_l",
        "indx",
        "feature",
        "feature",
        "l0",
        "feature",
        "feature",
        "l0",
        "feature",
        "l0",
        "l",
        "lines",
        "i",
        "f",
        "l",
        "max",
        "max_l",
        "i",
        "f",
        "min",
        "min_l",
        "i",
        "f",
        "i",
        "me",
        "mean_l",
        "mean_l",
        "i",
        "sum_l",
        "i",
        "max_l",
        "i",
        "min_l",
        "i",
        "i",
        "l",
        "lines",
        "j",
        "f",
        "l",
        "range_l",
        "j",
        "lines",
        "i",
        "lines",
        "i",
        "j",
        "mean_l",
        "j",
        "range_l",
        "j",
        "throw_id",
        "l",
        "l",
        "lines",
        "lines",
        "y",
        "mean_l",
        "range_l",
        "lines",
        "voc",
        "mean_l",
        "range_l",
        "throw_id",
        "delim",
        "l",
        "split",
        "delim",
        "l",
        "lines",
        "translate_lines",
        "lines",
        "voc",
        "i",
        "l",
        "lines",
        "j",
        "f",
        "l",
        "range_l",
        "j",
        "lines",
        "i",
        "lines",
        "i",
        "j",
        "mean_l",
        "j",
        "range_l",
        "j",
        "throw_id",
        "l",
        "l",
        "lines",
        "lines",
        "training_lines",
        "test_lines",
        "voc",
        "throw_id",
        "delim",
        "process_training_lines",
        "training_lines",
        "voc",
        "throw_id",
        "delim",
        "process_test_lines",
        "test_lines",
        "voc",
        "mean_l",
        "range_l",
        "throw_id",
        "delim",
        "training_x",
        "training_y",
        "test_x",
        "file_name",
        "skip_header",
        "open",
        "file_name",
        "f",
        "skip_header",
        "line",
        "f",
        "header",
        "line",
        "rstrip",
        "rstrip",
        "lines",
        "append",
        "line",
        "lines",
        "lines",
        "l",
        "lines",
        "random",
        "randint",
        "test",
        "append",
        "l",
        "train",
        "append",
        "l",
        "train",
        "test",
        "file_name",
        "skip_header",
        "delim",
        "load_data",
        "file_name",
        "skip_header",
        "split_cva",
        "lines",
        "l",
        "split",
        "delim",
        "l",
        "test_lines",
        "l",
        "l",
        "test_lines",
        "reduce",
        "x",
        "y",
        "x",
        "y",
        "l",
        "l",
        "testing_lines",
        "train_lines",
        "testing_lines",
        "testingTruth"
    ],
    "literals": [
        "\"Problem in translate_feature, value not in vocabulary\"",
        "\"Probleme in translate_line\"",
        "','",
        "','",
        "','",
        "'r'",
        "'\\n'",
        "'\\r'",
        "','",
        "','"
    ],
    "variables": [
        "l",
        "res",
        "offset",
        "new_columns",
        "res",
        "res",
        "lines",
        "y",
        "lines",
        "l0",
        "sum_l",
        "mean_l",
        "min_l",
        "max_l",
        "range_l",
        "max_l",
        "i",
        "min_l",
        "i",
        "mean_l",
        "i",
        "range_l",
        "i",
        "j",
        "lines",
        "lines",
        "lines",
        "j",
        "lines",
        "training_x",
        "training_y",
        "mean_l",
        "range_l",
        "test_x",
        "lines",
        "header",
        "header",
        "line",
        "train",
        "test",
        "lines",
        "train_lines",
        "test_lines",
        "test_lines",
        "testingTruth",
        "testing_lines"
    ],
    "comments": [
        "!/usr/bin/env python",
        "-*- coding: utf-8 -*-",
        "def load_data_and_split_cva(file_name, voc, skip_header=True, delim=',',",
        "throw_id=True):",
        "\"\"\" Load data from file, preprocess it and split into training set",
        "and testing set.\"\"\"",
        "x, y = load_data(file_name, voc, skip_header, delim, throw_id)",
        "train, test = split_cva(zip(x, y))",
        "train_x = [t[0] for t in train]",
        "train_y = [t[1] for t in train]",
        "test_x = [t[0] for t in test]",
        "test_y = [t[1] for t in test]",
        "return train_x, train_y, test_x, test_y",
        "def load_and_split(file_name, voc, skip_header=True, delim=',', throw_id=True):",
        "\"\"\" Load data from file, preprocesss it and split into training set,",
        "cva set and testing set. Thats the function you want to use.\"\"\"",
        "x, y = load_data(file_name, voc, skip_header, delim, throw_id)",
        "train, test = split_cva(zip(x, y))",
        "train, cva = split_cva(train)",
        "train_x = [t[0] for t in train]",
        "train_y = [t[1] for t in train]",
        "test_x = [t[0] for t in test]",
        "test_y = [t[1] for t in test]",
        "cva_x = [t[0] for t in cva]",
        "cva_y = [t[1] for t in cva]",
        "return train_x, train_y, cva_x, cva_y, test_x, test_y"
    ],
    "docstrings": [
        "\"\"\"\nlines_parse.py\n~~~~~~~~~~\n\nParsing lines before feeding them into ML algorithms\n\"\"\"",
        "\"\"\"Translate a feature with respect with its vocabulary.\"\"\"",
        "\"\"\"Translate one line with respect to the vocabulary.\"\"\"",
        "\"\"\"Translate all the lines of input data with respect to a vocabulary.\"\"\"",
        "\"\"\"Translate the row input and perform feature scaling.\"\"\"",
        "\"\"\"Transform the testing lines. Usually not needed.\"\"\"",
        "\"\"\"Transform both the training lines and testing lines.\"\"\"",
        "\"\"\" Load the data from a file.\"\"\"",
        "\"\"\" Split the input into 3/4 and 1/4.\"\"\"",
        "\"\"\" Load data, split into testing set and cva, split the cva truth\n\tAND returns the set as string lists.\"\"\""
    ],
    "functions": [
        "char_range",
        "translate_feature",
        "translate_line",
        "translate_lines",
        "process_training_lines",
        "process_test_lines",
        "process_inputs",
        "load_data",
        "split_cva",
        "load_data_and_split"
    ],
    "classes": []
}