{
    "identifiers": [
        "os",
        "json",
        "fastapi",
        "FastAPI",
        "HTTPException",
        "Body",
        "fastapi",
        "responses",
        "StreamingResponse",
        "JSONResponse",
        "pydantic",
        "BaseModel",
        "typing",
        "Dict",
        "Optional",
        "httpx",
        "FastAPI",
        "os",
        "environ",
        "get",
        "BaseModel",
        "Optional",
        "prompt",
        "model",
        "LLM_SERVER_URL",
        "httpx",
        "AsyncClient",
        "timeout",
        "client",
        "model",
        "prompt",
        "client",
        "stream",
        "url",
        "json",
        "payload",
        "response",
        "response",
        "status_code",
        "HTTPException",
        "status_code",
        "response",
        "status_code",
        "detail",
        "response",
        "status_code",
        "response",
        "text",
        "chunk",
        "response",
        "aiter_bytes",
        "chunk",
        "decode",
        "line",
        "decoded_chunk",
        "split",
        "line",
        "strip",
        "json",
        "loads",
        "line",
        "data",
        "data",
        "json",
        "JSONDecodeError",
        "httpx",
        "RequestError",
        "e",
        "HTTPException",
        "status_code",
        "detail",
        "e",
        "prompt",
        "model",
        "LLM_SERVER_URL",
        "httpx",
        "AsyncClient",
        "timeout",
        "client",
        "model",
        "prompt",
        "client",
        "post",
        "url",
        "json",
        "payload",
        "response",
        "raise_for_status",
        "line",
        "response",
        "text",
        "splitlines",
        "line",
        "strip",
        "json",
        "loads",
        "line",
        "data",
        "combined_response",
        "data",
        "json",
        "JSONDecodeError",
        "combined_response",
        "httpx",
        "RequestError",
        "e",
        "HTTPException",
        "status_code",
        "detail",
        "e",
        "app",
        "post",
        "query",
        "Query",
        "query",
        "stream",
        "StreamingResponse",
        "stream_generated_text",
        "query",
        "prompt",
        "query",
        "model",
        "media_type",
        "get_generated_text",
        "query",
        "prompt",
        "query",
        "model",
        "JSONResponse",
        "response",
        "app",
        "post",
        "llm_name",
        "Body",
        "embed",
        "LLM_SERVER_URL",
        "httpx",
        "AsyncClient",
        "client",
        "client",
        "post",
        "url",
        "json",
        "llm_name",
        "response",
        "raise_for_status",
        "llm_name",
        "httpx",
        "RequestError",
        "e",
        "HTTPException",
        "status_code",
        "detail",
        "e",
        "app",
        "get",
        "LLM_SERVER_URL",
        "httpx",
        "AsyncClient",
        "client",
        "client",
        "get",
        "url",
        "response",
        "raise_for_status",
        "response",
        "json",
        "httpx",
        "RequestError",
        "e",
        "HTTPException",
        "status_code",
        "detail",
        "e",
        "uvicorn",
        "uvicorn",
        "run",
        "app",
        "host",
        "port"
    ],
    "literals": [
        "\"LLM_SERVER_URL\"",
        "\"http://localhost:11434\"",
        "\"mistral:7b\"",
        "\"mistral:7b\"",
        "f\"{LLM_SERVER_URL}/api/generate\"",
        "\"model\"",
        "\"prompt\"",
        "\"stream\"",
        "\"options\"",
        "\"temperature\"",
        "\"top_p\"",
        "\"top_k\"",
        "\"POST\"",
        "f\"Error al conectar con el servidor LLM: {response.status_code} - {response.text}\"",
        "'utf-8'",
        "\"\\n\"",
        "\"response\"",
        "\"response\"",
        "f\"Error de comunicación con el servidor LLM: {e}\"",
        "\"mistral:7b\"",
        "f\"{LLM_SERVER_URL}/api/generate\"",
        "\"model\"",
        "\"prompt\"",
        "\"stream\"",
        "\"options\"",
        "\"temperature\"",
        "\"top_p\"",
        "\"top_k\"",
        "\"\"",
        "\"response\"",
        "\"response\"",
        "\"response\"",
        "f\"Error de comunicación con el servidor LLM: {e}\"",
        "\"/api/generate\"",
        "\"text/plain\"",
        "\"/api/models/download\"",
        "f\"{LLM_SERVER_URL}/api/pull\"",
        "\"name\"",
        "\"message\"",
        "f\"Model {llm_name} downloaded successfully\"",
        "f\"Error al descargar el modelo: {e}\"",
        "\"/api/models\"",
        "f\"{LLM_SERVER_URL}/api/tags\"",
        "\"models\"",
        "\"models\"",
        "f\"Error al obtener la lista de modelos: {e}\"",
        "\"__main__\"",
        "\"0.0.0.0\""
    ],
    "variables": [
        "app",
        "LLM_SERVER_URL",
        "prompt",
        "model",
        "stream",
        "url",
        "payload",
        "decoded_chunk",
        "data",
        "url",
        "payload",
        "response",
        "combined_response",
        "data",
        "response",
        "url",
        "response",
        "url",
        "response"
    ],
    "comments": [
        "Obtener la URL del servidor LLM desde una variable de entorno",
        "Cambiado el modelo por defecto a mistral",
        "Configuración específica para Mistral",
        "Configuración específica para Mistral",
        "Los endpoints permanecen iguales"
    ],
    "docstrings": [
        "\"\"\"\n    Modelo para las solicitudes a la API.\n    \"\"\"",
        "\"\"\"\n    Genera texto de forma asíncrona usando Mistral.\n    \"\"\"",
        "\"\"\"\n    Obtiene el texto completo generado por Mistral.\n    \"\"\""
    ],
    "functions": [
        "stream_generated_text",
        "get_generated_text",
        "generate_text",
        "download_model",
        "list_models"
    ],
    "classes": [
        "Query"
    ]
}