{
    "identifiers": [
        "numpy",
        "np",
        "h5py",
        "matplotlib",
        "pyplot",
        "plt",
        "testCases_v4a",
        "dnn_utils_v2",
        "sigmoid",
        "sigmoid_backward",
        "relu",
        "relu_backward",
        "plt",
        "rcParams",
        "plt",
        "rcParams",
        "plt",
        "rcParams",
        "np",
        "random",
        "seed",
        "n_x",
        "n_h",
        "n_y",
        "np",
        "random",
        "seed",
        "np",
        "random",
        "randn",
        "n_h",
        "n_x",
        "np",
        "zeros",
        "n_h",
        "np",
        "random",
        "randn",
        "n_y",
        "n_h",
        "np",
        "zeros",
        "n_y",
        "W1",
        "shape",
        "n_h",
        "n_x",
        "b1",
        "shape",
        "n_h",
        "W2",
        "shape",
        "n_y",
        "n_h",
        "b2",
        "shape",
        "n_y",
        "W1",
        "b1",
        "W2",
        "b2",
        "parameters",
        "layer_dims",
        "np",
        "random",
        "seed",
        "len",
        "layer_dims",
        "l",
        "L",
        "l",
        "np",
        "random",
        "randn",
        "layer_dims",
        "l",
        "layer_dims",
        "l",
        "l",
        "np",
        "zeros",
        "layer_dims",
        "l",
        "parameters",
        "l",
        "shape",
        "layer_dims",
        "l",
        "layer_dims",
        "l",
        "parameters",
        "l",
        "shape",
        "layer_dims",
        "l",
        "parameters",
        "A",
        "W",
        "b",
        "np",
        "dot",
        "W",
        "A",
        "b",
        "Z",
        "shape",
        "W",
        "shape",
        "A",
        "shape",
        "A",
        "W",
        "b",
        "Z",
        "cache",
        "A_prev",
        "W",
        "b",
        "activation",
        "activation",
        "linear_forward",
        "A_prev",
        "W",
        "b",
        "sigmoid",
        "Z",
        "activation",
        "linear_forward",
        "A_prev",
        "W",
        "b",
        "relu",
        "Z",
        "A",
        "shape",
        "W",
        "shape",
        "A_prev",
        "shape",
        "linear_cache",
        "activation_cache",
        "A",
        "cache",
        "X",
        "parameters",
        "X",
        "len",
        "parameters",
        "l",
        "L",
        "A",
        "parameters",
        "l",
        "parameters",
        "l",
        "linear_activation_forward",
        "A_prev",
        "W",
        "b",
        "activation",
        "caches",
        "append",
        "cache",
        "parameters",
        "L",
        "parameters",
        "L",
        "A",
        "linear_activation_forward",
        "A_prev",
        "W",
        "b",
        "activation",
        "caches",
        "append",
        "cache",
        "AL",
        "shape",
        "X",
        "shape",
        "AL",
        "caches",
        "AL",
        "Y",
        "Y",
        "shape",
        "m",
        "np",
        "dot",
        "np",
        "log",
        "AL",
        "Y",
        "T",
        "m",
        "np",
        "dot",
        "np",
        "log",
        "AL",
        "Y",
        "T",
        "np",
        "squeeze",
        "cost",
        "cost",
        "shape",
        "cost",
        "dZ",
        "cache",
        "cache",
        "A_prev",
        "shape",
        "m",
        "np",
        "dot",
        "dZ",
        "A_prev",
        "T",
        "m",
        "np",
        "sum",
        "dZ",
        "axis",
        "keepdims",
        "np",
        "dot",
        "W",
        "T",
        "dZ",
        "dA_prev",
        "shape",
        "A_prev",
        "shape",
        "dW",
        "shape",
        "W",
        "shape",
        "db",
        "shape",
        "b",
        "shape",
        "dA_prev",
        "dW",
        "db",
        "dA",
        "cache",
        "activation",
        "cache",
        "activation",
        "relu_backward",
        "dA",
        "activation_cache",
        "linear_backward",
        "dZ",
        "linear_cache",
        "activation",
        "sigmoid_backward",
        "dA",
        "activation_cache",
        "linear_backward",
        "dZ",
        "linear_cache",
        "dA_prev",
        "dW",
        "db",
        "AL",
        "Y",
        "caches",
        "len",
        "caches",
        "AL",
        "shape",
        "Y",
        "reshape",
        "AL",
        "shape",
        "np",
        "divide",
        "Y",
        "AL",
        "np",
        "divide",
        "Y",
        "AL",
        "caches",
        "L",
        "grads",
        "L",
        "grads",
        "L",
        "grads",
        "L",
        "linear_activation_backward",
        "dAL",
        "current_cache",
        "l",
        "reversed",
        "L",
        "caches",
        "l",
        "linear_activation_backward",
        "grads",
        "l",
        "current_cache",
        "l",
        "dA_prev_temp",
        "l",
        "dW_temp",
        "l",
        "db_temp",
        "grads",
        "parameters",
        "grads",
        "learning_rate",
        "len",
        "parameters",
        "l",
        "L",
        "l",
        "parameters",
        "l",
        "learning_rate",
        "grads",
        "l",
        "l",
        "parameters",
        "l",
        "learning_rate",
        "grads",
        "l",
        "parameters"
    ],
    "literals": [
        "'figure.figsize'",
        "'image.interpolation'",
        "'nearest'",
        "'image.cmap'",
        "'gray'",
        "\"W1\"",
        "\"b1\"",
        "\"W2\"",
        "\"b2\"",
        "'W'",
        "'b'",
        "'W'",
        "'b'",
        "\"sigmoid\"",
        "\"relu\"",
        "'W'",
        "'b'",
        "\"relu\"",
        "'W'",
        "'b'",
        "'sigmoid'",
        "\"relu\"",
        "\"sigmoid\"",
        "\"dA\"",
        "\"dW\"",
        "\"db\"",
        "'sigmoid'",
        "'dA'",
        "'relu'",
        "\"dA\"",
        "\"dW\"",
        "\"db\"",
        "\"W\"",
        "\"W\"",
        "\"dW\"",
        "\"b\"",
        "\"b\"",
        "\"db\""
    ],
    "variables": [
        "W1",
        "b1",
        "W2",
        "b2",
        "parameters",
        "parameters",
        "L",
        "parameters",
        "parameters",
        "Z",
        "cache",
        "Z",
        "linear_cache",
        "A",
        "activation_cache",
        "Z",
        "linear_cache",
        "A",
        "activation_cache",
        "cache",
        "caches",
        "A",
        "L",
        "A_prev",
        "W",
        "b",
        "A",
        "cache",
        "W",
        "b",
        "A_prev",
        "AL",
        "cache",
        "m",
        "cost",
        "cost",
        "A_prev",
        "W",
        "b",
        "m",
        "dW",
        "db",
        "dA_prev",
        "linear_cache",
        "activation_cache",
        "dZ",
        "dA_prev",
        "dW",
        "db",
        "dZ",
        "dA_prev",
        "dW",
        "db",
        "grads",
        "L",
        "m",
        "Y",
        "dAL",
        "current_cache",
        "current_cache",
        "dA_prev_temp",
        "dW_temp",
        "db_temp",
        "grads",
        "grads",
        "grads",
        "L",
        "parameters",
        "parameters"
    ],
    "comments": [
        "set default size of plots",
        "GRADED FUNCTION: initialize_parameters",
        "START CODE HERE ### (≈ 4 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: initialize_parameters_deep",
        "number of layers in the network",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: linear_forward",
        "START CODE HERE ### (≈ 1 line of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: linear_activation_forward",
        "Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: L_model_forward",
        "number of layers in the neural network",
        "Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: compute_cost",
        "Compute loss from aL and y.",
        "START CODE HERE ### (≈ 1 lines of code)",
        "END CODE HERE ###",
        "To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).",
        "GRADED FUNCTION: linear_backward",
        "START CODE HERE ### (≈ 3 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: linear_activation_backward",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "START CODE HERE ### (≈ 2 lines of code)",
        "END CODE HERE ###",
        "GRADED FUNCTION: L_model_backward",
        "the number of layers",
        "after this line, Y is the same shape as AL",
        "Initializing the backpropagation",
        "START CODE HERE ### (1 line of code)",
        "END CODE HERE ###",
        "Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]",
        "START CODE HERE ### (approx. 2 lines)",
        "END CODE HERE ###",
        "Loop from l=L-2 to l=0",
        "lth layer: (RELU -> LINEAR) gradients.",
        "Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]",
        "START CODE HERE ### (approx. 5 lines)",
        "END CODE HERE ###",
        "number of layers in the neural network",
        "Update rule for each parameter. Use a for loop.",
        "START CODE HERE ### (≈ 3 lines of code)",
        "END CODE HERE ###"
    ],
    "docstrings": [
        "\"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"",
        "\"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"",
        "\"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"",
        "\"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"",
        "\"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n    \"\"\"",
        "\"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"",
        "\"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"",
        "\"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"",
        "\"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"",
        "\"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\""
    ],
    "functions": [
        "initialize_parameters",
        "initialize_parameters_deep",
        "linear_forward",
        "linear_activation_forward",
        "L_model_forward",
        "compute_cost",
        "linear_backward",
        "linear_activation_backward",
        "L_model_backward",
        "update_parameters"
    ],
    "classes": []
}