{
    "identifiers": [
        "json",
        "re",
        "nltk",
        "nltk",
        "stem",
        "WordNetLemmatizer",
        "nltk",
        "probability",
        "FreqDist",
        "nltk",
        "trigrams",
        "nltk",
        "bigrams",
        "open",
        "f",
        "lines",
        "f",
        "my_data",
        "append",
        "json",
        "loads",
        "lines",
        "i",
        "my_data",
        "my_data",
        "index",
        "i",
        "i",
        "lower",
        "i",
        "my_data",
        "re",
        "sub",
        "pattern1",
        "i",
        "re",
        "sub",
        "pattern2",
        "a",
        "re",
        "sub",
        "pattern3",
        "b",
        "re",
        "sub",
        "pattern4",
        "c",
        "re",
        "sub",
        "pattern5",
        "d",
        "my_data",
        "index",
        "i",
        "nltk",
        "word_tokenize",
        "e",
        "WordNetLemmatizer",
        "news",
        "my_data",
        "i",
        "len",
        "news",
        "lemmatizer",
        "lemmatize",
        "news",
        "i",
        "i",
        "my_data",
        "corpus",
        "extend",
        "i",
        "len",
        "corpus",
        "corpus",
        "len",
        "vocab",
        "N",
        "V",
        "trigrams",
        "corpus",
        "FreqDist",
        "tri",
        "f",
        "most_common",
        "i",
        "i",
        "top_tri",
        "top_tri",
        "open",
        "f",
        "f",
        "read",
        "splitlines",
        "open",
        "f",
        "f",
        "read",
        "splitlines",
        "lis",
        "word",
        "len",
        "lis",
        "left",
        "right",
        "left",
        "right",
        "word",
        "lis",
        "mid",
        "mid",
        "word",
        "lis",
        "mid",
        "mid",
        "mid",
        "i",
        "corpus",
        "binary_search",
        "positive_words",
        "i",
        "w1",
        "pos",
        "binary_search",
        "negative_words",
        "i",
        "w2",
        "neg",
        "pos",
        "neg",
        "news",
        "my_data",
        "i",
        "news",
        "binary_search",
        "positive_words",
        "i",
        "w1",
        "pos",
        "binary_search",
        "negative_words",
        "i",
        "w2",
        "neg",
        "pos",
        "neg",
        "pos_news",
        "pos",
        "neg",
        "neg_news",
        "pos_news",
        "neg_news",
        "i",
        "my_data",
        "cor1",
        "extend",
        "i",
        "cor1",
        "trigrams",
        "cor1",
        "FreqDist",
        "tri_1",
        "i",
        "i",
        "ft1",
        "values",
        "x",
        "x",
        "count_t",
        "i",
        "i",
        "ft1",
        "keys",
        "trigram1",
        "count_t",
        "i",
        "i",
        "bigrams",
        "cor1",
        "FreqDist",
        "bi_1",
        "len",
        "bi_1",
        "i",
        "i",
        "fb1",
        "values",
        "x",
        "x",
        "V2",
        "count_b",
        "i",
        "i",
        "fb1",
        "keys",
        "bigram1",
        "count_b",
        "math",
        "w1",
        "w2",
        "w3",
        "w1",
        "w2",
        "w3",
        "tri1",
        "w1",
        "w2",
        "bi1",
        "tri1",
        "w1",
        "w2",
        "w3",
        "bi1",
        "w1",
        "w2",
        "math",
        "log",
        "p",
        "log_p",
        "i",
        "v",
        "vocab1",
        "prob",
        "w1",
        "w2",
        "v",
        "log_p",
        "score",
        "append",
        "round",
        "log_p",
        "words",
        "append",
        "v",
        "words",
        "score",
        "index",
        "max",
        "score",
        "w2",
        "w3",
        "w3",
        "sen",
        "append",
        "w3",
        "join",
        "sen",
        "sentence"
    ],
    "literals": [
        "\"signal-news1.jsonl\"",
        "\"r\"",
        "'content'",
        "\"(http://[A-Za-z0-9]+(\\.[A-Za-z0-9]*)*(/[A-Za-z0-9]*)*)\"",
        "\"\\W\"",
        "\" [A-Za-z] \"",
        "\"^[A-Za-z] \"",
        "\"([^A-Za-z]+[0-9]+)|([0-9]*[^A-Za-z]+)\"",
        "''",
        "' '",
        "' '",
        "''",
        "' '",
        "\"opinion-lexicon-English/positive-words.txt\"",
        "'r'",
        "\"opinion-lexicon-English/negative-words.txt\"",
        "'r'",
        "'is'",
        "'this'",
        "'is'",
        "'this'",
        "' '"
    ],
    "variables": [
        "my_data",
        "my_data",
        "pattern1",
        "pattern2",
        "pattern3",
        "pattern4",
        "pattern5",
        "a",
        "b",
        "c",
        "d",
        "e",
        "my_data",
        "lemmatizer",
        "news",
        "i",
        "corpus",
        "N",
        "vocab",
        "V",
        "tri",
        "f",
        "top_tri",
        "top_tri",
        "positive_words",
        "negative_words",
        "positive_words",
        "negative_words",
        "left",
        "right",
        "mid",
        "right",
        "left",
        "pos",
        "neg",
        "w1",
        "w2",
        "pos_news",
        "neg_news",
        "pos",
        "neg",
        "w1",
        "w2",
        "cor1",
        "vocab1",
        "tri_1",
        "ft1",
        "count_t",
        "count_t",
        "trigram1",
        "tri1",
        "bi_1",
        "fb1",
        "V2",
        "count_b",
        "count_b",
        "bigram1",
        "bi1",
        "p",
        "log_p",
        "w1",
        "w2",
        "sen",
        "score",
        "words",
        "log_p",
        "w3",
        "w1",
        "w2",
        "sentence"
    ],
    "comments": [
        "Part A",
        "match url",
        "match non-alphabetic",
        "3 and 4 match single character",
        "match pure digits",
        "keep white space",
        "Part B",
        "Part C"
    ],
    "docstrings": [
        "'''\nfrom nltk.corpus import wordnet\ndef get_word_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\nfor news in my_data:\n    news_pos=nltk.pos_tag(news)\n    for i in range(len(news)):\n        word_pos=get_word_pos(news_pos[i][1])\n        if word_pos!='':\n            news[i]=lemmatizer.lemmatize(news[i],pos=word_pos)\n'''",
        "'''\ncor2=[]\nfor i in my_data[16000:]:\n    cor2.extend(i)\nvocab2=set(cor2)\n\ntri2=[i for i in bigrams(cor2)]\nft2=FreqDist(tri2) # the dict to store (trigram-number of trigram) pairs\ncount_t2=[i for i in ft2.values()]\ncount_t2=list(map(lambda x :x+1,count_t2))\ntrigram2=[i for i in ft2.keys()]\ntri2=dict(zip(trigram2,count_t2))\n\nbi2=[i for i in bigrams(cor2)]\nfb2=FreqDist(bi2)\nV22=len(set(bi2))\ncount_b2=[i for i in fb2.values()]\ncount_b2=list(map(lambda x :x+V22,count_b2))\nbigram2=[i for i in fb2.keys()]\nbi2=dict(zip(bigram2,count_b2))\n\ndef prob2(w1,w2,w3):\n    p=tri2[(w1,w2,w3)]/bi2[(w1,w2)]\n    log_p=math.log(p)\n    return log_p\n\npp=[]\nfor news in my_data[16000:]:\n    t=trigrams(news)\n    b=bigrams(news)\n    w1=news[0]\n    w2=news[1]\n    total_log=[]\n    for i in news[2:]:\n        log_p=prob2(w1,w2,i)\n        if log_p!=None:\n            total_log.append(log_p)\n        w1=w2\n        w2=i\n    sum_t=sum(total_log)\n    print(sum_t)\n    perplexity=pow(1/abs(sum_t),1/len(news))\n    pp.append(perplexity)\n\nperp=sum(pp)/len(pp)\nprint(perp)\n'''"
    ],
    "functions": [
        "binary_search",
        "prob"
    ],
    "classes": []
}