{
    "identifiers": [
        "os",
        "pickle",
        "torch",
        "path",
        "os",
        "path",
        "join",
        "path",
        "open",
        "input_file",
        "f",
        "f",
        "read",
        "data",
        "dataset_path",
        "token_lookup",
        "create_lookup_tables",
        "load_data",
        "dataset_path",
        "text",
        "token_lookup",
        "key",
        "token",
        "token_dict",
        "items",
        "text",
        "replace",
        "key",
        "format",
        "token",
        "text",
        "lower",
        "text",
        "split",
        "create_lookup_tables",
        "text",
        "SPECIAL_WORDS",
        "values",
        "vocab_to_int",
        "word",
        "word",
        "text",
        "pickle",
        "dump",
        "int_text",
        "vocab_to_int",
        "int_to_vocab",
        "token_dict",
        "open",
        "pickle",
        "load",
        "open",
        "mode",
        "filename",
        "decoder",
        "os",
        "path",
        "splitext",
        "os",
        "path",
        "basename",
        "filename",
        "torch",
        "save",
        "decoder",
        "save_filename",
        "filename",
        "os",
        "path",
        "splitext",
        "os",
        "path",
        "basename",
        "filename",
        "torch",
        "load",
        "save_filename"
    ],
    "literals": [
        "'PADDING'",
        "'<PAD>'",
        "\"r\"",
        "' {} '",
        "'preprocess.p'",
        "'wb'",
        "'preprocess.p'",
        "'rb'",
        "'.pt'",
        "'.pt'"
    ],
    "variables": [
        "SPECIAL_WORDS",
        "input_file",
        "data",
        "text",
        "text",
        "token_dict",
        "text",
        "text",
        "text",
        "vocab_to_int",
        "int_to_vocab",
        "int_text",
        "save_filename",
        "save_filename"
    ],
    "comments": [
        "Ignore notice, since we don't use it for analysing the data"
    ],
    "docstrings": [
        "\"\"\"\n    Load Dataset from File\n    \"\"\"",
        "\"\"\"\n    Preprocess Text Data\n    \"\"\"",
        "\"\"\"\n    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n    \"\"\""
    ],
    "functions": [
        "load_data",
        "preprocess_and_save_data",
        "load_preprocess",
        "save_model",
        "load_model"
    ],
    "classes": []
}