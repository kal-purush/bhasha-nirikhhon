{
    "identifiers": [
        "armory",
        "log",
        "meter",
        "meters",
        "append",
        "meter",
        "preprocessing",
        "named_values",
        "measured",
        "update",
        "named_values",
        "meters",
        "_warned",
        "log",
        "warning",
        "name",
        "value",
        "named_values",
        "items",
        "meter",
        "is_measuring",
        "name",
        "meter",
        "meters",
        "p",
        "preprocessing",
        "p",
        "value",
        "meter",
        "meters",
        "meter",
        "is_measuring",
        "name",
        "meter",
        "update",
        "name",
        "value",
        "preprocessing",
        "input",
        "output",
        "mode",
        "mode",
        "hook_torch",
        "preprocessing",
        "input",
        "input",
        "output",
        "output",
        "mode",
        "hook_tf",
        "preprocessing",
        "input",
        "input",
        "output",
        "output",
        "ValueError",
        "mode",
        "preprocessing",
        "input",
        "output",
        "NotImplementedError",
        "preprocessing",
        "input",
        "output",
        "hasattr",
        "ValueError",
        "input",
        "input",
        "isinstance",
        "input",
        "ValueError",
        "input",
        "output",
        "output",
        "isinstance",
        "output",
        "ValueError",
        "output",
        "input",
        "output",
        "ValueError",
        "_hooks",
        "ValueError",
        "hook_module",
        "hook_input",
        "hook_output",
        "hook_module",
        "input",
        "hook_input",
        "output",
        "hook_output",
        "update",
        "preprocessing",
        "key_values",
        "register_forward_hook",
        "hook_fn",
        "_hooks",
        "hook",
        "_hooks",
        "mode",
        "hook",
        "remove",
        "mode",
        "NotImplementedError",
        "ValueError",
        "mode",
        "_hooks",
        "pop",
        "_hooks",
        "remove",
        "name",
        "named_values",
        "args",
        "kwargs",
        "NotImplementedError",
        "Meter",
        "name",
        "Meter",
        "name",
        "named_values",
        "name",
        "value",
        "named_values",
        "items",
        "values",
        "value",
        "log",
        "info",
        "step",
        "stage",
        "name",
        "value",
        "value",
        "stage",
        "step",
        "set_stage",
        "stage",
        "set_step",
        "step",
        "stage",
        "isinstance",
        "stage",
        "ValueError",
        "stage",
        "stage",
        "step",
        "isinstance",
        "step",
        "ValueError",
        "step",
        "step",
        "name",
        "name",
        "name",
        "NotImplementedError",
        "_PROBES",
        "name",
        "_PROBES",
        "Probe",
        "_PROBES",
        "name",
        "meter",
        "name",
        "get_probe",
        "name",
        "name",
        "probe",
        "connect",
        "meter"
    ],
    "literals": [
        "\"No Meter set up!\"",
        "\"pytorch\"",
        "\"pytorch\"",
        "\"tf\"",
        "f\"mode {mode} not in ('pytorch', 'tf')\"",
        "\"hooking not ready for tensorflow\"",
        "\"register_forward_hook\"",
        "f\"module {module} does not have method 'register_forward_hook'. Is it a torch.nn.Module?\"",
        "\"\"",
        "f\"input {input} must be None or a non-empty string\"",
        "\"\"",
        "f\"output {output} must be None or a non-empty string\"",
        "\"input and output cannot both be None\"",
        "f\"module {module} is already hooked\"",
        "\"pytorch\"",
        "\"pytorch\"",
        "\"tf\"",
        "f\"mode {mode} not in ('pytorch', 'tf')\"",
        "\"Implement in subclasses of Meter if needed\"",
        "f\"LogMeter: step {self.step}, {self.stage}_{name} = {value}, type = {type(value)}\"",
        "\"\"",
        "f\"'stage' must be a str, not {type(stage)}\"",
        "f\"'step' must be an int, not {type(step)}\"",
        "\"name hierarchy not implemented yet. Set name to None\""
    ],
    "variables": [
        "_PROBES",
        "meters",
        "_hooks",
        "_warned",
        "measured",
        "_warned",
        "value",
        "key_values",
        "key_values",
        "input",
        "key_values",
        "output",
        "hook",
        "hook",
        "mode",
        "values",
        "name",
        "stage",
        "step",
        "name",
        "_PROBES",
        "name",
        "probe"
    ],
    "comments": [
        "NOTE:",
        "https://discuss.pytorch.org/t/get-the-activations-of-the-second-to-last-layer/55629/6",
        "TensorFlow hooks",
        "https://www.tensorflow.org/api_docs/python/tf/estimator/SessionRunHook",
        "https://github.com/tensorflow/tensorflow/issues/33478",
        "https://github.com/tensorflow/tensorflow/issues/33129",
        "https://stackoverflow.com/questions/48966281/get-intermediate-output-from-keras-tensorflow-during-prediction",
        "https://stackoverflow.com/questions/59493222/access-output-of-intermediate-layers-in-tensor-flow-2-0-in-eager-mode/60945216#60945216",
        "instead of \"Process\" or \"Experiment\", which are overloaded",
        "class MetricsMeter(Meter):",
        "def __init__(self, **kwargs):",
        "super().__init__(**kwargs)",
        "self.probes = set()",
        "self.values = {}",
        "self.metrics_dict = {}",
        "self.stages_mapping = {}",
        "",
        "@classmethod",
        "def from_config(cls, config, skip_benign=False, skip_attack=False, targeted=False):",
        "return cls.from_kwargs(**config)",
        "",
        "@classmethod",
        "def from_kwargs(",
        "cls,",
        "means=True,",
        "perturbation=\"linf\",",
        "profiler_type=None,  # Ignored",
        "record_metric_per_sample=True,",
        "task=(\"categorical_accuracy\",),",
        "skip_benign=None,",
        "skip_attack=None,",
        "targeted=False,",
        "):",
        "if not record_metric_per_sample:",
        "log.warning(\"record_metric_per_sample overridden to True\")",
        "",
        "m = cls()",
        "kwargs = dict(aggregator=\"mean\" if bool(means) else None,)",
        "",
        "if isinstance(perturbation, str):",
        "perturbation = [perturbation]",
        "for metric in perturbation:",
        "m.add_metric(",
        "f\"perturbation_{metric}\",",
        "metric,",
        "\"x\",",
        "\"x_adv\",",
        "stages=\"perturbation\",",
        ")",
        "for metric in task:",
        "if not skip_benign:",
        "m.add_metric(",
        "f\"benign_{metric}\",",
        "metric,",
        "\"y\",",
        "\"y_pred\",",
        "stages=\"benign_task\",",
        ")",
        "if not skip_attack:",
        "m.add_metric(",
        "f\"adversarial_{metric}\",",
        "metric,",
        "\"y\",",
        "\"y_pred_adv\",",
        "stages=\"adversarial_task\",",
        ")",
        "if targeted:",
        "m.add_metric(",
        "f\"targeted_{metric}\",",
        "metric,",
        "\"y_target\",",
        "\"y_pred_adv\",",
        "stages=\"adversarial_task\",",
        ")",
        "",
        "return m",
        "",
        "def is_measuring(self, name):",
        "return name in self.probes",
        "",
        "def update(self, **probe_named_values):",
        "for probe_name in probe_named_values:",
        "if not self.is_measuring(probe_name):",
        "raise ValueError(f\"{probe_name} is not being measured\")",
        "",
        "for probe_name, value in probe_named_values.items():",
        "# NOTE: ignore step for now",
        "self.values[probe_name] = value",
        "",
        "def add_metric(",
        "self,",
        "description,",
        "metric,",
        "stages=None,",
        "aggregator=\"mean\",",
        "record_metric_per_sample=True,",
        "elementwise=True,",
        "):",
        "\"\"\"",
        "Add a metric. Example usage:",
        "metrics_meter = MetricsMeter()",
        "metrics_meter.add_metric(\"benign_categorical_accuracy\", \"categorical_accuracy\", \"y\", \"y_pred\")",
        "",
        "metrics_meter.add_metric(\"mean_output\", lambda x: x.mean(), \"y_pred\")",
        "metrics_meter.add_metric(\"l2 perturbation\", metrics.perturbation.l2, \"x\", \"adv_x\")",
        "metrics_meter.add_metric(\"constant\", lambda: 1)",
        "\"\"\"",
        "if description in self.metrics_dict:",
        "raise ValueError(f\"Metric description '{description}' already exists\")",
        "if not isinstance(metric, MetricList):",
        "metric = MetricList(description, function=metric, aggregator=aggregator,)",
        "",
        "if stages is None:",
        "stages = []",
        "elif isinstance(stages, str):",
        "stages = [stages]",
        "else:",
        "stages = list(stages)",
        "stages.append(description)",
        "",
        "self.probes.update(probe_names)",
        "self.metrics_dict[description] = (metric, probe_names)",
        "for stage in stages:",
        "self.stages_mapping[stage] = description",
        "",
        "def measure(self, *names):",
        "\"\"\"",
        "Measure the metrics based on the current values",
        "",
        "names - list of named metrics to measure",
        "if empty, measure all metrics",
        "\"\"\"",
        "",
        "if not names:",
        "descriptions = list(self.metrics_dict)",
        "probes = self.probes",
        "else:",
        "descriptions = []",
        "probes = set()",
        "for name in names:",
        "if name not in self.stages_mapping:",
        "raise ValueError(f\"metric or stage {name} has not been added\")",
        "description = self.stages_mapping[name]",
        "descriptions.append(description)",
        "_, probe_names = self.metrics_dict[description]",
        "probes.update(probe_names)",
        "",
        "for p in probes:",
        "if p not in self.values:",
        "raise ValueError(f\"probe {p} value has not been set\")",
        "",
        "for name in descriptions:",
        "metric, probe_names = self.metrics_dict[name]",
        "metric.update(*(self.values[x] for x in probe_names))",
        "",
        "def finalize(self):",
        "\"\"\"",
        "Finalize all metric computations",
        "\"\"\"",
        "for metric, _ in self.metrics_dict.values():",
        "metric.finalize()",
        "",
        "def results(self):",
        "results = {}",
        "for description, (metric, _) in self.metrics_dict.items():",
        "sub_results = metric.results()",
        "if not results.keys().isdisjoint(sub_results):",
        "log.error(",
        "f\"Overwritting duplicate keys in {list(results)} and {list(sub_results)}\"",
        ")",
        "results.update(sub_results)",
        "return results",
        "",
        "",
        "class MetricList:",
        "\"\"\"",
        "Keeps track of all results from a single metric",
        "\"\"\"",
        "",
        "def __init__(self, name, function=None, aggregator=\"mean\"):",
        "if callable(function):",
        "self.function = function",
        "elif isinstance(function, str):",
        "self.function = metrics.get(function)",
        "elif function is None:",
        "self.function = metrics.get(name)",
        "else:",
        "raise ValueError(f\"function must be callable or None, not {function}\")",
        "",
        "self.name = name",
        "self.elementwise = True",
        "if name == \"word_error_rate\":",
        "self.aggregator = metrics.task.aggregate.total_wer",
        "self.aggregator_name = \"total_wer\"",
        "elif name in (",
        "\"object_detection_AP_per_class\",",
        "\"apricot_patch_targeted_AP_per_class\",",
        "\"dapricot_patch_targeted_AP_per_class\",",
        "):",
        "self.aggregator = metrics.task.aggregate.mean_ap",
        "self.aggregator_name = \"mean_\" + self.name",
        "self.elementwise = False",
        "elif aggregator == \"mean\":",
        "self.aggregator = metrics.task.aggregate.mean",
        "self.aggregator_name = \"mean_\" + self.name",
        "elif not aggregator:",
        "self.aggregator = None",
        "else:",
        "raise ValueError(f\"Aggregator {aggregator} not recognized\")",
        "self._values = []",
        "self._results = {}",
        "",
        "def update(self, *function_args):",
        "if self.elementwise:",
        "self._values.extend(self.function(*function_args))",
        "else:",
        "self._values.extend(zip(*function_args))",
        "",
        "def finalize(self):",
        "r = {}",
        "if self.elementwise:",
        "r[self.name] = list(self._values)",
        "if self.aggregator is not None:",
        "r[self.aggregator_name] = self.aggregator(self._values)",
        "else:",
        "computed_values = self.function(*zip(*self._values))",
        "r[self.name] = computed_values",
        "if self.aggregator is not None:",
        "r[self.aggregator_name] = self.aggregator(computed_values)",
        "self._results = r",
        "",
        "def results(self):",
        "return self._results",
        "",
        "def results_keys(self):",
        "if self.aggregator is None:",
        "return set([self.name, self.aggregator_name])",
        "return set([self.name])"
    ],
    "docstrings": [
        "\"\"\"\nOOP structure for Armory logging\n\nThis roughly follows the python logging (Logger / Handler) framework,\nwith some key differences\n\"\"\"",
        "\"\"\"\n        Measure values, applying preprocessing if a meter is available\n\n        Example: probe.update(lambda x: x.detach().cpu().numpy(), a=layer_3_output)\n\n        named_values can be any object, tuple, dict, etc.\n            To add attributes, you could do:\n                probe.update(data_point=(x_i, is_poisoned))\n        \"\"\"",
        "\"\"\"\n    Ensure probe preprocessing is all done, but otherwise do nothing.\n    \"\"\"",
        "\"\"\"\n    Log all probed values\n    \"\"\"",
        "\"\"\"\n    Provides context for meter and probes\n    \"\"\"",
        "**kwargs,",
        "**kwargs,",
        "**kwargs,",
        "**kwargs,",
        "*probe_names,"
    ],
    "functions": [
        "connect",
        "update",
        "hook",
        "hook_tf",
        "hook_torch",
        "hook_fn",
        "unhook",
        "is_measuring",
        "update",
        "measure",
        "is_measuring",
        "is_measuring",
        "update",
        "set_stage",
        "set_step",
        "get_probe",
        "connect"
    ],
    "classes": [
        "Probe",
        "Meter",
        "NullMeter",
        "LogMeter",
        "Procedure"
    ]
}