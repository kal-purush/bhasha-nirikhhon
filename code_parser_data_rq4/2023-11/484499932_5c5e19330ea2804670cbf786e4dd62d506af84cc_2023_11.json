{
    "identifiers": [
        "gurobipy",
        "gp",
        "numpy",
        "np",
        "gurobipy",
        "GRB",
        "matplotlib",
        "cm",
        "matplotlib",
        "pyplot",
        "plt",
        "sklearn",
        "metrics",
        "sklearn",
        "neural_network",
        "MLPRegressor",
        "sklearn",
        "pipeline",
        "make_pipeline",
        "sklearn",
        "preprocessing",
        "PolynomialFeatures",
        "gurobi_ml",
        "add_predictor_constr",
        "x1",
        "x2",
        "x1",
        "np",
        "exp",
        "x1",
        "x2",
        "x1",
        "x1",
        "x2",
        "np",
        "exp",
        "x1",
        "x2",
        "np",
        "exp",
        "x1",
        "x2",
        "np",
        "meshgrid",
        "np",
        "arange",
        "np",
        "arange",
        "peak2d",
        "x1",
        "x2",
        "plt",
        "subplots",
        "subplot_kw",
        "ax",
        "plot_surface",
        "x1",
        "x2",
        "y",
        "cmap",
        "cm",
        "coolwarm",
        "linewidth",
        "antialiased",
        "fig",
        "colorbar",
        "surf",
        "shrink",
        "aspect",
        "plt",
        "show",
        "np",
        "concatenate",
        "x1",
        "ravel",
        "reshape",
        "x2",
        "ravel",
        "reshape",
        "axis",
        "y",
        "ravel",
        "MLPRegressor",
        "hidden_layer_sizes",
        "layers",
        "activation",
        "make_pipeline",
        "PolynomialFeatures",
        "regression",
        "pipe",
        "fit",
        "X",
        "X",
        "y",
        "y",
        "np",
        "random",
        "random",
        "metrics",
        "r2_score",
        "peak2d",
        "X_test",
        "X_test",
        "pipe",
        "predict",
        "X_test",
        "metrics",
        "max_error",
        "peak2d",
        "X_test",
        "X_test",
        "pipe",
        "predict",
        "X_test",
        "format",
        "r2_score",
        "max_error",
        "plt",
        "subplots",
        "subplot_kw",
        "ax",
        "plot_surface",
        "x1",
        "x2",
        "pipe",
        "predict",
        "X",
        "reshape",
        "x1",
        "shape",
        "cmap",
        "cm",
        "coolwarm",
        "linewidth",
        "antialiased",
        "fig",
        "colorbar",
        "surf",
        "shrink",
        "aspect",
        "plt",
        "show",
        "gp",
        "Model",
        "m",
        "addVars",
        "lb",
        "ub",
        "name",
        "m",
        "addVar",
        "lb",
        "GRB",
        "INFINITY",
        "name",
        "m",
        "setObjective",
        "y_approx",
        "gp",
        "GRB",
        "MINIMIZE",
        "add_predictor_constr",
        "m",
        "pipe",
        "x",
        "y_approx",
        "pred_constr",
        "print_stats",
        "m",
        "Params",
        "m",
        "Params",
        "m",
        "Params",
        "m",
        "optimize",
        "format",
        "np",
        "max",
        "pred_constr",
        "get_error",
        "x",
        "X",
        "x",
        "X",
        "m",
        "ObjVal",
        "peak2d",
        "x",
        "X",
        "x",
        "X",
        "abs",
        "peak2d",
        "x",
        "X",
        "x",
        "X",
        "m",
        "ObjVal"
    ],
    "literals": [
        "r\"\"\"\nSurrogate Models\n================\n\nSome industrial applications require modeling complex processes that can\nresult either in highly nonlinear functions or functions defined by a\nsimulation process. In those contexts, optimization solvers often\nstruggle. The reason may be that relaxations of the nonlinear functions\nare not good enough to make the solver prove an acceptable bound in a\nreasonable amount of time. Another issue may be that the solver is not\nable to represent the functions.\n\nAn approach that has been proposed in the literature is to approximate\nthe problematic nonlinear functions via neural networks with ReLU\nactivation and use MIP technology to solve the constructed approximation\n(see e.g. \\ `Heneao Maravelias\n2011 <https://doi.org/https://doi.org/10.1002/aic.12341>`__\\ ,\n`Schweitdmann et.al. 2022 <https://arxiv.org/abs/2207.12722>`__\\ ). This\nuse of neural networks can be motivated by their ability to provide a\nuniversal approximation (see e.g. \\ `Lu et.al.\n2017 <https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf>`__\\ ).\nThis use of ML models to replace complex processes is often referred to\nas *surrogate models*.\n\nIn the following example, we approximate a nonlinear function via\n``Scikit-learn`` ``MLPRegressor`` and then solve an optimization problem\nthat uses the approximation of the nonlinear function with Gurobi.\n\nThe purpose of this example is solely illustrative and doesn’t relate to\nany particular application.\n\nThe function we approximate is the `2D peaks\nfunction <https://www.mathworks.com/help/matlab/ref/peaks.html#mw_46aeee28-390e-4373-aa47-e4a52447fc85>`__.\n\nThe function is given as\n\n.. math::\n\n    \\begin{aligned} f(x) = & 3 \\cdot (1-x_1)^2 \\cdot \\exp(-x_1^2 - (x_2+1)^2) -\n   \\\\\n            & 10 \\cdot (\\frac{x_1}{5} - x_1^3 - x_2^5) \\cdot \\exp(-x_1^2 - x_2^2) - \\\\\n            & \\frac{1}{3} \\cdot \\exp(-(x_1+1)^2 - x_2^2).\n   \\end{aligned}\n\nIn this example, we want to find the minimum of :math:`f` over the\ninterval :math:`[-2, 2]^2`:\n\n.. math::  y = \\min \\{f(x) : x \\in [-2,2]^2\\}.\n\nThe `global minimum of this problem can be found\nnumerically <https://www.math.uwaterloo.ca/~hwolkowi/henry/reports/talks.d/t09talks.d/09waterloomatlab.d/optimTipsWebinar/html/optimTipsTricksWalkthrough.html#18>`__\nto have value :math:`-6.55113` at the point :math:`(0.2283, -1.6256)`.\n\nHere to find this minimum of :math:`f`, we approximate :math:`f(x)`\nthrough a neural network function :math:`g(x)` to obtain a MIP and solve\n\n.. math::  \\hat y = \\min \\{g(x) : x \\in [-2,2]^2\\} \\approx y.\n\nFirst import the necessary packages. Before applying the neural network,\nwe do a preprocessing to extract polynomial features of degree 2.\nHopefully this will help us to approximate the smooth function. Besides,\n``gurobipy``, ``numpy`` and the appropriate ``sklearn`` objects, we also\nuse ``matplotlib`` to plot the function, and its approximation.\n\n\"\"\"",
        "\"projection\"",
        "\"3d\"",
        "\"relu\"",
        "\"R2 error {}, maximal error {}\"",
        "\"projection\"",
        "\"3d\"",
        "\"x\"",
        "\"y\"",
        "\"Maximum error in approximating the regression {:.6}\"",
        "f\"solution point of the approximated problem ({x[0].X:.4}, {x[1].X:.4}), \"",
        "f\"objective value {m.ObjVal}.\"",
        "f\"Function value at the solution point {peak2d(x[0].X, x[1].X)} error {abs(peak2d(x[0].X, x[1].X) - m.ObjVal)}.\""
    ],
    "variables": [
        "x1",
        "x2",
        "y",
        "fig",
        "ax",
        "surf",
        "X",
        "y",
        "layers",
        "regression",
        "pipe",
        "X_test",
        "r2_score",
        "max_error",
        "fig",
        "ax",
        "surf",
        "m",
        "x",
        "y_approx",
        "pred_constr",
        "TimeLimit",
        "MIPGap",
        "NonConvex"
    ],
    "comments": [
        "",
        "Define the nonlinear function of interest",
        "-----------------------------------------",
        "",
        "We define the 2D peak function as a python function.",
        "",
        "",
        "To train the neural network, we make a uniform sample of the domain of",
        "the function in the region of interest using ``numpy``\\ ’s ``arrange``",
        "function.",
        "",
        "We then plot the function with ``matplotlib``.",
        "",
        "Plot the surface.",
        "Add a color bar which maps values to colors.",
        "",
        "Approximate the function",
        "------------------------",
        "",
        "To fit a model, we need to reshape our data. We concatenate the values",
        "of ``x1`` and ``x2`` in an array ``X`` and make ``y`` one dimensional.",
        "",
        "",
        "To approximate the function, we use a ``Pipeline`` with polynomial",
        "features and a neural-network regressor. We do a relatively small",
        "neural-network.",
        "",
        "Run our regression",
        "",
        "To test the accuracy of the approximation, we take a random sample of",
        "points, and we print the :math:`R^2` value and the maximal error.",
        "",
        "",
        "While the :math:`R^2` value is good, the maximal error is quite high.",
        "For the purpose of this example we still deem it acceptable. We plot the",
        "function.",
        "",
        "Plot the surface.",
        "Add a color bar which maps values to colors.",
        "",
        "Visually, the approximation looks close enough to the original function.",
        "",
        "Build and Solve the Optimization Model",
        "--------------------------------------",
        "",
        "We now turn to the optimization model. For this model we want to find",
        "the minimal value of ``y_approx`` which is the approximation given by",
        "our pipeline on the interval.",
        "",
        "Note that in this simple example, we don’t use matrix variables but",
        "regular Gurobi variables instead.",
        "",
        "add \"surrogate constraint\"",
        "",
        "Now call ``optimize``. Since we use polynomial features the resulting",
        "model is a non-convex quadratic problem. In Gurobi, we need to set the",
        "parameter ``NonConvex`` to 2 to be able to solve it.",
        "",
        "",
        "After solving the model, we check the error in the estimate of the",
        "Gurobi solution.",
        "",
        "",
        "Finally, we look at the solution and the objective value found.",
        "",
        "",
        "The difference between the function and the approximation at the",
        "computed solution point is noticeable, but the point we found is",
        "reasonably close to the actual global minimum. Depending on the use case",
        "this might be deemed acceptable. Of course, training a larger network",
        "should result in a better approximation.",
        "",
        "",
        "Copyright © 2023 Gurobi Optimization, LLC",
        ""
    ],
    "docstrings": [],
    "functions": [
        "peak2d"
    ],
    "classes": []
}