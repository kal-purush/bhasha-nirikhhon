{
    "identifiers": [
        "lrtdp",
        "LRTDP",
        "get_successor_states",
        "is_goal",
        "LRTDP",
        "s",
        "_get_applicable_actions",
        "s",
        "state_values",
        "max",
        "min_Q",
        "s",
        "a",
        "a",
        "app_acts",
        "s",
        "a",
        "get_successor_states",
        "s",
        "actions",
        "sucessors",
        "a",
        "get_possible_resulting_states",
        "s",
        "i",
        "ns",
        "future_states",
        "is_goal",
        "ns",
        "parser",
        "positive_goals",
        "parser",
        "negative_goals",
        "min",
        "min_q",
        "probs",
        "i",
        "reward",
        "discount_rate",
        "state_values",
        "ns",
        "min_q",
        "ippddl_parser",
        "parser",
        "Parser",
        "heuristics",
        "lm_cut",
        "LMCutHeuristic",
        "Parser",
        "Parser",
        "parser",
        "scan_tokens",
        "domain_file",
        "parser",
        "scan_tokens",
        "problem_file",
        "parser",
        "parse_domain",
        "domain_file",
        "parser",
        "parse_problem",
        "problem_file",
        "LMCutHeuristic",
        "parser",
        "STLRTDP",
        "parser",
        "lmcut",
        "lrtdp",
        "execute",
        "parser",
        "state",
        "s",
        "_",
        "is_goal",
        "s",
        "parser",
        "positive_goals",
        "parser",
        "negative_goals",
        "lrtdp",
        "policy",
        "s",
        "act",
        "apply",
        "s",
        "act",
        "name",
        "act",
        "parameters",
        "s"
    ],
    "literals": [
        "\"__main__\"",
        "'problems/blocksworld/domain.pddl'",
        "'problems/blocksworld/5blocks.pddl'",
        "\"GOAL REACHED\"",
        "f'|-> {act.name}[{act.parameters}] -> {s}'"
    ],
    "variables": [
        "app_acts",
        "s",
        "sucessors",
        "_",
        "min_q",
        "future_states",
        "probs",
        "reward",
        "reward",
        "min_q",
        "domain_file",
        "problem_file",
        "parser",
        "lmcut",
        "lrtdp",
        "s",
        "act",
        "s"
    ],
    "comments": [
        "If there are no successor states (aka the state is absorbing) we consider the state solved."
    ],
    "docstrings": [
        "'''\n        Adapts the traditional Bellman Update method to follow the minmax\n        criteria, where we assume that all actions will result in the future\n        state that offers the minimum reward.\n\n        As seen in page 54 of\n        https://teses.usp.br/teses/disponiveis/45/45134/tde-15022010-161012/publico/dissertacao.pdf\n        '''"
    ],
    "functions": [
        "_bellman_update",
        "min_Q"
    ],
    "classes": [
        "STLRTDP"
    ]
}