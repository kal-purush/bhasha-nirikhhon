{
    "identifiers": [
        "stable_baselines3",
        "PPO",
        "stable_baselines3",
        "common",
        "evaluation",
        "evaluate_policy",
        "stable_baselines3",
        "common",
        "monitor",
        "Monitor",
        "stable_baselines3",
        "common",
        "vec_env",
        "DummyVecEnv",
        "stable_baselines3",
        "common",
        "env_checker",
        "check_env",
        "stable_baselines3",
        "common",
        "callbacks",
        "EvalCallback",
        "dataclasses",
        "dataclass",
        "tyro",
        "typing",
        "Tuple",
        "wandb",
        "wandb",
        "integration",
        "sb3",
        "WandbCallback",
        "yaml",
        "argparse",
        "gymnasium",
        "gym",
        "gymnasium",
        "envs",
        "registration",
        "register",
        "numpy",
        "np",
        "os",
        "os",
        "path",
        "dirname",
        "os",
        "path",
        "dirname",
        "os",
        "path",
        "abspath",
        "os",
        "sys",
        "path",
        "insert",
        "parent_dir",
        "common",
        "logger",
        "get_logger",
        "typing",
        "Literal",
        "datetime",
        "datetime",
        "get_logger",
        "dataclass",
        "os",
        "path",
        "basename",
        "len",
        "tyro",
        "cli",
        "Args",
        "args",
        "exp_name",
        "experiment_name",
        "args",
        "reward_fun",
        "args",
        "wandb_project_name",
        "args",
        "total_timesteps",
        "args",
        "env_name",
        "args",
        "batch_size",
        "args",
        "learning_rate",
        "args",
        "reward_fun",
        "args",
        "history_len",
        "args",
        "action_scale",
        "args",
        "device",
        "args",
        "max_steps",
        "args",
        "a",
        "args",
        "b",
        "args",
        "c",
        "args",
        "mode",
        "args",
        "bw",
        "args",
        "latency",
        "args",
        "queue_size",
        "args",
        "loss",
        "register",
        "id",
        "entry_point",
        "register",
        "id",
        "entry_point",
        "gym",
        "make",
        "config",
        "reward_fun",
        "config",
        "history_len",
        "args",
        "history_len",
        "ACTION_SCALE",
        "args",
        "action_scale",
        "max_steps",
        "args",
        "max_steps",
        "reward_coefficients",
        "args",
        "a",
        "args",
        "b",
        "args",
        "c",
        "mode",
        "args",
        "mode",
        "network_values",
        "args",
        "bw",
        "args",
        "latency",
        "args",
        "queue_size",
        "args",
        "loss",
        "Monitor",
        "env",
        "env",
        "DummyVecEnv",
        "make_env",
        "make_env",
        "check_env",
        "env",
        "args",
        "tracking",
        "wandb",
        "login",
        "wandb",
        "init",
        "project",
        "args",
        "wandb_project_name",
        "config",
        "config",
        "sync_tensorboard",
        "save_code",
        "PPO",
        "config",
        "env",
        "batch_size",
        "config",
        "verbose",
        "tensorboard_log",
        "device",
        "args",
        "device",
        "config",
        "evaluate_policy",
        "model",
        "env",
        "n_eval_episodes",
        "N_EVAL_EPISODES",
        "log",
        "debug",
        "experiment_name",
        "mean_reward",
        "std_reward",
        "N_EVAL_EPISODES",
        "os",
        "path",
        "exists",
        "models_dir",
        "os",
        "makedirs",
        "models_dir",
        "args",
        "tracking",
        "models_dir",
        "experiment_name",
        "args",
        "reward_fun",
        "TIMESTEPS",
        "run",
        "id",
        "models_dir",
        "experiment_name",
        "args",
        "reward_fun",
        "TIMESTEPS",
        "datetime",
        "now",
        "strftime",
        "model_path",
        "args",
        "use_pretrained",
        "PPO",
        "load",
        "model_path",
        "log",
        "debug",
        "model_path",
        "log",
        "debug",
        "model_path",
        "args",
        "use_pretrained",
        "log",
        "debug",
        "config",
        "args",
        "tracking",
        "model",
        "learn",
        "total_timesteps",
        "config",
        "tb_log_name",
        "experiment_name",
        "progress_bar",
        "callback",
        "WandbCallback",
        "gradient_save_freq",
        "model_save_path",
        "run",
        "id",
        "verbose",
        "model",
        "learn",
        "total_timesteps",
        "config",
        "tb_log_name",
        "experiment_name",
        "progress_bar",
        "model",
        "save",
        "model_path",
        "log",
        "debug",
        "model_path",
        "PPO",
        "load",
        "model_path",
        "evaluate_policy",
        "trained_model",
        "env",
        "n_eval_episodes",
        "N_EVAL_EPISODES",
        "log",
        "debug",
        "experiment_name",
        "mean_reward",
        "std_reward",
        "N_EVAL_EPISODES",
        "gym",
        "make",
        "args",
        "env_name",
        "reward_fun",
        "args",
        "reward_fun",
        "reward_coefficients",
        "args",
        "a",
        "args",
        "b",
        "args",
        "c",
        "Monitor",
        "env",
        "env",
        "reset",
        "np",
        "zeros",
        "LEN",
        "dtype",
        "np",
        "float16",
        "step",
        "LEN",
        "trained_model",
        "predict",
        "obs",
        "deterministic",
        "env",
        "step",
        "action",
        "action",
        "item",
        "dones",
        "env",
        "reset",
        "log",
        "debug",
        "np",
        "min",
        "actions",
        "np",
        "max",
        "actions",
        "log",
        "debug",
        "np",
        "mean",
        "actions",
        "np",
        "std",
        "actions",
        "env",
        "close",
        "args",
        "tracking",
        "run",
        "finish",
        "log",
        "debug"
    ],
    "literals": [
        "'PccNs-v0'",
        "\".py\"",
        "\"test_model\"",
        "\"PccNs-v1\"",
        "\"default\"",
        "\"auto\"",
        "\"training\"",
        "\"exp_name\"",
        "\"r=\"",
        "\"project\"",
        "\"policy_type\"",
        "\"MlpPolicy\"",
        "\"total_timesteps\"",
        "\"env_name\"",
        "\"batch_size\"",
        "\"learning_rate\"",
        "\"reward_fun\"",
        "\"history-len\"",
        "\"action_scale\"",
        "\"device\"",
        "\"max_steps\"",
        "\"a\"",
        "\"b\"",
        "\"c\"",
        "\"mode\"",
        "\"bw\"",
        "\"latency\"",
        "\"queue_size\"",
        "\"loss\"",
        "\"PccNs-v0\"",
        "'network_sim:SimulatedNetworkEnv'",
        "\"PccNs-v1\"",
        "'network_sim_v1:SimulatedNetworkEnv'",
        "\"env_name\"",
        "\"reward_fun\"",
        "\"policy_type\"",
        "\"batch_size\"",
        "\"logs/PCCNs_SB3\"",
        "\"total_timesteps\"",
        "f\"Untrained {experiment_name} -> mean_reward:{mean_reward:.2f} +/- {std_reward:.2f} over {N_EVAL_EPISODES} episodes\"",
        "\"models/\"",
        "\"_\"",
        "\"_\"",
        "\"_\"",
        "\"_\"",
        "\"_\"",
        "\"%Y%m%d_%H%M%S\"",
        "f\"Model path: {model_path} \"",
        "f\"Loaded model from {model_path}\"",
        "f\"Failed to load model from {model_path}\"",
        "f\"Starting training for {config['total_timesteps']} timesteps\"",
        "'total_timesteps'",
        "\"total_timesteps\"",
        "f\"models/{run.id}\"",
        "\"total_timesteps\"",
        "f\"Saved model to {model_path}\"",
        "f\"Trained {experiment_name} -> mean_reward:{mean_reward:.2f} +/- {std_reward:.2f} over {N_EVAL_EPISODES} episodes\"",
        "f\"Actions: Min: {np.min(actions)}, Max: {np.max(actions)}\"",
        "f\"Mean: {np.mean(actions)}, Std: {np.std(actions)}\"",
        "\"Experiment finished.\""
    ],
    "variables": [
        "parent_dir",
        "log",
        "ENV_NAME",
        "exp_name",
        "wandb_project_name",
        "tracking",
        "env_name",
        "total_timesteps",
        "learning_rate",
        "num_envs",
        "batch_size",
        "learning_rate",
        "reward_fun",
        "history_len",
        "action_scale",
        "device",
        "max_steps",
        "a",
        "b",
        "c",
        "use_pretrained",
        "mode",
        "bw",
        "latency",
        "queue_size",
        "loss",
        "args",
        "experiment_name",
        "config",
        "env",
        "env",
        "env",
        "env",
        "run",
        "model",
        "TIMESTEPS",
        "N_EVAL_EPISODES",
        "mean_reward",
        "std_reward",
        "models_dir",
        "model_path",
        "model_path",
        "trained_model",
        "args",
        "use_pretrained",
        "trained_model",
        "mean_reward",
        "std_reward",
        "env",
        "env",
        "obs",
        "_",
        "LEN",
        "actions",
        "action",
        "_states",
        "obs",
        "rewards",
        "dones",
        "truncated",
        "info",
        "actions",
        "step",
        "obs",
        "_"
    ],
    "comments": [
        "Use Tyro for CLI arguments",
        "Algorithm specific arguments",
        "TODO: restrict options reward_fun: Literal[\"default\", \"alternative1\", \"alternative2\"] = \"default\"",
        "\"\"\"Reward function to use. Must be one of: 'default', 'alternative1', 'alternative2'\"\"\"",
        "reward_coefficients: tuple[float, float, float] = (10, -1000, -2000)",
        "\"\"\"Coefficients for reward calculation (throughput, latency, loss)\"\"\"",
        "default config.",
        "\"reward_coefficients\": args.reward_coefficients",
        "env = gym.make(config[\"env_name\"])",
        "Passes.",
        "Initialize wandb",
        "auto-upload sb3's tensorboard metrics",
        "monitor_gym=True,  # auto-upload the videos of agents playing the game",
        "optional",
        "name=\"default_batch\"",
        "Use PPO to learn a policy & evaluate it",
        "TODO: Use absolute sending rate instead of delta scaling",
        "Evaluate the untrained agent",
        "Add path if it doesn't exist",
        "TODO: Add a check to see if the model already exists and if so, add a suffix to the name",
        "TODO: Change model path format to model_{TIMESTEPS}_{args.reward_fun}-a{args.reward_coefficients[0]} E.g.: model_4000000_custom_a5",
        "print(f\"Starting training for {args.timesteps} timesteps\")",
        "callback=EvalCallback(",
        "env,",
        "n_eval_episodes=N_EVAL_EPISODES,",
        "eval_freq=10000,",
        "# deterministic=True,",
        "best_model_save_path=f\"models/{experiment_name}_best\"",
        ")",
        "Save the model.",
        "# Load the model.",
        "trained_model = PPO.load(model_path)",
        "# Check if the best model was saved correctly",
        "best_model = PPO.load(f\"{model_path}_best/best_model.zip\")",
        "Evaluate the trained agent",
        "mean_reward, std_reward = evaluate_policy(best_model, env, n_eval_episodes=N_EVAL_EPISODES)",
        "log.debug(f\"Best {experiment_name} -> mean_reward:{mean_reward:.2f} +/- {std_reward:.2f} over {N_EVAL_EPISODES} episodes\")",
        "A quick eval of model to see range of values produced.",
        "Examine the range of outputs from the model to see if they are reasonable",
        "10 episodes",
        "",
        "Close the environment"
    ],
    "docstrings": [
        "\"\"\"the name of this experiment\"\"\"",
        "\"\"\"the name of the wandb project\"\"\"",
        "\"\"\"whether to use wandb for tracking\"\"\"",
        "\"\"\"the id of the environment\"\"\"",
        "\"\"\"total timesteps of the experiments\"\"\"",
        "\"\"\"the learning rate of the optimizer\"\"\"",
        "\"\"\"the number of parallel game environments\"\"\"",
        "\"\"\"size of the batch\"\"\"",
        "\"\"\"the learning rate of the optimizer\"\"\"",
        "\"\"\"Reward function to use\"\"\"",
        "\"\"\"How many historical steps to take\"\"\"",
        "\"\"\"The action space of the environment [+/-(action_space)]\"\"\"",
        "\"\"\"The device to use for training\"\"\"",
        "\"\"\"The maximum number of steps to take in the environment\"\"\"",
        "\"\"\"The throughput reward coefficient\"\"\"",
        "\"\"\"The latency reward coefficient\"\"\"",
        "\"\"\"The loss reward coefficient\"\"\"",
        "\"\"\"Whether to use a pretrained model\"\"\"",
        "\"\"\"The mode to run the model in (training, testing)\"\"\"",
        "\"\"\"The fixed bandwidth of the network link in packets per second (100-500)\"\"\"",
        "\"\"\"The fixed delay of the network link in milliseconds (0.05, 0.5)\"\"\"",
        "\"\"\"The natural log of fixed queue size in packets of the network link (0-8)\"\"\"",
        "\"\"\"The fixed loss rate of the network link (0.0, 0.05)\"\"\""
    ],
    "functions": [
        "make_env"
    ],
    "classes": [
        "Args"
    ]
}