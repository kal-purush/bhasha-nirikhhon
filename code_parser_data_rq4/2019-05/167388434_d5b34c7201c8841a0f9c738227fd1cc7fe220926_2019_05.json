{
    "identifiers": [
        "math",
        "random",
        "itertools",
        "accumulate",
        "collections",
        "defaultdict",
        "numpy",
        "np",
        "scipy",
        "special",
        "scipy",
        "ndimage",
        "n_components",
        "number_of_documents",
        "alpha_theta",
        "alpha_beta",
        "tau",
        "kappa",
        "vocab_prune_interval",
        "number_of_samples",
        "ranking_smooth_factor",
        "burn_in_sweeps",
        "maximum_size_vocabulary",
        "n_components",
        "number_of_documents",
        "alpha_theta",
        "alpha_beta",
        "tau",
        "kappa",
        "vocab_prune_interval",
        "number_of_samples",
        "ranking_smooth_factor",
        "burn_in_sweeps",
        "maximum_size_vocabulary",
        "defaultdict",
        "np",
        "ones",
        "defaultdict",
        "np",
        "array",
        "alpha_beta",
        "topic",
        "n_components",
        "nu_1",
        "np",
        "ones",
        "nu_2",
        "np",
        "array",
        "alpha_beta",
        "counter",
        "_extract_word",
        "_update_indexes",
        "word_list",
        "word_list",
        "word_to_index",
        "word",
        "word",
        "word_list",
        "_get_document_topics_representation",
        "words_ids_list",
        "_update_documents_topics_representation",
        "sufficient_statistics",
        "sufficient_statistics",
        "counter",
        "vocab_prune_interval",
        "_prune_word_indexes",
        "batch_document_topic_distribution",
        "classmethod",
        "cls",
        "split",
        "word_list",
        "word",
        "word_list",
        "word",
        "word_to_index",
        "keys",
        "len",
        "word_to_index",
        "word_to_index",
        "new_index",
        "index_to_word",
        "word",
        "truncation_size_prime",
        "classmethod",
        "cls",
        "n_components",
        "nu_1",
        "nu_2",
        "topic",
        "n_components",
        "special",
        "psi",
        "nu_1",
        "topic",
        "special",
        "psi",
        "nu_2",
        "topic",
        "special",
        "psi",
        "nu_1",
        "topic",
        "nu_2",
        "topic",
        "np",
        "cumsum",
        "psi_nu_2",
        "psi_nu_1_nu_2",
        "axis",
        "np",
        "exp",
        "psi_nu_1_nu_2_minus_psi_nu_2",
        "ndimage",
        "interpolation",
        "shift",
        "input",
        "psi_nu_1_nu_2_minus_psi_nu_2",
        "shift",
        "cval",
        "np",
        "exp",
        "psi_nu_1",
        "psi_nu_1_nu_2",
        "psi_nu_1_nu_2_minus_psi_nu_2",
        "exp_weights",
        "exp_oov_weights",
        "sufficient_statistics",
        "k",
        "n_components",
        "ndimage",
        "interpolation",
        "shift",
        "input",
        "sufficient_statistics",
        "k",
        "shift",
        "cval",
        "np",
        "flip",
        "reverse_cumulated_phi",
        "k",
        "np",
        "cumsum",
        "reverse_cumulated_phi",
        "k",
        "np",
        "flip",
        "reverse_cumulated_phi",
        "k",
        "tau",
        "counter",
        "kappa",
        "k",
        "n_components",
        "truncation_size",
        "truncation_size_prime",
        "truncation_size_prime",
        "truncation_size",
        "nu_1",
        "np",
        "append",
        "nu_1",
        "k",
        "np",
        "ones",
        "difference_truncation",
        "nu_2",
        "np",
        "append",
        "nu_2",
        "k",
        "np",
        "ones",
        "difference_truncation",
        "nu_1",
        "k",
        "epsilon",
        "number_of_documents",
        "np",
        "array",
        "sufficient_statistics",
        "k",
        "nu_1",
        "k",
        "nu_2",
        "k",
        "epsilon",
        "alpha_beta",
        "number_of_documents",
        "np",
        "array",
        "reverse_cumulated_phi",
        "k",
        "nu_2",
        "k",
        "truncation_size_prime",
        "words_ids_list",
        "defaultdict",
        "np",
        "zeros",
        "truncation_size_prime",
        "_get_document_latent_variables",
        "n_components",
        "n_components",
        "nu_1",
        "nu_1",
        "nu_2",
        "nu_2",
        "len",
        "words_ids_list",
        "np",
        "random",
        "random",
        "n_components",
        "size_vocab",
        "phi",
        "np",
        "sum",
        "phi",
        "axis",
        "np",
        "sum",
        "phi",
        "axis",
        "sample_index",
        "number_of_samples",
        "word_index",
        "size_vocab",
        "phi_sum",
        "phi",
        "word_index",
        "phi_sum",
        "clip",
        "min",
        "phi_sum",
        "alpha_theta",
        "k",
        "n_components",
        "words_ids_list",
        "word_index",
        "truncation_size",
        "temp_phi",
        "k",
        "exp_oov_weights",
        "k",
        "temp_phi",
        "k",
        "exp_weights",
        "k",
        "words_ids_list",
        "word_index",
        "temp_phi",
        "temp_phi",
        "sum",
        "np",
        "random",
        "multinomial",
        "temp_phi",
        "temp_phi",
        "phi_sum",
        "temp_phi",
        "sample_index",
        "burn_in_sweeps",
        "k",
        "n_components",
        "words_ids_list",
        "word_index",
        "sufficient_statistics",
        "k",
        "index",
        "temp_phi",
        "k",
        "alpha_theta",
        "phi_sum",
        "k",
        "n_components",
        "sufficient_statistics",
        "k",
        "number_of_samples",
        "burn_in_sweeps",
        "sufficient_statistics",
        "document_topic_distribution",
        "nu_1",
        "shape",
        "maximum_size_vocabulary",
        "topic",
        "n_components",
        "nu_1",
        "nu_1",
        "topic",
        "maximum_size_vocabulary",
        "nu_2",
        "nu_2",
        "topic",
        "maximum_size_vocabulary",
        "index",
        "maximum_size_vocabulary",
        "index_to_word",
        "index",
        "index",
        "word",
        "new_word_to_index",
        "new_index_to_word",
        "nu_1",
        "shape",
        "truncation_size"
    ],
    "literals": [
        "'OnlineLda'",
        "' '"
    ],
    "variables": [
        "__all__",
        "n_components",
        "number_of_documents",
        "alpha_theta",
        "alpha_beta",
        "tau",
        "kappa",
        "vocab_prune_interval",
        "number_of_samples",
        "ranking_smooth_factor",
        "burn_in_sweeps",
        "maximum_size_vocabulary",
        "counter",
        "truncation_size_prime",
        "truncation_size",
        "word_to_index",
        "index_to_word",
        "nu_1",
        "nu_2",
        "topic",
        "topic",
        "word_list",
        "words_ids_list",
        "sufficient_statistics",
        "batch_document_topic_distribution",
        "new_index",
        "word",
        "new_index",
        "exp_weights",
        "exp_oov_weights",
        "psi_nu_1",
        "psi_nu_2",
        "psi_nu_1_nu_2",
        "psi_nu_1_nu_2_minus_psi_nu_2",
        "exp_oov_weights",
        "topic",
        "psi_nu_1_nu_2_minus_psi_nu_2",
        "exp_weights",
        "topic",
        "reverse_cumulated_phi",
        "reverse_cumulated_phi",
        "k",
        "reverse_cumulated_phi",
        "k",
        "reverse_cumulated_phi",
        "k",
        "reverse_cumulated_phi",
        "k",
        "epsilon",
        "difference_truncation",
        "k",
        "k",
        "truncation_size",
        "sufficient_statistics",
        "exp_weights",
        "exp_oov_weights",
        "size_vocab",
        "phi",
        "phi",
        "phi_sum",
        "phi_sum",
        "temp_phi",
        "temp_phi",
        "phi",
        "word_index",
        "index",
        "document_topic_distribution",
        "topic",
        "topic",
        "new_word_to_index",
        "new_index_to_word",
        "word",
        "new_word_to_index",
        "word",
        "new_index_to_word",
        "index",
        "word_to_index",
        "index_to_word",
        "truncation_size",
        "truncation_size_prime"
    ],
    "comments": [
        "Python",
        "Third-parties:",
        "Updates number of documents:",
        "Extracts words of the document as a list of words:",
        "Update words indexes:",
        "Replace the words by their index:",
        "Sample empirical topic assignment:",
        "Oline variational inference",
        "Epsilon will be between 0 and 1.",
        "Epsilon value says how much to weight the information we got from this document.",
        "Variational Approximation",
        "Normalize document topic distribution before applying multinomial distribution:",
        "Sample a topic based a given probability distribution:",
        "Updates words latent variables",
        "Updates words indexes"
    ],
    "docstrings": [
        "\"\"\"Online Latent Dirichlet Allocation with Infinite Vocabulary.\n\n    Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document\n    collections.\n\n    Args:\n        n_components (int): Number of topics of the latent Drichlet allocation.\n        number_of_documents (int): Estimated number of documents.\n        alpha_theta (float): Hyper-parameter for Dirichlet distribution of topics,\n            1.0/n_components in the paper.\n        alpha_beta (float): Hyper-parameter for Dirichlet process of distribution over words 1e3\n            in the paper.\n        tau (float): The learning inertia tau prevents premature convergence.\n        kappa (float): The learning rate kappa controls how quickly new parameters estimates replace\n            the old ones. kappa ∈ (0.5, 1] is required for convergence.\n        vocab_prune_interval (int): Interval to refresh the words topics distribution.\n        number_of_samples (int): Number of iteration to computes documents topics distribution.\n        burn_in_sweeps (int): Number of iteration necessaries while analyzing a document\n             before updating document topics distribution.\n        maximum_size_vocabulary (int): Maximum size of the vocabulary stored.\n\n    Attributes:\n        n_components (int): Number of topics of the latent Drichlet allocation.\n        number_of_documents (int): Estimated number of documents.\n        alpha_theta (float): Hyper-parameter for Dirichlet distribution of topics,\n            1.0/n_components in the paper.\n        alpha_beta (float): Hyper-parameter for Dirichlet process of distribution over words 1e3\n            in the paper.\n        tau (float): The learning inertia tau prevents premature convergence.\n        kappa (float): The learning rate kappa controls how quickly new parameters estimates replace\n            the old ones. kappa ∈ (0.5, 1] is required for convergence.\n        vocab_prune_interval (int): Interval to refresh the words topics distribution.\n        number_of_samples (int): Number of iteration to computes documents topics distribution.\n        burn_in_sweeps (int): Number of iteration necessaries while analyzing a document\n             before updating document topics distribution.\n        maximum_size_vocabulary (int): Maximum size of the vocabulary stored.\n        counter (int): Number of documents that updated the latent Dirichlet allocation.\n        truncation_size_prime (int): Number of distincts words stored in the vocabulary. Updated\n            before processing a document.\n        truncation_size (int) : Number of distincts words stored in the vocabulary. Updated after\n            processing a document.\n        word_to_index (dict): Words as keys and indexes as values.\n        index_to_word (dict): Indexes as keys and words as values.\n        nu_1 (dict): Weights of the words. Component of the variational inference.\n        nu_2 (dict): Weights of the words. Component of the variational inference.\n\n\n        Example:\n                ::\n                    >>> from creme import decomposition\n                    >>> import numpy as np\n\n                    >>> np.random.seed(42)\n\n                    >>> X = [\n                    ...    'weather cold',\n                    ...    'weather hot dry',\n                    ...    'weather cold rainny',\n                    ...    'weather hot',\n                    ...    'weather cold humid',\n                    ... ]\n\n                    >>> online_lda = decomposition.OnlineLda(n_components=2, number_of_documents=60)\n                    >>> for x in X:\n                    ...     print(online_lda.update(x))\n                    [0.5 2.5]\n                    [3.5 0.5]\n                    [0.5 3.5]\n                    [1.5 1.5]\n                    [2.5 1.5]\n\n    References:\n\n        1. Jordan Boyd-Graber, Ke Zhai, Online Latent Dirichlet Allocation with Infinite Vocabulary.\n        http://proceedings.mlr.press/v28/zhai13.pdf\n\n        2. Creme's Online LDA reproduces exactly the same results of the original one:\n        https://github.com/kzhai/PyInfVoc.\n    \"\"\"",
        "\"\"\"\n        Updates running latent Dirichlet allocation. Splits the words of the document into a list.\n        Updates the word indexes. Assign topics to the document. Runs the online variational\n        inference.\n\n        Args:\n            document (str): Current document to reduce dimensionally.\n\n        Returns\n            list: Distribution of the topics for the input document.\n        \"\"\"",
        "\"\"\"\n        Adds the words of the document to the index if they are not part of the current vocabulary.\n        Updates of the number of distinct words seen.\n\n        Args:\n            word_list (list): Content of the document as a list of words.\n\n        Returns:\n            None\n        \"\"\"",
        "\"\"\"\n        Calculates the weighting of the vocabulary according to the distribution of words in the\n        previous vocabulary. documents. The Psi function is the logarithmic derivative of the gamma\n        function.\n\n        Args:\n            n_components (int): Number of topics\n            nu_1 (dict): Weights of the words of the vocabulary.\n            nu_2 (dict): Weights of the words of the vocabulary.\n\n        Returns:\n            Tuple[dict, dict]: Weights of the words of the current vocabulary.\n        \"\"\"",
        "\"\"\"\n        Learns documents and word representations. Calculates the variational approximation. Update\n        of latent variables for words.\n\n        Args:\n            sufficient_statistics (defaultdict): Weights associated to the words.\n\n        Returns:\n            None\n        \"\"\"",
        "\"\"\"\n        Extract latent variables from the document and words.\n\n        Args:\n            words_ids_list (list): Ids of the words of the input document.\n\n        Returns:\n            Tuple[dict, dict]: Computed statistics over the words. Document reprensetation across\n            topics.\n        \"\"\"",
        "\"\"\"\n        Prunes word indexes and latent word variables when the vocabulary size is larger than\n        the maximum allowed size of the vocabulary.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\""
    ],
    "functions": [
        "update",
        "_extract_word",
        "_update_indexes",
        "_get_document_latent_variables",
        "_update_documents_topics_representation",
        "_get_document_topics_representation",
        "_prune_word_indexes"
    ],
    "classes": [
        "OnlineLda"
    ]
}