{
    "identifiers": [
        "tensorflow",
        "tf",
        "tf",
        "enable_eager_execution",
        "os",
        "os",
        "environ",
        "os",
        "environ",
        "data",
        "importlib",
        "importlib",
        "reload",
        "data",
        "data",
        "download",
        "data",
        "prune_dataset",
        "annotation_file",
        "path",
        "PATH",
        "num_examples",
        "pickle",
        "pickle",
        "dump",
        "all_captions",
        "train_captions",
        "open",
        "pickle",
        "dump",
        "img_name_vector",
        "all_img_name_vector",
        "open",
        "irenet",
        "importlib",
        "importlib",
        "reload",
        "irenet",
        "model",
        "image_decoder",
        "summary",
        "irenet",
        "IreNet",
        "embedding_dim",
        "max_vocab_size",
        "pickle",
        "pickle",
        "load",
        "open",
        "data",
        "cache_img_features",
        "model",
        "inception",
        "img_name_vector",
        "batch_size",
        "train_test_split",
        "img_name_vector",
        "cap_vector",
        "test_size",
        "random_state",
        "data",
        "get_tf_data",
        "img_name_vector",
        "train_captions",
        "model",
        "doc2vec",
        "batch_size",
        "buffer_size",
        "parallel_workers",
        "top_k",
        "irenet",
        "train",
        "model",
        "dataset",
        "tokenizer",
        "batch_size",
        "epochs",
        "tf",
        "train",
        "AdamOptimizer",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "image_encoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "img_encoder_checkpoint",
        "save",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "image_decoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "img_decoder_checkpoint",
        "save",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "text_encoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "txt_encoder_checkpoint",
        "save",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "text_decoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "txt_decoder_checkpoint",
        "save",
        "numpy",
        "np",
        "model",
        "tokenizer",
        "image",
        "model",
        "text_decoder",
        "reset_state",
        "batch_size",
        "tf",
        "expand_dims",
        "data",
        "load_image",
        "image",
        "model",
        "inception",
        "temp_input",
        "tf",
        "reshape",
        "img_tensor_val",
        "img_tensor_val",
        "shape",
        "img_tensor_val",
        "shape",
        "model",
        "image_encoder",
        "img_tensor_val",
        "model",
        "image_decoder",
        "features",
        "tf",
        "expand_dims",
        "tokenizer",
        "word_index",
        "i",
        "model",
        "text_decoder",
        "dec_input",
        "features",
        "hidden",
        "tf",
        "argmax",
        "predictions",
        "numpy",
        "result",
        "append",
        "tokenizer",
        "index_word",
        "predicted_id",
        "tokenizer",
        "index_word",
        "predicted_id",
        "result",
        "recon",
        "tf",
        "expand_dims",
        "predicted_id",
        "result",
        "recon",
        "model",
        "tokenizer",
        "image",
        "caption",
        "model",
        "text_decoder",
        "reset_state",
        "batch_size",
        "tf",
        "expand_dims",
        "data",
        "load_image",
        "image",
        "model",
        "inception",
        "temp_input",
        "tf",
        "reshape",
        "img_tensor_val",
        "img_tensor_val",
        "shape",
        "img_tensor_val",
        "shape",
        "model",
        "image_encoder",
        "img_tensor_val",
        "model",
        "doc2vec",
        "transform",
        "caption",
        "doc2vec_emb",
        "shape",
        "model",
        "text_encoder",
        "np",
        "array",
        "doc2vec_emb",
        "np",
        "newaxis",
        "tf",
        "reduce_mean",
        "features",
        "features2",
        "axis",
        "model",
        "image_decoder",
        "final_features",
        "tf",
        "expand_dims",
        "tokenizer",
        "word_index",
        "i",
        "model",
        "text_decoder",
        "dec_input",
        "features",
        "hidden",
        "tf",
        "argmax",
        "predictions",
        "numpy",
        "result",
        "append",
        "tokenizer",
        "index_word",
        "predicted_id",
        "tokenizer",
        "index_word",
        "predicted_id",
        "result",
        "recon",
        "tf",
        "expand_dims",
        "predicted_id",
        "result",
        "recon",
        "model",
        "tokenizer",
        "image",
        "caption",
        "model",
        "text_decoder",
        "reset_state",
        "batch_size",
        "tf",
        "expand_dims",
        "data",
        "load_image",
        "image",
        "model",
        "inception",
        "temp_input",
        "tf",
        "reshape",
        "img_tensor_val",
        "img_tensor_val",
        "shape",
        "img_tensor_val",
        "shape",
        "model",
        "image_encoder",
        "img_tensor_val",
        "model",
        "doc2vec",
        "transform",
        "caption",
        "model",
        "text_encoder",
        "np",
        "array",
        "doc2vec_emb",
        "np",
        "newaxis",
        "tf",
        "reduce_mean",
        "features",
        "features2",
        "axis",
        "features",
        "features2",
        "final_features",
        "train_captions",
        "img_name_vector",
        "data",
        "load_image",
        "img_name",
        "tf",
        "train",
        "AdamOptimizer",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "image_encoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "img_encoder_checkpoint",
        "restore",
        "tf",
        "train",
        "latest_checkpoint",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "image_decoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "img_decoder_checkpoint",
        "restore",
        "tf",
        "train",
        "latest_checkpoint",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "text_encoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "txt_encoder_checkpoint",
        "restore",
        "tf",
        "train",
        "latest_checkpoint",
        "tf",
        "train",
        "Checkpoint",
        "optimizer",
        "optimizer",
        "model",
        "model",
        "text_decoder",
        "optimizer_step",
        "tf",
        "train",
        "get_or_create_global_step",
        "txt_decoder_checkpoint",
        "restore",
        "tf",
        "train",
        "latest_checkpoint",
        "matplotlib",
        "pyplot",
        "plt",
        "np",
        "zeros",
        "np",
        "zeros",
        "np",
        "zeros",
        "np",
        "zeros",
        "i",
        "train_captions",
        "i",
        "img_name_vector",
        "i",
        "data",
        "load_image",
        "img_name",
        "evaluate_latent_space",
        "model",
        "tokenizer",
        "img_name",
        "caption",
        "f2",
        "f1",
        "f3",
        "f1",
        "f2",
        "np",
        "reshape",
        "difs",
        "np",
        "reshape",
        "f3s",
        "np",
        "reshape",
        "f1s",
        "np",
        "reshape",
        "f2s",
        "i",
        "np",
        "random",
        "randint",
        "f2s2",
        "shape",
        "np",
        "random",
        "randint",
        "f2s2",
        "shape",
        "j",
        "k",
        "plt",
        "figure",
        "plt",
        "scatter",
        "f1s2",
        "j",
        "f1s2",
        "k",
        "c",
        "plt",
        "scatter",
        "f2s2",
        "j",
        "f2s2",
        "k",
        "c",
        "plt",
        "show",
        "matplotlib",
        "pyplot",
        "plt",
        "np",
        "reshape",
        "difs",
        "np",
        "reshape",
        "f3s",
        "np",
        "reshape",
        "f1s",
        "np",
        "reshape",
        "f2s",
        "np",
        "sum",
        "f2s2",
        "plt",
        "figure",
        "plt",
        "scatter",
        "f1s2",
        "f1s2",
        "c",
        "plt",
        "scatter",
        "f2s2",
        "f2s2",
        "c",
        "plt",
        "show",
        "sklearn",
        "manifold",
        "sklearn",
        "decomposition",
        "decomposition",
        "PCA",
        "n_components",
        "pca",
        "fit_transform",
        "f3s2",
        "plt",
        "scatter",
        "Y1",
        "Y1",
        "cmap",
        "plt",
        "cm",
        "Spectral",
        "train_captions",
        "img_name_vector",
        "data",
        "load_image",
        "img_name",
        "evaluate_image_and_word",
        "model",
        "tokenizer",
        "img_name",
        "caption",
        "matplotlib",
        "pyplot",
        "plt",
        "plt",
        "figure",
        "plt",
        "imshow",
        "img_vec",
        "numpy",
        "plt",
        "figure",
        "plt",
        "imshow",
        "recon",
        "numpy",
        "caption",
        "result"
    ],
    "literals": [
        "'CUDA_VISIBLE_DEVICES'",
        "'2'",
        "'CUDA_VISIBLE_DEVICES'",
        "'./annotations/captions_train2014.json'",
        "'./train2014/'",
        "'captions.pickle'",
        "'wb'",
        "'img_names.pickle'",
        "'wb'",
        "'img_names.pickle'",
        "'rb'",
        "'./checkpoints2/img_encoder'",
        "'./checkpoints2/img_decoder'",
        "'./checkpoints2/txt_encoder'",
        "'./checkpoints2/txt_decoder'",
        "'<start>'",
        "'<end>'",
        "'<start>'",
        "'<end>'",
        "'./checkpoints4/img_encoder'",
        "'./checkpoints4/img_decoder'",
        "'./checkpoints4/txt_encoder'",
        "'./checkpoints4/txt_decoder'",
        "'blue'",
        "'red'",
        "'blue'",
        "'red'"
    ],
    "variables": [
        "annotation_file",
        "PATH",
        "annotation_file",
        "PATH",
        "train_captions",
        "img_name_vector",
        "all_captions",
        "all_img_name_vector",
        "model",
        "img_name_vector",
        "all_name_vector",
        "img_name_train",
        "img_name_val",
        "cap_train",
        "cap_val",
        "dataset",
        "tokenizer",
        "optimizer",
        "img_encoder_checkpoint",
        "img_decoder_checkpoint",
        "txt_encoder_checkpoint",
        "txt_decoder_checkpoint",
        "hidden",
        "temp_input",
        "img_tensor_val",
        "img_tensor_val",
        "features",
        "recon",
        "dec_input",
        "result",
        "predictions",
        "hidden",
        "attention_weights",
        "predicted_id",
        "dec_input",
        "hidden",
        "temp_input",
        "img_tensor_val",
        "img_tensor_val",
        "features",
        "doc2vec_emb",
        "features2",
        "final_features",
        "recon",
        "dec_input",
        "result",
        "predictions",
        "hidden",
        "attention_weights",
        "predicted_id",
        "dec_input",
        "hidden",
        "temp_input",
        "img_tensor_val",
        "img_tensor_val",
        "features",
        "doc2vec_emb",
        "features2",
        "final_features",
        "caption",
        "img_name",
        "img_vec",
        "optimizer",
        "img_encoder_checkpoint",
        "img_decoder_checkpoint",
        "txt_encoder_checkpoint",
        "txt_decoder_checkpoint",
        "difs",
        "f1s",
        "f2s",
        "f3s",
        "caption",
        "img_name",
        "img_vec",
        "f1",
        "f2",
        "f3",
        "difs",
        "i",
        "f3s",
        "i",
        "f1s",
        "i",
        "f2s",
        "i",
        "difs2",
        "f3s2",
        "f1s2",
        "f2s2",
        "j",
        "k",
        "difs2",
        "f3s2",
        "f1s2",
        "f2s2",
        "pca",
        "Y1",
        "caption",
        "img_name",
        "img_vec",
        "result",
        "recon"
    ],
    "comments": [
        "coding: utf-8",
        "In[2]:",
        "In[3]:",
        "In[ ]:",
        "In[4]:",
        "In[5]:",
        "In[ ]:",
        "pickle.dump((train_captions, all_captions), open('captions.pickle', 'wb'))",
        "In[6]:",
        "importlib.reload(image_encoder)",
        "In[ ]:",
        "In[7]:",
        "In[ ]:",
        "In[ ]:",
        "In[ ]:",
        "Create training and validation sets using 80-20 split",
        "In[8]:",
        "In[ ]:",
        "In[ ]:",
        "In[9]:",
        "attention_plot = np.zeros((max_length, attention_features_shape))",
        "attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()",
        "attention_plot = attention_plot[:len(result), :]",
        "attention_plot = np.zeros((max_length, attention_features_shape))",
        "attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()",
        "attention_plot = attention_plot[:len(result), :]",
        "attention_plot = np.zeros((max_length, attention_features_shape))",
        "print(doc2vec_emb.shape)",
        "In[9]:",
        "In[10]:",
        "In[12]:",
        "In[23]:",
        "In[24]:",
        "In[15]:",
        "In[16]:",
        "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)",
        "Y1 = tsne.fit_transform(f3s2)",
        "In[25]:",
        "In[26]:"
    ],
    "docstrings": [],
    "functions": [
        "evaluate_image",
        "evaluate_image_and_word",
        "evaluate_latent_space"
    ],
    "classes": []
}