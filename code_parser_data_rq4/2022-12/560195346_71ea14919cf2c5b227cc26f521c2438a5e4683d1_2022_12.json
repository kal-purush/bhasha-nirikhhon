{
    "identifiers": [
        "os",
        "sys",
        "numpy",
        "np",
        "pandas",
        "pd",
        "sklearn",
        "model_selection",
        "train_test_split",
        "keras",
        "preprocessing",
        "text",
        "Tokenizer",
        "keras",
        "preprocessing",
        "sequence",
        "pad_sequences",
        "tensorflow",
        "keras",
        "layers",
        "Input",
        "Concatenate",
        "Attention",
        "LSTM",
        "Embedding",
        "Dense",
        "TimeDistributed",
        "Conv1D",
        "MaxPooling1D",
        "tensorflow",
        "keras",
        "models",
        "Model",
        "tensorflow",
        "keras",
        "callbacks",
        "EarlyStopping",
        "csv",
        "rouge",
        "Rouge",
        "embedding_type",
        "embedding_type",
        "embedding_type",
        "embedding_map",
        "embedding_type",
        "embedding_type",
        "os",
        "path",
        "join",
        "os",
        "path",
        "dirname",
        "os",
        "getcwd",
        "pd",
        "read_csv",
        "os",
        "path",
        "join",
        "corpus_dir",
        "train_test_split",
        "trimmed_df",
        "trimmed_df",
        "test_size",
        "test_size",
        "random_state",
        "train_test_split",
        "test_val_x",
        "test_val_y",
        "test_size",
        "random_state",
        "trimmed_df",
        "test_x",
        "test_y",
        "Tokenizer",
        "t_tokenizer",
        "fit_on_texts",
        "train_x",
        "key",
        "value",
        "t_tokenizer",
        "word_counts",
        "items",
        "total_count",
        "total_frequency",
        "value",
        "value",
        "thresh",
        "count",
        "frequency",
        "value",
        "count",
        "total_count",
        "frequency",
        "total_frequency",
        "total_count",
        "count",
        "t_max_features",
        "Tokenizer",
        "s_tokenizer",
        "fit_on_texts",
        "train_y",
        "key",
        "value",
        "s_tokenizer",
        "word_counts",
        "items",
        "total_count",
        "total_frequency",
        "value",
        "value",
        "thresh",
        "count",
        "frequency",
        "value",
        "count",
        "total_count",
        "frequency",
        "total_frequency",
        "total_count",
        "count",
        "s_max_features",
        "Tokenizer",
        "num_words",
        "t_max_features",
        "t_tokenizer",
        "fit_on_texts",
        "train_x",
        "t_tokenizer",
        "texts_to_sequences",
        "train_x",
        "t_tokenizer",
        "texts_to_sequences",
        "val_x",
        "t_tokenizer",
        "texts_to_sequences",
        "test_x",
        "pad_sequences",
        "train_x",
        "maxlen",
        "maxlen_text",
        "padding",
        "pad_sequences",
        "val_x",
        "maxlen",
        "maxlen_text",
        "padding",
        "pad_sequences",
        "test_x",
        "maxlen",
        "maxlen_text",
        "padding",
        "Tokenizer",
        "num_words",
        "s_max_features",
        "s_tokenizer",
        "fit_on_texts",
        "train_y",
        "s_tokenizer",
        "texts_to_sequences",
        "train_y",
        "s_tokenizer",
        "texts_to_sequences",
        "val_y",
        "s_tokenizer",
        "texts_to_sequences",
        "test_y",
        "pad_sequences",
        "train_y",
        "maxlen",
        "maxlen_summ",
        "padding",
        "pad_sequences",
        "val_y",
        "maxlen",
        "maxlen_summ",
        "padding",
        "pad_sequences",
        "test_y",
        "maxlen",
        "maxlen_summ",
        "padding",
        "train_x",
        "shape",
        "train_y",
        "shape",
        "val_x",
        "shape",
        "val_y",
        "shape",
        "open",
        "embedding_file",
        "encoding",
        "f",
        "line",
        "f",
        "line",
        "split",
        "values",
        "np",
        "asarray",
        "values",
        "dtype",
        "coefs",
        "np",
        "zeros",
        "t_max_features",
        "embed_dim",
        "word",
        "i",
        "t_tokenizer",
        "word_index",
        "items",
        "embedding_index",
        "get",
        "word",
        "i",
        "t_max_features",
        "vec",
        "vec",
        "np",
        "zeros",
        "s_max_features",
        "embed_dim",
        "word",
        "i",
        "s_tokenizer",
        "word_index",
        "items",
        "embedding_index",
        "get",
        "word",
        "i",
        "s_max_features",
        "vec",
        "vec",
        "Input",
        "shape",
        "maxlen_text",
        "Embedding",
        "t_max_features",
        "embed_dim",
        "input_length",
        "maxlen_text",
        "weights",
        "t_embed",
        "trainable",
        "encoder_inputs",
        "Conv1D",
        "filters",
        "kernel_size",
        "padding",
        "activation",
        "strides",
        "encoder_embedding",
        "MaxPooling1D",
        "pool_size",
        "pool_size",
        "encoder_cnn",
        "Bidirectional",
        "LSTM",
        "lstm_output_size",
        "return_sequences",
        "return_state",
        "dropout",
        "recurrent_dropout",
        "merge_mode",
        "encoder_bi_lstm1",
        "encoder_maxpool",
        "forward_state_h1",
        "forward_state_c1",
        "backward_state_h1",
        "backward_state_c1",
        "Input",
        "shape",
        "Embedding",
        "s_max_features",
        "embed_dim",
        "weights",
        "s_embed",
        "trainable",
        "dec_emb_layer",
        "decoder_inputs",
        "Bidirectional",
        "LSTM",
        "lstm_output_size",
        "return_sequences",
        "return_state",
        "dropout",
        "recurrent_dropout",
        "merge_mode",
        "decoder_bi_lstm",
        "decoder_embedding",
        "initial_state",
        "encoder_states1",
        "decoder_fwd_state_h1",
        "decoder_fwd_state_c1",
        "decoder_back_state_h1",
        "decoder_back_state_c1",
        "Attention",
        "encoder_output1",
        "decoder_outputs",
        "Concatenate",
        "axis",
        "name",
        "decoder_outputs",
        "attn_out",
        "TimeDistributed",
        "Dense",
        "s_max_features",
        "activation",
        "decoder_dense",
        "decoder_concat_input",
        "Model",
        "encoder_inputs",
        "decoder_inputs",
        "outputs",
        "model",
        "summary",
        "model",
        "compile",
        "optimizer",
        "loss",
        "EarlyStopping",
        "monitor",
        "mode",
        "verbose",
        "patience",
        "model",
        "fit",
        "train_x",
        "train_y",
        "train_y",
        "reshape",
        "train_y",
        "shape",
        "train_y",
        "shape",
        "epochs",
        "callbacks",
        "early_stop",
        "batch_size",
        "verbose",
        "validation_data",
        "val_x",
        "val_y",
        "val_y",
        "reshape",
        "val_y",
        "shape",
        "val_y",
        "shape",
        "model",
        "save",
        "embedding_type",
        "Model",
        "inputs",
        "encoder_inputs",
        "outputs",
        "encoder_states1",
        "Input",
        "shape",
        "lstm_output_size",
        "Input",
        "shape",
        "lstm_output_size",
        "Input",
        "shape",
        "lstm_output_size",
        "Input",
        "shape",
        "lstm_output_size",
        "Input",
        "shape",
        "maxlen_text",
        "lstm_output_size",
        "dec_emb_layer",
        "decoder_inputs",
        "decoder_bi_lstm",
        "dec_emb2",
        "initial_state",
        "decoder_states",
        "decoder_fwd_state_h2",
        "decoder_fwd_state_c2",
        "decoder_back_state_h2",
        "decoder_back_state_c2",
        "decoder_dense",
        "decoder_outputs2",
        "Model",
        "decoder_inputs",
        "dec_hidden_inp",
        "dec_h_state_f",
        "dec_h_state_r",
        "dec_c_state_f",
        "dec_c_state_r",
        "decoder_outputs2",
        "decoder_states2",
        "input_seq",
        "encoder_model",
        "predict",
        "input_seq",
        "np",
        "zeros",
        "s_tokenizer",
        "word_index",
        "stop",
        "count",
        "decoder_model",
        "predict",
        "next_token",
        "f_h",
        "f_c",
        "b_h",
        "b_c",
        "np",
        "argmax",
        "decoder_out",
        "token_idx",
        "s_tokenizer",
        "word_index",
        "token_idx",
        "token_idx",
        "s_tokenizer",
        "word_index",
        "s_tokenizer",
        "index_word",
        "token_idx",
        "output_seq",
        "token",
        "np",
        "zeros",
        "token_idx",
        "d_f_h",
        "d_f_c",
        "d_b_h",
        "d_b_c",
        "count",
        "output_seq",
        "open",
        "embedding_type",
        "f",
        "csv",
        "writer",
        "f",
        "writer",
        "writerow",
        "i",
        "len",
        "test_x",
        "generate_summary",
        "test_x",
        "i",
        "reshape",
        "maxlen_text",
        "hyps",
        "append",
        "our_summ",
        "writer",
        "writerow",
        "og_test_x",
        "iloc",
        "i",
        "og_test_y",
        "iloc",
        "i",
        "our_summ",
        "Rouge",
        "rouge",
        "get_scores",
        "hyps",
        "og_test_y",
        "avg",
        "ignore_empty"
    ],
    "literals": [
        "'glove'",
        "'glove.6B.300d.txt'",
        "'law2vec'",
        "'Law2Vec.200d.txt'",
        "'glove'",
        "\"Embedding type: \"",
        "'corpus'",
        "'fulltext.csv'",
        "'cleaned_text'",
        "'cleaned_summary'",
        "'% of rare words in vocabulary: '",
        "'Total Coverage of rare words: '",
        "'Text Vocab: '",
        "'% of rare words in vocabulary: '",
        "'Total Coverage of rare words: '",
        "'Summary Vocab: '",
        "'post'",
        "'post'",
        "'post'",
        "'post'",
        "'post'",
        "'post'",
        "\"Training Sequence\"",
        "'Target Values Shape'",
        "'Test Sequence'",
        "'Target Test Shape'",
        "f'../embeddings/{embedding_file}'",
        "'r'",
        "'utf-8'",
        "'float32'",
        "'valid'",
        "'relu'",
        "\"concat\"",
        "\"concat\"",
        "'concat_layer'",
        "'softmax'",
        "'rmsprop'",
        "'sparse_categorical_crossentropy'",
        "'val_loss'",
        "'min'",
        "f'./{embedding_type}_model_bidirectional_att'",
        "'sostok'",
        "''",
        "'eostok'",
        "'sostok'",
        "' '",
        "f'./{embedding_type}_bidirectional_att_result.csv'",
        "'w'",
        "'Article'",
        "'Original Summary'",
        "'Model Output'"
    ],
    "variables": [
        "embedding_type",
        "embedding_file",
        "embed_dim",
        "epochs",
        "corpus_dir",
        "trimmed_df",
        "maxlen_text",
        "maxlen_summ",
        "test_size",
        "train_x",
        "test_val_x",
        "train_y",
        "test_val_y",
        "val_x",
        "test_x",
        "val_y",
        "test_y",
        "og_test_x",
        "og_test_y",
        "t_tokenizer",
        "thresh",
        "count",
        "total_count",
        "frequency",
        "total_frequency",
        "t_max_features",
        "s_tokenizer",
        "thresh",
        "count",
        "total_count",
        "frequency",
        "total_frequency",
        "s_max_features",
        "t_tokenizer",
        "train_x",
        "val_x",
        "test_x",
        "train_x",
        "val_x",
        "test_x",
        "s_tokenizer",
        "train_y",
        "val_y",
        "test_y",
        "train_y",
        "val_y",
        "test_y",
        "embedding_index",
        "values",
        "word",
        "coefs",
        "embedding_index",
        "word",
        "t_embed",
        "vec",
        "t_embed",
        "i",
        "s_embed",
        "vec",
        "s_embed",
        "i",
        "filters",
        "kernel_size",
        "pool_size",
        "lstm_output_size",
        "encoder_inputs",
        "encoder_embedding",
        "encoder_cnn",
        "encoder_maxpool",
        "encoder_bi_lstm1",
        "encoder_output1",
        "forward_state_h1",
        "forward_state_c1",
        "backward_state_h1",
        "backward_state_c1",
        "encoder_states1",
        "decoder_inputs",
        "dec_emb_layer",
        "decoder_embedding",
        "decoder_bi_lstm",
        "decoder_outputs",
        "decoder_fwd_state_h1",
        "decoder_fwd_state_c1",
        "decoder_back_state_h1",
        "decoder_back_state_c1",
        "decoder_states",
        "attn_out",
        "decoder_concat_input",
        "decoder_dense",
        "outputs",
        "model",
        "early_stop",
        "encoder_model",
        "dec_h_state_f",
        "dec_h_state_r",
        "dec_c_state_f",
        "dec_c_state_r",
        "dec_hidden_inp",
        "dec_emb2",
        "decoder_outputs2",
        "decoder_fwd_state_h2",
        "decoder_fwd_state_c2",
        "decoder_back_state_h2",
        "decoder_back_state_c2",
        "decoder_states2",
        "decoder_outputs2",
        "decoder_model",
        "f_h",
        "f_c",
        "b_h",
        "b_c",
        "next_token",
        "next_token",
        "output_seq",
        "stop",
        "count",
        "decoder_out",
        "d_f_h",
        "d_f_c",
        "d_b_h",
        "d_b_c",
        "token_idx",
        "stop",
        "token",
        "output_seq",
        "next_token",
        "next_token",
        "f_h",
        "f_c",
        "b_h",
        "b_c",
        "hyps",
        "writer",
        "our_summ",
        "rouge"
    ],
    "comments": [
        "Set up the decoder, using `encoder_states` as initial state.",
        "dense layer",
        "Encode the input sequence to get the feature vector",
        "Decoder setup",
        "Below tensors will hold the states of the previous time step",
        "Create the hidden input layer with twice the latent dimension,",
        "since we are using bi - directional LSTM's we will get",
        "two hidden states and two cell states",
        "Get the embeddings of the decoder sequence",
        "To predict the next word in the sequence, set the initial states to the states from the previous time step",
        "A dense softmax layer to generate prob dist. over the target vocabulary",
        "Final decoder model"
    ],
    "docstrings": [],
    "functions": [
        "embedding_map",
        "generate_summary"
    ],
    "classes": []
}