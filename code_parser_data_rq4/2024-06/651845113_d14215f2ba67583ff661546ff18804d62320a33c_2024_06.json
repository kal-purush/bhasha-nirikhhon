{
    "identifiers": [
        "fastapi",
        "FastAPI",
        "HTTPException",
        "Request",
        "fastapi",
        "middleware",
        "cors",
        "CORSMiddleware",
        "pydantic",
        "BaseModel",
        "dotenv",
        "load_dotenv",
        "openai",
        "OpenAI",
        "fastapi",
        "encoders",
        "jsonable_encoder",
        "google",
        "generativeai",
        "genai",
        "openai",
        "os",
        "logging",
        "uvicorn",
        "load_dotenv",
        "BaseModel",
        "logging",
        "basicConfig",
        "level",
        "logging",
        "DEBUG",
        "logging",
        "getLogger",
        "OpenAI",
        "organization",
        "os",
        "getenv",
        "project",
        "os",
        "getenv",
        "os",
        "getenv",
        "genai",
        "configure",
        "api_key",
        "os",
        "environ",
        "genai",
        "GenerativeModel",
        "logger",
        "info",
        "FastAPI",
        "app",
        "add_middleware",
        "CORSMiddleware",
        "allow_origins",
        "allow_credentials",
        "allow_methods",
        "allow_headers",
        "app",
        "get",
        "app",
        "post",
        "prompt_request",
        "PromptRequest",
        "prompt_request",
        "prompt",
        "logger",
        "debug",
        "prompt",
        "logger",
        "info",
        "len",
        "prompt",
        "split",
        "prompt",
        "gemini_model",
        "generate_content",
        "prompt",
        "logger",
        "info",
        "response",
        "text",
        "response",
        "text",
        "e",
        "logger",
        "error",
        "e",
        "HTTPException",
        "status_code",
        "detail",
        "uvicorn",
        "run",
        "app",
        "host",
        "port"
    ],
    "literals": [
        "\"gpt-3.5-turbo\"",
        "'ORGANISATION_ID'",
        "'PROJECT_ID'",
        "'OPENAI_API_KEY'",
        "\"GEMINI_API_KEY\"",
        "'gemini-1.5-flash'",
        "\"main.py is being executed\"",
        "\"http://localhost:5173\"",
        "\"GET\"",
        "\"POST\"",
        "\"PUT\"",
        "\"DELETE\"",
        "\"*\"",
        "\"/\"",
        "\"message\"",
        "\"Hello from GPT\"",
        "\"/\"",
        "f\"Received prompt: {prompt}\"",
        "f\"Input Tokens (length: {len(prompt.split())}): {prompt}\"",
        "f\"{response.text}\"",
        "\"bot\"",
        "f\"Error generating response: {e}\"",
        "\"Something went wrong\"",
        "\"__main__\"",
        "\"0.0.0.0\""
    ],
    "variables": [
        "prompt",
        "model",
        "logger",
        "client",
        "openai",
        "api_key",
        "gemini_model",
        "app",
        "prompt",
        "response"
    ],
    "comments": [
        "Configure logging",
        "Initialize OpenAI API",
        "if len(prompt.split()) > 4096:",
        "model = gemini_model",
        "if model == \"gpt-3.5-turbo\":",
        "stream = client.chat.completions.create(",
        "model=\"gpt-3.5-turbo\",",
        "messages=[{\"role\": \"user\", \"content\": prompt}],",
        "stream=True,",
        ")",
        "content = \"\"",
        "for chunk in stream:",
        "if chunk.choices[0].delta.content is not None:",
        "content += chunk.choices[0].delta.content",
        "return content",
        "else:"
    ],
    "docstrings": [],
    "functions": [
        "read_root",
        "generate_response"
    ],
    "classes": [
        "PromptRequest"
    ]
}