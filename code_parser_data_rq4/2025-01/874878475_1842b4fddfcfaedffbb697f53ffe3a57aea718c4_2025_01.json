{
    "identifiers": [
        "argparse",
        "math",
        "torch",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "RandomSampler",
        "SequentialSampler",
        "torch",
        "utils",
        "data",
        "distributed",
        "DistributedSampler",
        "transformers",
        "AutoModelForCausalLM",
        "SchedulerType",
        "get_scheduler",
        "deepspeed",
        "deepspeed",
        "ops",
        "adam",
        "DeepSpeedCPUAdam",
        "FusedAdam",
        "deepspeed",
        "get_accelerator",
        "dschat",
        "utils",
        "data",
        "data_utils",
        "create_prompt_dataset",
        "DataCollatorReward",
        "dschat",
        "utils",
        "utils",
        "print_rank_0",
        "to_device",
        "save_hf_format",
        "set_random_seed",
        "get_all_reduce_mean",
        "get_optimizer_grouped_parameters",
        "save_zero_three_model",
        "load_hf_tokenizer",
        "dschat",
        "utils",
        "ds_utils",
        "get_train_ds_config",
        "get_eval_ds_config",
        "dschat",
        "utils",
        "lora",
        "convert_linear_layer_to_lora",
        "convert_lora_to_linear_layer",
        "only_optimize_lora_parameters",
        "make_model_gradient_checkpointing_compatible",
        "dschat",
        "utils",
        "model",
        "model_utils",
        "create_hf_model",
        "causal_lm_model_to_fp32_loss",
        "dschat",
        "utils",
        "perf",
        "print_throughput",
        "argparse",
        "ArgumentParser",
        "description",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "required",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "SchedulerType",
        "help",
        "choices",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "choices",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "parser",
        "add_argument",
        "action",
        "help",
        "parser",
        "add_argument",
        "action",
        "help",
        "deepspeed",
        "add_config_arguments",
        "parser",
        "parser",
        "parse_args",
        "args",
        "logits",
        "input_ids",
        "label_mask",
        "input_ids",
        "clone",
        "label_mask",
        "logits",
        "shape",
        "labels",
        "shape",
        "labels",
        "label_mask",
        "logits",
        "torch",
        "gather",
        "logits",
        "log_softmax",
        "dim",
        "index",
        "labels",
        "unsqueeze",
        "squeeze",
        "per_token_logps",
        "label_mask",
        "sum",
        "parse_args",
        "args",
        "local_rank",
        "torch",
        "device",
        "get_accelerator",
        "device_name",
        "get_accelerator",
        "set_device",
        "args",
        "local_rank",
        "torch",
        "device",
        "get_accelerator",
        "device_name",
        "args",
        "local_rank",
        "deepspeed",
        "init_distributed",
        "torch",
        "distributed",
        "get_rank",
        "get_train_ds_config",
        "offload",
        "args",
        "offload",
        "dtype",
        "args",
        "dtype",
        "stage",
        "args",
        "zero_stage",
        "enable_tensorboard",
        "args",
        "enable_tensorboard",
        "tb_path",
        "args",
        "tensorboard_path",
        "tb_name",
        "args",
        "per_device_train_batch_size",
        "args",
        "per_device_train_batch_size",
        "torch",
        "distributed",
        "get_world_size",
        "args",
        "gradient_accumulation_steps",
        "set_random_seed",
        "args",
        "seed",
        "torch",
        "distributed",
        "barrier",
        "args",
        "end_of_conversation_token",
        "args",
        "add_eot_token",
        "load_hf_tokenizer",
        "args",
        "model_name_or_path",
        "fast_tokenizer",
        "add_special_tokens",
        "additional_special_tokens",
        "create_hf_model",
        "AutoModelForCausalLM",
        "args",
        "model_name_or_path",
        "tokenizer",
        "ds_config",
        "dropout",
        "args",
        "dropout",
        "args",
        "zero_stage",
        "ref_zero_stage",
        "get_eval_ds_config",
        "args",
        "offload_reference_model",
        "args",
        "dtype",
        "ref_zero_stage",
        "args",
        "per_device_train_batch_size",
        "args",
        "per_device_train_batch_size",
        "torch",
        "distributed",
        "get_world_size",
        "args",
        "gradient_accumulation_steps",
        "get_eval_ds_config",
        "offload",
        "dtype",
        "args",
        "dtype",
        "stage",
        "ref_zero_stage",
        "args",
        "per_device_train_batch_size",
        "args",
        "per_device_train_batch_size",
        "torch",
        "distributed",
        "get_world_size",
        "args",
        "gradient_accumulation_steps",
        "create_hf_model",
        "AutoModelForCausalLM",
        "args",
        "model_name_or_path",
        "tokenizer",
        "ref_ds_eval_config",
        "dropout",
        "args",
        "dropout",
        "args",
        "compute_fp32_loss",
        "print_rank_0",
        "model",
        "__class__",
        "args",
        "global_rank",
        "causal_lm_model_to_fp32_loss",
        "model",
        "args",
        "model_name_or_path",
        "args",
        "zero_stage",
        "model",
        "rwtranrsformer",
        "ln_f",
        "weight",
        "model",
        "rwtranrsformer",
        "ln_f",
        "bias",
        "deepspeed",
        "zero",
        "GatheredParameters",
        "modifier_rank",
        "enabled",
        "zero_init_enabled",
        "deepspeed",
        "comm",
        "get_rank",
        "zero_init_enabled",
        "torch",
        "nn",
        "init",
        "ones_",
        "model",
        "rwtransformer",
        "ln_f",
        "weight",
        "torch",
        "nn",
        "init",
        "zeros_",
        "model",
        "rwtransformer",
        "ln_f",
        "bias",
        "force_optimize_params",
        "extend",
        "args",
        "lora_dim",
        "convert_linear_layer_to_lora",
        "model",
        "args",
        "lora_module_name",
        "args",
        "lora_dim",
        "args",
        "only_optimize_lora",
        "only_optimize_lora_parameters",
        "model",
        "make_model_gradient_checkpointing_compatible",
        "model",
        "create_prompt_dataset",
        "args",
        "local_rank",
        "args",
        "data_path",
        "args",
        "data_split",
        "args",
        "data_output_path",
        "train_phase",
        "args",
        "seed",
        "tokenizer",
        "args",
        "max_seq_len",
        "DataCollatorReward",
        "args",
        "local_rank",
        "RandomSampler",
        "train_dataset",
        "SequentialSampler",
        "eval_dataset",
        "DistributedSampler",
        "train_dataset",
        "DistributedSampler",
        "eval_dataset",
        "DataLoader",
        "train_dataset",
        "collate_fn",
        "data_collator",
        "sampler",
        "train_sampler",
        "batch_size",
        "args",
        "per_device_train_batch_size",
        "DataLoader",
        "eval_dataset",
        "collate_fn",
        "data_collator",
        "sampler",
        "eval_sampler",
        "batch_size",
        "args",
        "per_device_eval_batch_size",
        "model",
        "ref_model",
        "tokenizer",
        "eval_dataloader",
        "model",
        "eval",
        "step",
        "batch",
        "eval_dataloader",
        "to_device",
        "batch",
        "device",
        "batch",
        "shape",
        "batch",
        "batch_size",
        "batch",
        "batch_size",
        "batch",
        "tokenizer",
        "pad_token_id",
        "i",
        "batch_size",
        "chosen_input_ids",
        "i",
        "rejected_input_ids",
        "i",
        "nonzero",
        "squeeze",
        "len",
        "divergence_ind",
        "divergence_ind",
        "label_mask",
        "i",
        "divergence_ind",
        "label_mask",
        "i",
        "batch_size",
        "divergence_ind",
        "torch",
        "no_grad",
        "model",
        "batch",
        "ref_model",
        "batch",
        "get_batch_logps",
        "outputs",
        "logits",
        "batch",
        "label_mask",
        "get_batch_logps",
        "ref_outputs",
        "logits",
        "batch",
        "label_mask",
        "logps",
        "batch_size",
        "logps",
        "batch_size",
        "ref_logps",
        "batch_size",
        "ref_logps",
        "batch_size",
        "args",
        "beta",
        "chosen_logps",
        "ref_chosen_logps",
        "rejected_logps",
        "ref_rejected_logps",
        "torch",
        "nn",
        "functional",
        "logsigmoid",
        "logits",
        "args",
        "label_smoothing",
        "torch",
        "nn",
        "functional",
        "logsigmoid",
        "logits",
        "args",
        "label_smoothing",
        "mean",
        "losses",
        "loss",
        "losses",
        "step",
        "get_all_reduce_mean",
        "losses",
        "args",
        "beta",
        "chosen_logps",
        "ref_chosen_logps",
        "detach",
        "args",
        "beta",
        "rejected_logps",
        "ref_rejected_logps",
        "detach",
        "chosen_rewards",
        "mean",
        "item",
        "rejected_rewards",
        "mean",
        "item",
        "losses",
        "item",
        "get_optimizer_grouped_parameters",
        "model",
        "args",
        "weight_decay",
        "args",
        "lora_learning_rate",
        "DeepSpeedCPUAdam",
        "args",
        "offload",
        "FusedAdam",
        "AdamOptimizer",
        "optimizer_grouped_parameters",
        "lr",
        "args",
        "learning_rate",
        "betas",
        "math",
        "ceil",
        "len",
        "train_dataloader",
        "args",
        "gradient_accumulation_steps",
        "get_scheduler",
        "name",
        "args",
        "lr_scheduler_type",
        "optimizer",
        "optimizer",
        "num_warmup_steps",
        "args",
        "num_warmup_steps",
        "num_training_steps",
        "args",
        "num_train_epochs",
        "num_update_steps_per_epoch",
        "deepspeed",
        "initialize",
        "model",
        "model",
        "optimizer",
        "optimizer",
        "args",
        "args",
        "config",
        "ds_config",
        "lr_scheduler",
        "lr_scheduler",
        "dist_init_required",
        "_",
        "deepspeed",
        "initialize",
        "model",
        "ref_model",
        "config",
        "ref_ds_config",
        "ref_model",
        "eval",
        "args",
        "gradient_checkpointing",
        "model",
        "gradient_checkpointing_enable",
        "print_rank_0",
        "args",
        "global_rank",
        "print_rank_0",
        "args",
        "num_train_epochs",
        "args",
        "global_rank",
        "evaluation",
        "model",
        "ref_model",
        "tokenizer",
        "eval_dataloader",
        "print_rank_0",
        "chosen_rewards",
        "rejected_rewards",
        "eval_loss",
        "args",
        "global_rank",
        "epoch",
        "args",
        "num_train_epochs",
        "print_rank_0",
        "epoch",
        "args",
        "num_train_epochs",
        "len",
        "train_dataloader",
        "args",
        "global_rank",
        "model",
        "train",
        "time",
        "step",
        "batch",
        "train_dataloader",
        "time",
        "time",
        "to_device",
        "batch",
        "device",
        "batch",
        "shape",
        "batch",
        "batch_size",
        "batch",
        "batch_size",
        "batch",
        "tokenizer",
        "pad_token_id",
        "i",
        "batch_size",
        "chosen_input_ids",
        "i",
        "rejected_input_ids",
        "i",
        "nonzero",
        "squeeze",
        "len",
        "divergence_ind",
        "divergence_ind",
        "label_mask",
        "i",
        "divergence_ind",
        "label_mask",
        "i",
        "batch_size",
        "divergence_ind",
        "model",
        "batch",
        "use_cache",
        "torch",
        "no_grad",
        "ref_model",
        "batch",
        "get_batch_logps",
        "outputs",
        "logits",
        "batch",
        "label_mask",
        "get_batch_logps",
        "ref_outputs",
        "logits",
        "batch",
        "label_mask",
        "logps",
        "batch_size",
        "logps",
        "batch_size",
        "ref_logps",
        "batch_size",
        "ref_logps",
        "batch_size",
        "args",
        "beta",
        "chosen_logps",
        "ref_chosen_logps",
        "rejected_logps",
        "ref_rejected_logps",
        "torch",
        "nn",
        "functional",
        "logsigmoid",
        "logits",
        "args",
        "label_smoothing",
        "torch",
        "nn",
        "functional",
        "logsigmoid",
        "logits",
        "args",
        "label_smoothing",
        "mean",
        "args",
        "print_loss",
        "epoch",
        "step",
        "torch",
        "distributed",
        "get_rank",
        "loss",
        "model",
        "backward",
        "loss",
        "model",
        "step",
        "time",
        "time",
        "torch",
        "distributed",
        "get_rank",
        "print_throughput",
        "model",
        "model",
        "args",
        "end",
        "start",
        "args",
        "global_rank",
        "print_rank_0",
        "epoch",
        "args",
        "num_train_epochs",
        "args",
        "global_rank",
        "evaluation",
        "model",
        "ref_model",
        "tokenizer",
        "eval_dataloader",
        "print_rank_0",
        "chosen_rewards",
        "rejected_rewards",
        "eval_loss",
        "args",
        "global_rank",
        "model",
        "tput_timer",
        "update_epoch_count",
        "args",
        "output_dir",
        "print_rank_0",
        "args",
        "global_rank",
        "convert_lora_to_linear_layer",
        "model",
        "args",
        "global_rank",
        "save_hf_format",
        "model",
        "tokenizer",
        "args",
        "args",
        "zero_stage",
        "save_zero_three_model",
        "model",
        "args",
        "global_rank",
        "args",
        "output_dir",
        "zero_stage",
        "args",
        "zero_stage",
        "main"
    ],
    "literals": [
        "\"Finetune a transformers model on a causal language modeling task\"",
        "'--data_path'",
        "'*'",
        "'Dahoas/rm-static'",
        "'Path to the training dataset. Accepted format:'",
        "'1) a single data path, 2) multiple datasets in the'",
        "'form: dataset1-path dataset2-path ...'",
        "'--data_split'",
        "'2,4,4'",
        "'Comma-separated list of proportions for training'",
        "'phase 1, 2, and 3 data. For example the split `6,2,2`'",
        "'will use 60%% of data for phase 1, 20%% for phase 2'",
        "'and 20%% for phase 3.'",
        "'--data_output_path'",
        "'/tmp/data_files/'",
        "'Where to store the data-related files such as shuffle index. This needs to be on a local storage of a node (not on a shared storage)'",
        "\"--model_name_or_path\"",
        "\"Path to pretrained model or model identifier from huggingface.co/models.\"",
        "\"--per_device_train_batch_size\"",
        "\"Batch size (per device) for the training dataloader.\"",
        "\"--per_device_eval_batch_size\"",
        "\"Batch size (per device) for the evaluation dataloader.\"",
        "\"--max_seq_len\"",
        "\"The maximum sequence length.\"",
        "\"--learning_rate\"",
        "\"Initial learning rate (after the potential warmup period) to use.\"",
        "\"--weight_decay\"",
        "\"Weight decay to use.\"",
        "\"--num_train_epochs\"",
        "\"Total number of training epochs to perform.\"",
        "\"--beta\"",
        "\"Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\"",
        "\"--label_smoothing\"",
        "\"conservativeness for DPO loss, which assumes that preferences are noisy (flipped with probability label_smoothing)\"",
        "\"--gradient_accumulation_steps\"",
        "\"Number of updates steps to accumulate before performing a backward/update pass.\"",
        "\"--lr_scheduler_type\"",
        "\"cosine\"",
        "\"The scheduler type to use.\"",
        "\"linear\"",
        "\"cosine\"",
        "\"cosine_with_restarts\"",
        "\"polynomial\"",
        "\"constant\"",
        "\"constant_with_warmup\"",
        "\"--num_warmup_steps\"",
        "\"Number of steps for the warmup in the lr scheduler.\"",
        "\"--output_dir\"",
        "\"Where to store the model.\"",
        "\"--seed\"",
        "\"A seed for reproducible training.\"",
        "\"--local_rank\"",
        "\"local_rank for distributed training on gpus\"",
        "'--gradient_checkpointing'",
        "'store_true'",
        "'Enable HF gradient checkpointing for model.'",
        "\"--dropout\"",
        "\"If dropout configured, use it. \"",
        "\"Otherwise, keep the default dropout configuration of the model.\"",
        "'--offload'",
        "'store_true'",
        "'Enable ZeRO Offload techniques.'",
        "'--dtype'",
        "'fp16'",
        "'fp16'",
        "'bf16'",
        "'Training data type'",
        "'--offload_reference_model'",
        "'store_true'",
        "'Enable ZeRO Offload techniques for reference model.'",
        "'--zero_stage'",
        "'ZeRO optimization stage for Actor model (and clones).'",
        "\"--lora_dim\"",
        "\"If > 0, use LoRA for efficient training.\"",
        "\"--lora_module_name\"",
        "\"decoder.layers.\"",
        "\"The scope of LoRA.\"",
        "'--only_optimize_lora'",
        "'store_true'",
        "'Only optimize the LoRA parameters.'",
        "\"--lora_learning_rate\"",
        "\"Initial LoRA learning rate (after the potential warmup period) to use.\"",
        "'--compute_fp32_loss'",
        "'store_true'",
        "'Relevant for low precision dtypes (fp16, bf16, etc.). '",
        "'If specified, loss is calculated in fp32.'",
        "'--enable_tensorboard'",
        "'store_true'",
        "'Enable tensorboard logging'",
        "'--tensorboard_path'",
        "\"step2_tensorboard\"",
        "\"--add_eot_token\"",
        "'store_true'",
        "\"Add <|endoftext|> as additional special token to tokenizer\"",
        "'--print_loss'",
        "'store_true'",
        "'Prints loss at each step.'",
        "\"Logits (batch and sequence length dim) and labels must have the same shape.\"",
        "\"step2_model\"",
        "'train_micro_batch_size_per_gpu'",
        "'train_batch_size'",
        "\"<|endoftext|>\"",
        "'train_micro_batch_size_per_gpu'",
        "'train_batch_size'",
        "'train_micro_batch_size_per_gpu'",
        "'train_batch_size'",
        "f\"Using model {model.__class__.__name__} with loss in fp32\"",
        "\"bigscience/bloom-\"",
        "'rwtransformer.ln_f.weight'",
        "'rwtransformer.ln_f.bias'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "\"***** Running training *****\"",
        "f\"***** Evaluating rewards, Epoch {1}/{args.num_train_epochs} *****\"",
        "f\"chosen: {chosen_rewards}, rejected: {rejected_rewards}, loss: {eval_loss}\"",
        "f\"Beginning of Epoch {epoch+1}/{args.num_train_epochs}, Total Micro Batches {len(train_dataloader)}\"",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "'input_ids'",
        "f\"Epoch: {epoch}, Step: {step}, Rank: {torch.distributed.get_rank()}, loss = {loss}\"",
        "f\"***** Evaluating rewards, Epoch {epoch+1}/{args.num_train_epochs} *****\"",
        "f\"chosen: {chosen_rewards}, rejected: {rejected_rewards}, loss: {eval_loss}\"",
        "'saving the final model ...'",
        "\"__main__\""
    ],
    "variables": [
        "parser",
        "parser",
        "args",
        "labels",
        "labels",
        "label_mask",
        "logits",
        "per_token_logps",
        "args",
        "device",
        "device",
        "args",
        "global_rank",
        "ds_config",
        "ds_config",
        "ds_config",
        "args",
        "end_of_conversation_token",
        "additional_special_tokens",
        "tokenizer",
        "model",
        "ref_zero_stage",
        "ref_zero_stage",
        "ref_ds_config",
        "ref_ds_config",
        "ref_ds_config",
        "ref_ds_eval_config",
        "ref_ds_eval_config",
        "ref_ds_eval_config",
        "ref_model",
        "force_optimize_params",
        "zero_init_enabled",
        "model",
        "model",
        "model",
        "train_phase",
        "train_dataset",
        "eval_dataset",
        "data_collator",
        "train_sampler",
        "eval_sampler",
        "train_sampler",
        "eval_sampler",
        "train_dataloader",
        "eval_dataloader",
        "losses",
        "batch",
        "batch_size",
        "chosen_input_ids",
        "rejected_input_ids",
        "label_mask",
        "divergence_ind",
        "divergence_ind",
        "divergence_ind",
        "outputs",
        "ref_outputs",
        "logps",
        "ref_logps",
        "chosen_logps",
        "rejected_logps",
        "ref_chosen_logps",
        "ref_rejected_logps",
        "logits",
        "loss",
        "losses",
        "losses",
        "chosen_rewards",
        "rejected_rewards",
        "optimizer_grouped_parameters",
        "AdamOptimizer",
        "optimizer",
        "num_update_steps_per_epoch",
        "lr_scheduler",
        "model",
        "optimizer",
        "_",
        "lr_scheduler",
        "ref_model",
        "chosen_rewards",
        "rejected_rewards",
        "eval_loss",
        "start",
        "batch",
        "batch_size",
        "chosen_input_ids",
        "rejected_input_ids",
        "label_mask",
        "divergence_ind",
        "divergence_ind",
        "divergence_ind",
        "outputs",
        "ref_outputs",
        "logps",
        "ref_logps",
        "chosen_logps",
        "rejected_logps",
        "ref_chosen_logps",
        "ref_rejected_logps",
        "logits",
        "loss",
        "end",
        "chosen_rewards",
        "rejected_rewards",
        "eval_loss",
        "model"
    ],
    "comments": [
        "!/usr/bin/env python",
        "Copyright (c) Microsoft Corporation.",
        "SPDX-License-Identifier: Apache-2.0",
        "DeepSpeed Team",
        "Reference: https://github.com/eric-mitchell/direct-preference-optimization/blob/main/trainers.py",
        "deepspeed features",
        "LoRA for efficient training setting",
        "low precision",
        "Tensorboard logging",
        "Tokenizer",
        "Print loss",
        "Reference: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py",
        "Initializes the distributed backend which will take care of sychronizing nodes/GPUs",
        "torch.distributed.init_process_group(backend='nccl')",
        "If passed along, set the training seed now.",
        "load_hf_tokenizer will get the correct tokenizer and set padding tokens based on the model family",
        "DS Config for ref model",
        "If it is ZeRO-3 then we use it for everything, otherwise assume we have enough memory for ref model",
        "End of DS config for ref model",
        "Copied from ../step2_reward_model_finetuning/main.py.",
        "Model bigscience/bloom-560m has large variance at ln_f.weight parameter",
        "This makes bf16 finetuning hard.",
        "In general, since we are replacing the model head, it makes sense to reset",
        "the LN that precedes it.",
        "Prepare the data",
        "DataLoaders creation:",
        "Split weights in two groups, one with weight decay and the other not.",
        "Train!",
        "Evaluate rewards on the validation set.",
        "For zero stage 3, each gpu only has a part of the model, so we need a special save function"
    ],
    "docstrings": [],
    "functions": [
        "parse_args",
        "get_batch_logps",
        "main",
        "evaluation"
    ],
    "classes": []
}