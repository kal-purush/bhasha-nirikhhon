{
    "identifiers": [
        "os",
        "sys",
        "pathlib",
        "Path",
        "typing",
        "torch",
        "transformers",
        "GenerationConfig",
        "LlamaTokenizer",
        "PretrainedConfig",
        "PreTrainedModel",
        "transformers",
        "modeling_outputs",
        "CausalLMOutputWithPast",
        "modules",
        "shared",
        "modules",
        "logging_colors",
        "logger",
        "modules",
        "relative_imports",
        "RelativeImport",
        "RelativeImport",
        "model",
        "ExLlama",
        "ExLlamaCache",
        "ExLlamaConfig",
        "PreTrainedModel",
        "config",
        "ExLlamaConfig",
        "PretrainedConfig",
        "config",
        "ExLlama",
        "ex_config",
        "GenerationConfig",
        "model_kwargs",
        "Dict",
        "Any",
        "input_ids",
        "kwargs",
        "input_ids",
        "kwargs",
        "property",
        "torch",
        "device",
        "torch",
        "device",
        "args",
        "kwargs",
        "len",
        "args",
        "kwargs",
        "kwargs",
        "tolist",
        "kwargs",
        "kwargs",
        "cache",
        "ExLlamaCache",
        "ex_model",
        "ex_model",
        "forward",
        "torch",
        "tensor",
        "seq",
        "dtype",
        "torch",
        "cache",
        "preprocess_only",
        "ex_model",
        "forward",
        "torch",
        "tensor",
        "seq",
        "dtype",
        "torch",
        "cache",
        "to",
        "device",
        "CausalLMOutputWithPast",
        "logits",
        "logits",
        "past_key_values",
        "cache",
        "use_cache",
        "classmethod",
        "cls",
        "pretrained_model_name_or_path",
        "Optional",
        "Union",
        "os",
        "PathLike",
        "model_args",
        "kwargs",
        "len",
        "model_args",
        "len",
        "kwargs",
        "isinstance",
        "pretrained_model_name_or_path",
        "Path",
        "pretrained_model_name_or_path",
        "Path",
        "shared",
        "args",
        "model_dir",
        "Path",
        "pretrained_model_name_or_path",
        "ExLlamaConfig",
        "pretrained_model_name_or_path",
        "ext",
        "pretrained_model_name_or_path",
        "glob",
        "ext",
        "len",
        "found",
        "found",
        "weight_path",
        "pretrained_model_name_or_path",
        "weight_path",
        "ExllamaHF",
        "config"
    ],
    "literals": [
        "\"repositories/exllama\"",
        "'input_ids'",
        "'no *args should be passed to forward'",
        "'use_cache'",
        "'input_ids'",
        "'past_key_values'",
        "'past_key_values'",
        "\"extra args is currently not supported\"",
        "f'{shared.args.model_dir}'",
        "'config.json'",
        "'.safetensors'",
        "'.pt'",
        "'.bin'",
        "f\"*{ext}\"",
        "f'could not find weight in \"{pretrained_model_name_or_path}\"'"
    ],
    "variables": [
        "ex_config",
        "ex_model",
        "generation_config",
        "use_cache",
        "seq",
        "cache",
        "cache",
        "logits",
        "pretrained_model_name_or_path",
        "pretrained_model_name_or_path",
        "config",
        "weight_path",
        "found",
        "weight_path",
        "config",
        "model_path",
        "config",
        "act_order",
        "config",
        "fused_attn",
        "config",
        "fused_mlp_thd"
    ],
    "comments": [
        "TODO: May cause problem on multi-gpu inference?",
        "TODO: Some decoding methods (such as Contrastive Search) may not work at this time",
        "from 'oobabooga/text-generation-webui/modules/exllama.py'",
        "This slowes down a bit but align better with autogptq generation.",
        "TODO: Should give user choice to tune the exllama config"
    ],
    "docstrings": [],
    "functions": [
        "_validate_model_class",
        "_validate_model_kwargs",
        "prepare_inputs_for_generation",
        "device",
        "__call__",
        "from_pretrained"
    ],
    "classes": [
        "ExllamaHF"
    ]
}