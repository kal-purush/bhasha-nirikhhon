{
    "identifiers": [
        "models",
        "EmbeddingModel",
        "k",
        "DEFAULT_EMBEDDING_SIZE",
        "eta",
        "DEFAULT_ETA",
        "epochs",
        "DEFAULT_EPOCH",
        "batches_count",
        "DEFAULT_BATCH_COUNT",
        "seed",
        "DEFAULT_SEED",
        "embedding_model_params",
        "optimizer",
        "DEFAULT_OPTIM",
        "optimizer_params",
        "DEFAULT_LR",
        "loss",
        "loss_params",
        "regularizer",
        "DEFAULT_REGULARIZER",
        "regularizer_params",
        "initializer",
        "DEFAULT_INITIALIZER",
        "initializer_params",
        "DEFAULT_XAVIER_IS_UNIFORM",
        "low_memory",
        "verbose",
        "DEFAULT_VERBOSE",
        "embedding_model_params",
        "k",
        "k",
        "embedding_model_params",
        "embedding_model_params",
        "i",
        "np",
        "sqrt",
        "n",
        "ksize",
        "n",
        "i",
        "i",
        "n",
        "i",
        "emb_img_width",
        "emb_img_height",
        "logger",
        "info",
        "msg",
        "ValueError",
        "msg",
        "emb_img_width",
        "emb_img_height",
        "emb_img_depth",
        "emb_img_width",
        "ksize",
        "emb_img_height",
        "ksize",
        "nfilters",
        "low_memory",
        "k",
        "k",
        "eta",
        "eta",
        "epochs",
        "epochs",
        "batches_count",
        "batches_count",
        "seed",
        "seed",
        "embedding_model_params",
        "embedding_model_params",
        "optimizer",
        "optimizer",
        "optimizer_params",
        "optimizer_params",
        "loss",
        "loss",
        "loss_params",
        "loss_params",
        "regularizer",
        "regularizer",
        "regularizer_params",
        "regularizer_params",
        "initializer",
        "initializer",
        "initializer_params",
        "initializer_params",
        "verbose",
        "verbose",
        "dealing_with_large_graphs",
        "tf",
        "variable_scope",
        "tf",
        "Variable",
        "trainable",
        "name",
        "tf",
        "assign",
        "tf_is_training",
        "tf",
        "assign",
        "tf_is_training",
        "embedding_model_params",
        "embedding_model_params",
        "embedding_model_params",
        "embedding_model_params",
        "embedding_model_params",
        "tf",
        "get_variable",
        "shape",
        "len",
        "ent_to_idx",
        "k",
        "initializer",
        "initializer",
        "get_tf_initializer",
        "dtype",
        "tf",
        "float32",
        "tf",
        "get_variable",
        "shape",
        "len",
        "rel_to_idx",
        "k",
        "initializer",
        "initializer",
        "get_tf_initializer",
        "dtype",
        "tf",
        "float32",
        "tf",
        "get_variable",
        "shape",
        "ksize",
        "ksize",
        "ninput",
        "nfilters",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "initializers",
        "he_normal",
        "seed",
        "seed",
        "dtype",
        "tf",
        "float32",
        "tf",
        "get_variable",
        "shape",
        "nfilters",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "zeros_initializer",
        "dtype",
        "tf",
        "float32",
        "tf",
        "get_variable",
        "shape",
        "dense_dim",
        "k",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "initializers",
        "he_normal",
        "seed",
        "seed",
        "dtype",
        "tf",
        "float32",
        "tf",
        "get_variable",
        "shape",
        "k",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "zeros_initializer",
        "dtype",
        "tf",
        "float32",
        "embedding_model_params",
        "embedding_model_params",
        "tf",
        "get_variable",
        "shape",
        "bn_input_shape",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "zeros_initializer",
        "tf",
        "get_variable",
        "shape",
        "bn_input_shape",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "ones_initializer",
        "tf",
        "get_variable",
        "shape",
        "nfilters",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "zeros_initializer",
        "tf",
        "get_variable",
        "shape",
        "nfilters",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "ones_initializer",
        "tf",
        "get_variable",
        "shape",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "zeros_initializer",
        "tf",
        "get_variable",
        "shape",
        "dtype",
        "tf",
        "float32",
        "trainable",
        "is_trainable",
        "initializer",
        "tf",
        "ones_initializer",
        "embedding_model_params",
        "tf",
        "get_variable",
        "shape",
        "len",
        "ent_to_idx",
        "initializer",
        "tf",
        "zeros_initializer",
        "trainable",
        "is_trainable",
        "dtype",
        "tf",
        "float32",
        "NotImplementedError",
        "X",
        "tf",
        "matmul",
        "X",
        "dense_W",
        "tf",
        "nn",
        "bias_add",
        "dense_B",
        "X",
        "tf",
        "nn",
        "conv2d",
        "X",
        "conv2d_W",
        "padding",
        "tf",
        "nn",
        "bias_add",
        "conv2d_B",
        "X",
        "beta",
        "gamma",
        "axes",
        "name",
        "tf",
        "nn",
        "moments",
        "X",
        "axes",
        "name",
        "tf",
        "train",
        "ExponentialMovingAverage",
        "decay",
        "ema",
        "apply",
        "batch_mean",
        "batch_var",
        "tf",
        "control_dependencies",
        "ema_apply_op",
        "tf",
        "identity",
        "batch_mean",
        "tf",
        "identity",
        "batch_var",
        "tf",
        "cond",
        "tf_is_training",
        "mean_var_with_update",
        "ema",
        "average",
        "batch_mean",
        "ema",
        "average",
        "batch_var",
        "tf",
        "nn",
        "batch_normalization",
        "X",
        "mean",
        "beta",
        "gamma",
        "name",
        "name",
        "normed",
        "X",
        "rate",
        "tf",
        "cond",
        "tf_is_training",
        "true_fn",
        "tf",
        "constant",
        "rate",
        "false_fn",
        "tf",
        "constant",
        "dtype",
        "tf",
        "float32",
        "tf",
        "nn",
        "dropout",
        "X",
        "rate",
        "dropout_rate",
        "name",
        "dataset_iterator",
        "y_true",
        "dataset_iterator",
        "get_next",
        "dealing_with_large_graphs",
        "NotImplementedError",
        "tf",
        "control_dependencies",
        "dependencies",
        "_lookup_embeddings",
        "x_pos_tf",
        "_fn",
        "e_s_pos",
        "e_p_pos",
        "e_o_pos",
        "loss",
        "apply",
        "y_true",
        "y_pred",
        "regularizer",
        "loss",
        "regularizer",
        "apply",
        "ent_emb",
        "rel_emb",
        "loss",
        "dealing_with_large_graphs",
        "sess_train",
        "run",
        "ent_emb",
        "sess_train",
        "run",
        "rel_emb",
        "sess_train",
        "run",
        "conv2d_W",
        "sess_train",
        "run",
        "conv2d_B",
        "sess_train",
        "run",
        "dense_W",
        "sess_train",
        "run",
        "dense_B",
        "embedding_model_params",
        "sess_train",
        "run",
        "bn_input_beta",
        "sess_train",
        "run",
        "bn_input_gamma",
        "sess_train",
        "run",
        "bn_conv_beta",
        "sess_train",
        "run",
        "bn_conv_gamma",
        "sess_train",
        "run",
        "bn_dense_beta",
        "sess_train",
        "run",
        "bn_dense_gamma",
        "embedding_model_params",
        "sess_train",
        "run",
        "bias",
        "output_mapping",
        "params_dict",
        "NotImplementedError",
        "np",
        "ceil",
        "len",
        "ent_to_idx",
        "batches_count",
        "len",
        "ent_to_idx",
        "ENTITY_THRESHOLD",
        "logger",
        "warning",
        "format",
        "len",
        "ent_to_idx",
        "logger",
        "warning",
        "logger",
        "warning",
        "dealing_with_large_graphs",
        "tf",
        "variable_scope",
        "tf",
        "Variable",
        "trainable",
        "name",
        "tf",
        "assign",
        "tf_is_training",
        "tf",
        "assign",
        "tf_is_training",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "name",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "name",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "embedding_model_params",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "embedding_model_params",
        "tf",
        "Variable",
        "trained_model_params",
        "dtype",
        "tf",
        "float32",
        "trained_model_params",
        "NotImplementedError",
        "e_s",
        "e_p",
        "e_o",
        "embedding_model_params",
        "tf",
        "stack",
        "e_s",
        "e_p",
        "axis",
        "name",
        "tf",
        "reshape",
        "stacked_emb",
        "shape",
        "tf",
        "shape",
        "stacked_emb",
        "embedding_model_params",
        "embedding_model_params",
        "name",
        "tf",
        "reshape",
        "e_s",
        "shape",
        "tf",
        "shape",
        "e_s",
        "embedding_model_params",
        "embedding_model_params",
        "tf",
        "reshape",
        "e_s",
        "shape",
        "tf",
        "shape",
        "e_p",
        "embedding_model_params",
        "embedding_model_params",
        "tf",
        "stack",
        "e_s_img",
        "e_p_img",
        "axis",
        "name",
        "inputs",
        "embedding_model_params",
        "_batch_norm",
        "x",
        "bn_input_beta",
        "bn_input_gamma",
        "axes",
        "name",
        "embedding_model_params",
        "_dropout",
        "x",
        "rate",
        "embedding_model_params",
        "_conv2d",
        "x",
        "embedding_model_params",
        "_batch_norm",
        "x",
        "bn_conv_beta",
        "bn_conv_gamma",
        "axes",
        "name",
        "tf",
        "nn",
        "relu",
        "x",
        "name",
        "embedding_model_params",
        "_dropout",
        "x",
        "rate",
        "embedding_model_params",
        "tf",
        "reshape",
        "x",
        "shape",
        "tf",
        "shape",
        "x",
        "embedding_model_params",
        "_dense",
        "x",
        "embedding_model_params",
        "_dropout",
        "x",
        "rate",
        "embedding_model_params",
        "embedding_model_params",
        "_batch_norm",
        "x",
        "bn_dense_beta",
        "bn_dense_gamma",
        "axes",
        "name",
        "tf",
        "nn",
        "relu",
        "x",
        "name",
        "tf",
        "matmul",
        "x",
        "tf",
        "transpose",
        "ent_emb",
        "name",
        "embedding_model_params",
        "tf",
        "add",
        "x",
        "bias",
        "name",
        "x",
        "scores",
        "entities",
        "embedding_type",
        "is_fitted",
        "logger",
        "error",
        "msg",
        "RuntimeError",
        "msg",
        "embedding_type",
        "trained_model_params",
        "ent_to_idx",
        "embedding_type",
        "trained_model_params",
        "rel_to_idx",
        "format",
        "embedding_type",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "np",
        "vectorize",
        "lookup_dict",
        "get",
        "entities",
        "emb_list",
        "idxs",
        "logger",
        "info",
        "logger",
        "info",
        "format",
        "train_dataset_handle",
        "get_size",
        "iter",
        "train_dataset_handle",
        "get_next_train_batch",
        "batch_size",
        "i",
        "batches_count",
        "next",
        "batch_iterator",
        "dealing_with_large_graphs",
        "NotImplementedError",
        "out_onehot",
        "StopIteration",
        "logger",
        "info",
        "format",
        "i",
        "X",
        "early_stopping",
        "early_stopping_params",
        "isinstance",
        "X",
        "np",
        "ndarray",
        "ConvEDatasetAdapter",
        "low_memory",
        "low_memory",
        "train_dataset_handle",
        "set_data",
        "X",
        "isinstance",
        "X",
        "AmpligraphDatasetAdapter",
        "X",
        "format",
        "X",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "rel_to_idx",
        "ent_to_idx",
        "train_dataset_handle",
        "generate_mappings",
        "len",
        "ent_to_idx",
        "ENTITY_THRESHOLD",
        "logger",
        "warning",
        "format",
        "len",
        "ent_to_idx",
        "logger",
        "warning",
        "logger",
        "warning",
        "early_stopping",
        "isinstance",
        "optimizer",
        "SGDOptimizer",
        "initializer",
        "get_np_initializer",
        "len",
        "ent_to_idx",
        "internal_k",
        "train_dataset_handle",
        "map_data",
        "is_fitted",
        "tf",
        "reset_default_graph",
        "check_random_state",
        "seed",
        "tf",
        "random",
        "set_random_seed",
        "seed",
        "tf",
        "Session",
        "config",
        "tf_config",
        "np",
        "ceil",
        "train_dataset_handle",
        "get_size",
        "batches_count",
        "batch_size",
        "len",
        "ent_to_idx",
        "ENTITY_THRESHOLD",
        "logger",
        "warning",
        "format",
        "batch_size",
        "_initialize_parameters",
        "train_dataset_handle",
        "generate_output_mapping",
        "dataset_type",
        "train_dataset_handle",
        "set_output_mapping",
        "output_mapping",
        "train_dataset_handle",
        "generate_onehot_outputs",
        "dataset_type",
        "tf",
        "data",
        "Dataset",
        "from_generator",
        "_training_data_generator",
        "output_types",
        "tf",
        "int32",
        "tf",
        "float32",
        "output_shapes",
        "len",
        "ent_to_idx",
        "dataset",
        "repeat",
        "prefetch",
        "prefetch_batches",
        "dataset",
        "make_one_shot_iterator",
        "loss",
        "get_state",
        "batch_size",
        "eta",
        "loss",
        "_set_hyperparams",
        "len",
        "ent_to_idx",
        "_get_model_loss",
        "dataset_iterator",
        "optimizer",
        "minimize",
        "loss",
        "early_stopping_params",
        "early_stopping",
        "_initialize_early_stopping",
        "sess_train",
        "run",
        "tf",
        "tables_initializer",
        "sess_train",
        "run",
        "tf",
        "global_variables_initializer",
        "sess_train",
        "run",
        "set_training_true",
        "ent_emb",
        "assign",
        "tf",
        "clip_by_norm",
        "ent_emb",
        "clip_norm",
        "axes",
        "rel_emb",
        "assign",
        "tf",
        "clip_by_norm",
        "rel_emb",
        "clip_norm",
        "axes",
        "embedding_model_params",
        "get",
        "DEFAULT_NORMALIZE_EMBEDDINGS",
        "sess_train",
        "run",
        "normalize_rel_emb_op",
        "sess_train",
        "run",
        "normalize_ent_emb_op",
        "tqdm",
        "epochs",
        "disable",
        "verbose",
        "unit",
        "epoch",
        "epoch_iterator_with_progress",
        "batch",
        "batches_count",
        "optimizer",
        "update_feed_dict",
        "feed_dict",
        "batch",
        "epoch",
        "dealing_with_large_graphs",
        "NotImplementedError",
        "sess_train",
        "run",
        "loss",
        "train",
        "feed_dict",
        "feed_dict",
        "np",
        "isnan",
        "loss_batch",
        "np",
        "isinf",
        "loss_batch",
        "format",
        "loss_batch",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "losses",
        "append",
        "loss_batch",
        "embedding_model_params",
        "get",
        "DEFAULT_NORMALIZE_EMBEDDINGS",
        "sess_train",
        "run",
        "normalize_ent_emb_op",
        "verbose",
        "format",
        "sum",
        "losses",
        "batch_size",
        "batches_count",
        "early_stopping",
        "early_stopping_best_value",
        "msg",
        "format",
        "early_stopping_criteria",
        "early_stopping_best_value",
        "logger",
        "debug",
        "msg",
        "epoch_iterator_with_progress",
        "set_description",
        "msg",
        "early_stopping",
        "sess_train",
        "run",
        "set_training_false",
        "_perform_early_stopping_test",
        "epoch",
        "_end_training",
        "sess_train",
        "run",
        "set_training_true",
        "_save_trained_params",
        "_end_training",
        "e",
        "_end_training",
        "e",
        "mode",
        "logger",
        "debug",
        "format",
        "mode",
        "is_filtered",
        "is_filtered",
        "partial",
        "eval_dataset_handle",
        "get_next_batch_with_filter",
        "batch_size",
        "dataset_type",
        "mode",
        "partial",
        "eval_dataset_handle",
        "get_next_eval_batch",
        "batch_size",
        "dataset_type",
        "mode",
        "iter",
        "test_generator",
        "next",
        "batch_iterator",
        "StopIteration",
        "out_onehot_filter",
        "mode",
        "logger",
        "debug",
        "format",
        "mode",
        "tf",
        "data",
        "Dataset",
        "from_generator",
        "partial",
        "_test_generator",
        "mode",
        "mode",
        "output_types",
        "tf",
        "int32",
        "tf",
        "float32",
        "output_shapes",
        "len",
        "ent_to_idx",
        "dataset",
        "repeat",
        "dataset",
        "prefetch",
        "dataset",
        "make_one_shot_iterator",
        "X_test_tf",
        "X_test_filter_tf",
        "dataset_iter",
        "get_next",
        "dealing_with_large_graphs",
        "NotImplementedError",
        "_lookup_embeddings",
        "X_test_tf",
        "tf",
        "sigmoid",
        "tf",
        "squeeze",
        "_fn",
        "e_s",
        "e_p",
        "e_o",
        "name",
        "tf",
        "gather",
        "scores",
        "indices",
        "X_test_tf",
        "name",
        "tf",
        "boolean_mask",
        "scores",
        "tf",
        "cast",
        "X_test_filter_tf",
        "tf",
        "tf",
        "reduce_sum",
        "tf",
        "cast",
        "scores",
        "score_positive",
        "tf",
        "int32",
        "tf",
        "reduce_sum",
        "tf",
        "cast",
        "scores_filtered",
        "score_positive",
        "tf",
        "int32",
        "tf",
        "subtract",
        "total_rank",
        "filter_rank",
        "name",
        "early_stopping_params",
        "isinstance",
        "x_valid",
        "np",
        "ndarray",
        "x_valid",
        "ndim",
        "np",
        "shape",
        "x_valid",
        "format",
        "np",
        "shape",
        "x_valid",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "to_idx",
        "x_valid",
        "ent_to_idx",
        "ent_to_idx",
        "rel_to_idx",
        "rel_to_idx",
        "train_dataset_handle",
        "set_data",
        "x_valid",
        "mapped_status",
        "train_dataset_handle",
        "logger",
        "debug",
        "isinstance",
        "x_valid",
        "AmpligraphDatasetAdapter",
        "eval_dataset_handle",
        "data_exists",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "x_valid",
        "format",
        "x_valid",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "KeyError",
        "logger",
        "error",
        "msg",
        "KeyError",
        "msg",
        "early_stopping_params",
        "get",
        "DEFAULT_CRITERIA_EARLY_STOPPING",
        "early_stopping_criteria",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "early_stopping_params",
        "isinstance",
        "x_filter",
        "np",
        "ndarray",
        "x_filter",
        "ndim",
        "np",
        "shape",
        "x_filter",
        "format",
        "np",
        "shape",
        "x_filter",
        "logger",
        "error",
        "msg",
        "ValueError",
        "msg",
        "to_idx",
        "x_filter",
        "ent_to_idx",
        "ent_to_idx",
        "rel_to_idx",
        "rel_to_idx",
        "eval_dataset_handle",
        "set_filter",
        "x_filter",
        "mapped_status",
        "set_filter_for_eval",
        "KeyError",
        "logger",
        "debug",
        "_initialize_eval_graph",
        "X",
        "from_idx",
        "is_fitted",
        "logger",
        "error",
        "msg",
        "RuntimeError",
        "msg",
        "ConvEDatasetAdapter",
        "low_memory",
        "low_memory",
        "dataset_handle",
        "use_mappings",
        "rel_to_idx",
        "ent_to_idx",
        "dataset_handle",
        "set_data",
        "X",
        "mapped_status",
        "from_idx",
        "dataset_handle",
        "sess_predict",
        "tf",
        "reset_default_graph",
        "check_random_state",
        "seed",
        "tf",
        "random",
        "set_random_seed",
        "seed",
        "_load_model_from_trained_params",
        "_initialize_eval_graph",
        "tf",
        "Session",
        "sess",
        "run",
        "tf",
        "tables_initializer",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "sess",
        "sess_predict",
        "run",
        "set_training_false",
        "eval_dataset_handle",
        "set_output_mapping",
        "output_mapping",
        "eval_dataset_handle",
        "generate_onehot_outputs",
        "dataset_type",
        "i",
        "tqdm",
        "eval_dataset_handle",
        "get_size",
        "sess_predict",
        "run",
        "score_positive",
        "eval_config",
        "get",
        "DEFAULT_PROTOCOL_EVAL",
        "scores",
        "extend",
        "score",
        "scores",
        "append",
        "score",
        "scores",
        "dataset_handle",
        "is_fitted",
        "logger",
        "error",
        "msg",
        "RuntimeError",
        "msg",
        "dataset_handle",
        "sess_predict",
        "tf",
        "reset_default_graph",
        "check_random_state",
        "seed",
        "tf",
        "random",
        "set_random_seed",
        "seed",
        "_load_model_from_trained_params",
        "_initialize_eval_graph",
        "mode",
        "tf",
        "Session",
        "sess",
        "run",
        "tf",
        "tables_initializer",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "sess",
        "sess_predict",
        "run",
        "set_training_false",
        "i",
        "tqdm",
        "eval_dataset_handle",
        "get_size",
        "sess_predict",
        "run",
        "rank",
        "eval_config",
        "get",
        "DEFAULT_PROTOCOL_EVAL",
        "ranks",
        "append",
        "rank",
        "ranks",
        "append",
        "rank",
        "ranks"
    ],
    "literals": [
        "'conv_filters'",
        "'conv_kernel_size'",
        "'dropout_embed'",
        "'dropout_conv'",
        "'dropout_dense'",
        "'use_bias'",
        "'use_batchnorm'",
        "'corrupt_sides'",
        "'o'",
        "'is_trainable'",
        "'checkerboard'",
        "'lr'",
        "'bce'",
        "'scoring_strategy'",
        "'1-N'",
        "'label_weighting'",
        "'label_smoothing'",
        "'uniform'",
        "'checkerboard'",
        "'conv_kernel_size'",
        "'conv_filters'",
        "'Unable to determine factor pairs for embedding reshape. Choose a smaller convolution kernel size, '",
        "'or a larger embedding dimension.'",
        "'embed_image_width'",
        "'embed_image_height'",
        "'embed_image_depth'",
        "'dense_dim'",
        "'meta'",
        "'is_training'",
        "'is_trainable'",
        "'conv_filters'",
        "'embed_image_depth'",
        "'conv_kernel_size'",
        "'dense_dim'",
        "'ent_emb'",
        "'rel_emb'",
        "'conv2d_weights'",
        "'conv2d_bias'",
        "'dense_weights'",
        "'dense_bias'",
        "'use_batchnorm'",
        "'checkerboard'",
        "'bn_input_beta'",
        "'bn_input_gamma'",
        "'bn_conv_beta'",
        "'bn_conv_gamma'",
        "'bn_dense_beta'",
        "'bn_dense_gamma'",
        "'use_bias'",
        "'activation_bias'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'VALID'",
        "'moments'",
        "'dropout_dense'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'ent_emb'",
        "'rel_emb'",
        "'conv2d_W'",
        "'conv2d_B'",
        "'dense_W'",
        "'dense_B'",
        "'use_batchnorm'",
        "'bn_input_beta'",
        "'bn_input_gamma'",
        "'bn_conv_beta'",
        "'bn_conv_gamma'",
        "'bn_dense_beta'",
        "'bn_dense_gamma'",
        "'use_bias'",
        "'bias'",
        "'output_mapping'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'Your graph has a large number of distinct entities. '",
        "'Found {} distinct entities'",
        "'Changing the variable loading strategy to use lazy loading of variables...'",
        "'Evaluation would take longer than usual.'",
        "'meta'",
        "'is_training'",
        "'ent_emb'",
        "'ent_emb'",
        "'rel_emb'",
        "'rel_emb'",
        "'conv2d_W'",
        "'conv2d_B'",
        "'dense_W'",
        "'dense_B'",
        "'use_batchnorm'",
        "'bn_input_beta'",
        "'bn_input_gamma'",
        "'bn_conv_beta'",
        "'bn_conv_gamma'",
        "'bn_dense_beta'",
        "'bn_dense_gamma'",
        "'use_bias'",
        "'bias'",
        "'output_mapping'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'checkerboard'",
        "'stacked_embeddings'",
        "'embed_image_height'",
        "'embed_image_width'",
        "'embed_image'",
        "'embed_image_height'",
        "'embed_image_width'",
        "'embed_image_height'",
        "'embed_image_width'",
        "'embed_image'",
        "'use_batchnorm'",
        "'input'",
        "'dropout_embed'",
        "'dropout_embed'",
        "'use_batchnorm'",
        "'conv'",
        "'conv_relu'",
        "'dropout_conv'",
        "'dropout_conv'",
        "'dense_dim'",
        "'dropout_dense'",
        "'dropout_dense'",
        "'use_batchnorm'",
        "'dense'",
        "'dense_relu'",
        "'matmul'",
        "'use_bias'",
        "'add_bias'",
        "'entity'",
        "'Model has not been fitted.'",
        "'entity'",
        "'ent_emb'",
        "'relation'",
        "'rel_emb'",
        "'Invalid entity type: {}'",
        "'Initializing training data generator.'",
        "'Size of training data: {}'",
        "'train'",
        "'train'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'Hit stop iteration at batch {}'",
        "\"train\"",
        "'Invalid type for input X. Expected ndarray/AmpligraphDataset object, got {}'",
        "'Your graph has a large number of distinct entities. '",
        "'Found {} distinct entities'",
        "'Changing the variable initialization strategy.'",
        "'Changing the strategy to use lazy loading of variables...'",
        "'Early stopping not supported for large graphs'",
        "\"This mode works well only with SGD optimizer with decay (read docs for details). \"",
        "\"Kindly change the optimizer and restart the experiment\"",
        "\"train\"",
        "'Only {} embeddings would be loaded in memory per batch...'",
        "'train'",
        "'train'",
        "'require_same_size_pos_neg'",
        "'num_entities'",
        "'normalize_ent_emb'",
        "'epoch'",
        "'ConvE not implemented when dealing with large graphs (yet)'",
        "'Loss is {}. Please change the hyperparameters.'",
        "'normalize_ent_emb'",
        "'Average Loss: {:10f}'",
        "' â€” Best validation ({}): {:5f}'",
        "'Initializing test generator [Mode: {}, Filtered: {}]'",
        "'test'",
        "'Initializing eval graph [mode: {}]'",
        "'ConvE not implemented with large graphs (yet)'",
        "'sigmoid_scores'",
        "'score_positive'",
        "'rank'",
        "'x_valid'",
        "'Invalid size for input x_valid. Expected (n,3):  got {}'",
        "\"valid\"",
        "'Initialized eval_dataset from train_dataset'",
        "'valid'",
        "'Dataset `valid` has not been set in the DatasetAdapter.'",
        "'Initialized eval_dataset from AmpligraphDatasetAdapter'",
        "'Invalid type for input X. Expected np.ndarray or AmpligraphDatasetAdapter object, \\\n                       got {}'",
        "'x_valid must be passed for early fitting.'",
        "'criteria'",
        "'hits10'",
        "'hits1'",
        "'hits3'",
        "'mrr'",
        "'Unsupported early stopping criteria.'",
        "'x_filter'",
        "'Invalid size for input x_valid. Expected (n,3):  got {}'",
        "'x_filter not found in early_stopping_params.'",
        "\"valid\"",
        "'Model has not been fitted.'",
        "'test'",
        "'test'",
        "'test'",
        "'default_protocol'",
        "'Model has not been fitted.'",
        "'test'",
        "'test'",
        "'default_protocol'"
    ],
    "variables": [
        "n",
        "emb_img_depth",
        "n",
        "emb_img_depth",
        "ksize",
        "nfilters",
        "emb_img_width",
        "emb_img_height",
        "emb_img_width",
        "emb_img_height",
        "msg",
        "embedding_model_params",
        "embedding_model_params",
        "embedding_model_params",
        "embedding_model_params",
        "low_memory",
        "tf_is_training",
        "set_training_true",
        "set_training_false",
        "is_trainable",
        "nfilters",
        "ninput",
        "ksize",
        "dense_dim",
        "ent_emb",
        "rel_emb",
        "conv2d_W",
        "conv2d_B",
        "dense_W",
        "dense_B",
        "bn_input_shape",
        "bn_input_shape",
        "bn_input_beta",
        "bn_input_gamma",
        "bn_conv_beta",
        "bn_conv_gamma",
        "bn_dense_beta",
        "bn_dense_gamma",
        "bias",
        "batch_mean",
        "batch_var",
        "ema",
        "ema_apply_op",
        "mean",
        "normed",
        "dropout_rate",
        "x_pos_tf",
        "dependencies",
        "e_s_pos",
        "e_p_pos",
        "e_o_pos",
        "y_pred",
        "loss",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "params_dict",
        "trained_model_params",
        "batch_size",
        "dealing_with_large_graphs",
        "tf_is_training",
        "set_training_true",
        "set_training_false",
        "ent_emb",
        "rel_emb",
        "conv2d_W",
        "conv2d_B",
        "dense_W",
        "dense_B",
        "bn_input_beta",
        "bn_input_gamma",
        "bn_conv_beta",
        "bn_conv_gamma",
        "bn_dense_beta",
        "bn_dense_gamma",
        "bias",
        "output_mapping",
        "stacked_emb",
        "inputs",
        "e_s_img",
        "e_p_img",
        "inputs",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "scores",
        "msg",
        "emb_list",
        "lookup_dict",
        "emb_list",
        "lookup_dict",
        "msg",
        "idxs",
        "batch_iterator",
        "out_onehot",
        "train_dataset_handle",
        "train_dataset_handle",
        "train_dataset_handle",
        "msg",
        "dealing_with_large_graphs",
        "prefetch_batches",
        "ent_emb_cpu",
        "rnd",
        "sess_train",
        "batch_size",
        "batch_size",
        "output_mapping",
        "dataset",
        "prefetch_batches",
        "dataset",
        "dataset_iterator",
        "batch_size",
        "loss",
        "train",
        "early_stopping_params",
        "normalize_ent_emb_op",
        "normalize_rel_emb_op",
        "epoch_iterator_with_progress",
        "losses",
        "feed_dict",
        "loss_batch",
        "_",
        "msg",
        "msg",
        "test_generator",
        "test_generator",
        "batch_iterator",
        "out_onehot_filter",
        "dataset",
        "dataset",
        "dataset",
        "dataset_iter",
        "test_dependency",
        "e_s",
        "e_p",
        "e_o",
        "scores",
        "score_positive",
        "scores_filtered",
        "total_rank",
        "filter_rank",
        "rank",
        "x_valid",
        "msg",
        "x_valid",
        "eval_dataset_handle",
        "msg",
        "eval_dataset_handle",
        "msg",
        "msg",
        "early_stopping_criteria",
        "msg",
        "early_stopping_best_value",
        "early_stopping_stop_counter",
        "x_filter",
        "msg",
        "x_filter",
        "msg",
        "dataset_handle",
        "eval_dataset_handle",
        "rnd",
        "sess",
        "sess_predict",
        "scores",
        "score",
        "msg",
        "eval_dataset_handle",
        "rnd",
        "sess",
        "sess_predict",
        "ranks",
        "rank"
    ],
    "comments": [
        "Find factor pairs (i,j) of concatenated embedding dimensions, where min(i,j) >= conv_kernel_size",
        "Calculate dense dimension",
        "self.saver = tf.train.Saver()",
        "training input placeholder",
        "list of dependent ops that need to be evaluated before computing the loss",
        "if the graph is large",
        "run the dependencies",
        "look up embeddings from input training triples",
        "Get positive predictions",
        "Label smoothing and/or weighting is applied within Loss class",
        "Generate the batch size based on entity length and batch_count",
        "Inputs",
        "create iterator to iterate over the train batches",
        "logger.info('gen - batch {} out {} onehot {}'.format(i, out.shape, out_onehot.shape))",
        "If large graph, load batch_size*2 entities on GPU memory",
        "try-except block is mainly to handle clean up in case of exception or manual stop in jupyter notebook",
        "Adapt the numpy data in the internal format - to generalize",
        "create internal IDs mappings",
        "CPU matrix of embeddings",
        "This is useful when we re-fit the same model (e.g. retraining in model selection)",
        "Output mapping is dict of (s, p) to list of existing object triple indices",
        "init tf graph/dataflow for training",
        "init variables (model parameters to be learned - i.e. the embeddings)",
        "Required for label smoothing",
        "early stopping",
        "Entity embeddings normalization",
        "Dependencies that need to be run before scoring",
        "For large graphs",
        "Compute scores for positive",
        "Score of positive triple",
        "Score of every other positive sample for <s, p>, excluding positive sample",
        "this is to remove the positives from output",
        "Total rank over all possible entities",
        "Rank over positive triples",
        "Rank of positive sample, with other positives filtered out",
        "store the validation data in the data handler",
        "If the filter has already been set in the dataset adapter then just pass x_filter = True",
        "set the filter triples in the data handler",
        "set the flag to perform filtering",
        "initialize evaluation graph in validation mode i.e. to use validation set",
        "adapt the data with conve adapter for internal use",
        "build tf graph for predictions",
        "build tf graph for predictions"
    ],
    "docstrings": [
        "\"\"\" Convolutional 2D Knowledge Graph Embeddings\n\n    The ConvE model :cite:`Dettmers2016`:\n\n    .. math::\n\n        f(vec(f([\\overline{e_s};\\overline{r_r}] * \\omega)) W ) e_o\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from ampligraph.latent_features import conve\n    >>> model = conve(batches_count=1, seed=555, epochs=20, k=10,       # TODO\n    >>>              loss='bce', loss_params={},\n    >>>              regularizer='LP', regularizer_params={'lambda':0.1})\n    >>>\n    >>> X = np.array([['a', 'y', 'b'],\n    >>>               ['b', 'y', 'a'],\n    >>>               ['a', 'y', 'c'],\n    >>>               ['c', 'y', 'a'],\n    >>>               ['a', 'y', 'd'],\n    >>>               ['c', 'y', 'd'],\n    >>>               ['b', 'y', 'c'],\n    >>>               ['f', 'y', 'e']])\n    >>> model.fit(X)\n    >>> model.predict(np.array([['f', 'y', 'e'], ['b', 'y', 'd']]), get_ranks=True)\n    ([-0.06213863, 0.01563319], [13, 3])\n    >>> model.get_embeddings(['f','e'], embedding_type='entity')\n        array([[ 0.17335348,  0.15826802,  0.24862595,  0.21404941, -0.00968813,\n         0.06185953, -0.24956754,  0.01114257, -0.1038138 ,  0.40461722,\n        -0.12298391, -0.10997348,  0.28220937,  0.34238952,  0.58363295,\n         0.03315138, -0.37830347,  0.13480346,  0.49922466, -0.26328272],\n        [-0.19098252,  0.20133668,  0.04635337,  0.4364128 ,  0.07014864,\n         0.5713923 ,  0.28131518,  0.31721675, -0.06636801,  0.2848032 ,\n        -0.2121708 ,  0.56917167, -0.05311433,  0.03093261,  0.01571475,\n        -0.11373658,  0.29417998,  0.34896123,  0.22993243, -0.5499186 ]],\n        dtype=float32)\n\n    \"\"\"",
        "\"\"\"Initialize an EmbeddingModel\n\n        Also creates a new Tensorflow session for training.\n\n        Parameters\n        ----------\n        k : int\n            Embedding space dimensionality.\n\n        eta : int\n            The number of negatives that must be generated at runtime during training for each positive.\n\n        epochs : int\n            The iterations of the training loop.\n\n        batches_count : int\n            The number of batches in which the training set must be split during the training loop.\n\n        seed : int\n            The seed used by the internal random numbers generator.\n\n        embedding_model_params : dict\n            ConvE-specific hyperparams:\n            - **conv_filters** - Number of convolution feature maps.\n            - **conv_kernel_size** - Convolution kernel size.\n            - **dropout_embed** - Dropout on the embedding layer.\n            - **dropout_conv** -  Dropout on the convolution maps.\n            - **dropout_dense** - Dropout on the dense layer.\n            - **use_bias** - Use bias layer.\n            - **use_batchnorm** - Use batch normalization after input, convolution, and dense layers.\n\n        optimizer : string\n            The optimizer used to minimize the loss function. Choose between\n            'sgd', 'adagrad', 'adam', 'momentum'.\n\n        optimizer_params : dict\n            Arguments specific to the optimizer, passed as a dictionary.\n\n            Supported keys:\n\n            - **'lr'** (float): learning rate (used by all the optimizers). Default: 0.1.\n            - **'momentum'** (float): learning momentum (only used when ``optimizer=momentum``). Default: 0.9.\n\n            Example: ``optimizer_params={'lr': 0.01}``\n\n        loss : string\n            The type of loss function to use during training.\n\n            - ``bce``  the model will use binary cross entropy loss function.\n\n        loss_params : dict\n            Dictionary of loss-specific hyperparameters. See :ref:`loss functions <loss>`\n            documentation for additional details.\n\n            Supported keys:\n\n            - **'lr'** (float): learning rate (used by all the optimizers). Default: 0.1.\n            - **'momentum'** (float): learning momentum (only used when ``optimizer=momentum``). Default: 0.9.\n            - **'label_smoothing'** (float): applies label smoothing to onehot outputs. Default: None.\n            - **'label_weighting'** (bool): applies label weighting to onehot outputs. Default: False\n\n            Example: ``optimizer_params={'lr': 0.01, 'label_smoothing': 0.1}``\n\n        regularizer : string\n            The regularization strategy to use with the loss function.\n\n            - ``None``: the model will not use any regularizer (default)\n            - ``LP``: the model will use L1, L2 or L3 based on the value of ``regularizer_params['p']`` (see below).\n\n        regularizer_params : dict\n            Dictionary of regularizer-specific hyperparameters. See the\n            :ref:`regularizers <ref-reg>`\n            documentation for additional details.\n\n            Example: ``regularizer_params={'lambda': 1e-5, 'p': 2}`` if ``regularizer='LP'``.\n\n        initializer : string\n            The type of initializer to use.\n\n            - ``normal``: The embeddings will be initialized from a normal distribution\n            - ``uniform``: The embeddings will be initialized from a uniform distribution\n            - ``xavier``: The embeddings will be initialized using xavier strategy (default)\n\n        initializer_params : dict\n            Dictionary of initializer-specific hyperparameters. See the\n            :ref:`initializer <ref-init>`\n            documentation for additional details.\n\n            Example: ``initializer_params={'mean': 0, 'std': 0.001}`` if ``initializer='normal'``.\n\n        verbose : bool\n            Verbose mode.\n        \"\"\"",
        "\"\"\"Initialize parameters of the model.\n\n            This function creates and initializes entity and relation embeddings (with size k).\n            If the graph is large, then it loads only the required entity embeddings (max:batch_size*2)\n            and all relation embeddings.\n            Overload this function if the parameters needs to be initialized differently.\n        \"\"\"",
        "\"\"\"\n        Internal function to create dense layer. Note this does not create new variables.\n        Parameters\n        ----------\n        X: tf.Variable\n\n        Returns\n        -------\n\n        \"\"\"",
        "\"\"\"\n        Internal function to create conv2d layer. Note this does not create new variables.\n        Parameters\n        ----------\n        X\n\n        Returns\n        -------\n\n        \"\"\"",
        "\"\"\" Internal function to create batch normalization layer. Note this does not create new variables.\n\n        Parameters\n        ----------\n        X : tf.Variable\n        beta : tf.Variable\n        gamma : tf.Variable\n        axes : axes to perform normalization over\n        name : string\n\n        Returns\n        -------\n\n        \"\"\"",
        "\"\"\"\n        Internal function to create dropout layer.\n        Parameters\n        ----------\n        X\n        rate\n\n        Returns\n        -------\n\n        \"\"\"",
        "\"\"\"Get the current loss including loss due to regularization.\n        This function must be overridden if the model uses combination of different losses(eg: VAE).\n\n        Parameters\n        ----------\n        dataset_iterator : tf.data.Iterator\n            Dataset iterator.\n\n        Returns\n        -------\n        loss : tf.Tensor\n            The loss value that must be minimized.\n        \"\"\"",
        "\"\"\"After model fitting, save all the trained parameters in trained_model_params in some order.\n        The order would be useful for loading the model.\n        This method must be overridden if the model has any other parameters (apart from entity-relation embeddings).\n        \"\"\"",
        "\"\"\"Load the model from trained params.\n            While restoring make sure that the order of loaded parameters match the saved order.\n            It's the duty of the embedding model to load the variables correctly.\n            This method must be overridden if the model has any other parameters (apart from entity-relation embeddings)\n            This function also set's the evaluation mode to do lazy loading of variables based on the number of\n            distinct entities present in the graph.\n        \"\"\"",
        "\"\"\"The ConvE scoring function.\n\n            The function implements the scoring function as defined by\n            .. math::\n\n                f(vec(f([\\overline{e_s};\\overline{r_r}] * \\omega)) W ) e_o\n\n            Additional details for equivalence of the models available in :cite:`Dettmers2016`.\n\n\n        Parameters\n        ----------\n        e_s : Tensor, shape [n]\n            The embeddings of a list of subjects.\n        e_p : Tensor, shape [n]\n            The embeddings of a list of predicates.\n        e_o : Tensor, shape [n]\n            The embeddings of a list of objects.\n\n        Returns\n        -------\n        score : TensorFlow operation\n            The operation corresponding to the ConvE scoring function.\n\n        \"\"\"",
        "\"\"\"Get the embeddings of entities or relations.\n\n        .. Note ::\n            Use :meth:`ampligraph.utils.create_tensorboard_visualizations` to visualize the embeddings with TensorBoard.\n\n        Parameters\n        ----------\n        entities : array-like, dtype=int, shape=[n]\n            The entities (or relations) of interest. Element of the vector\n            must be the original string literals, and not internal IDs.\n        embedding_type : string\n            If 'entity', ``entities`` argument will be considered as a list of knowledge graph entities (i.e. nodes).\n            If set to 'relation', they will be treated as relation types instead (i.e. predicates).\n\n        Returns\n        -------\n        embeddings : ndarray, shape [n, k]\n            An array of k-dimensional embeddings.\n\n        \"\"\"",
        "\"\"\"Generates the training data for ConvE model.\n        \"\"\"",
        "\"\"\"Train a ConvE (with optional early stopping).\n\n        The model is trained on a training set X using the training protocol\n        described in :cite:`Dettmers2016`.\n\n        Parameters\n        ----------\n        X : ndarray (shape [n, 3]) or object of ConvEDatasetAdapter\n            Numpy array of training triples OR handle of Dataset adapter which would help retrieve data.\n        early_stopping: bool\n            Flag to enable early stopping (default:``False``)\n        early_stopping_params: dictionary\n            Dictionary of hyperparameters for the early stopping heuristics.\n\n            The following string keys are supported:\n\n                - **'x_valid'**: ndarray (shape [n, 3]) or object of AmpligraphDatasetAdapter :\n                                 Numpy array of validation triples OR handle of Dataset adapter which\n                                 would help retrieve data.\n                - **'criteria'**: string : criteria for early stopping 'hits10', 'hits3', 'hits1' or 'mrr'(default).\n                - **'x_filter'**: ndarray, shape [n, 3] : Positive triples to use as filter if a 'filtered' early\n                                  stopping criteria is desired (i.e. filtered-MRR if 'criteria':'mrr').\n                                  Note this will affect training time (no filter by default).\n                                  If the filter has already been set in the adapter, pass True\n                - **'burn_in'**: int : Number of epochs to pass before kicking in early stopping (default: 100).\n                - **check_interval'**: int : Early stopping interval after burn-in (default:10).\n                - **'stop_interval'**: int : Stop if criteria is performing worse over n consecutive checks (default: 3)\n                - **'corruption_entities'**: List of entities to be used for corruptions. If 'all',\n                  it uses all entities (default: 'all')\n                - **'corrupt_side'**: Specifies which side to corrupt. 's', 'o', 's+o' (default)\n\n                Example: ``early_stopping_params={x_valid=X['valid'], 'criteria': 'mrr'}``\n\n        \"\"\"",
        "\"\"\"Generates the test/validation data. If filter_triples are passed, then it returns the False Negatives\n           that could be present in the generated corruptions.\n\n           If we are dealing with large graphs, then along with the above, this method returns the idx of the\n           entities present in the batch and their embeddings.\n        \"\"\"",
        "\"\"\"Initialize the evaluation graph.\n\n        Parameters\n        ----------\n        mode: string\n            Indicates which data generator to use.\n        \"\"\"",
        "\"\"\"Initializes and creates evaluation graph for early stopping.\n        \"\"\"",
        "\"\"\"Predict the scores of triples using a trained embedding model.\n            The function returns raw scores generated by the model.\n\n            .. note::\n\n                To obtain probability estimates, use a logistic sigmoid: ::\n\n                    >>> model.fit(X)\n                    >>> y_pred = model.predict(np.array([['f', 'y', 'e'], ['b', 'y', 'd']]))\n                    >>> print(y_pred)\n                    [-0.13863425, -0.09917116]\n                    >>> from scipy.special import expit\n                    >>> expit(y_pred)\n                    array([0.4653968 , 0.47522753], dtype=float32)\n\n        Parameters\n        ----------\n        X : ndarray, shape [n, 3]\n            The triples to score.\n        from_idx : bool\n            If True, will skip conversion to internal IDs. (default: False).\n\n        Returns\n        -------\n        scores_predict : ndarray, shape [n]\n            The predicted scores for input triples X.\n\n        \"\"\"",
        "\"\"\" Used by evaluate_predictions to get the ranks for evaluation.\n\n        Parameters\n        ----------\n        dataset_handle : Object of AmpligraphDatasetAdapter\n                         This contains handles of the generators used to get test triples and filters.\n\n        Returns\n        -------\n        ranks : ndarray, shape [n] or [n,2] depending on the value of use_default_protocol.\n                An array of ranks of test triples.\n        \"\"\""
    ],
    "functions": [
        "_initialize_parameters",
        "_dense",
        "_conv2d",
        "_batch_norm",
        "mean_var_with_update",
        "_dropout",
        "_get_model_loss",
        "_save_trained_params",
        "_load_model_from_trained_params",
        "_fn",
        "get_embeddings",
        "_training_data_generator",
        "fit",
        "_test_generator",
        "_initialize_eval_graph",
        "_initialize_early_stopping",
        "predict",
        "get_ranks"
    ],
    "classes": [
        "ConvE"
    ]
}