{
    "identifiers": [
        "math",
        "abc",
        "abstractmethod",
        "torchvtk",
        "utils",
        "volume_utils",
        "make_5d",
        "numpy",
        "np",
        "torch",
        "torch",
        "nn",
        "functional",
        "F",
        "device",
        "apply_on",
        "dtype",
        "torch",
        "float32",
        "device",
        "apply_on",
        "dtype",
        "abstractmethod",
        "data",
        "data",
        "isinstance",
        "data",
        "key",
        "apply_on",
        "data",
        "key",
        "isinstance",
        "tmp",
        "np",
        "ndarray",
        "torch",
        "from_numpy",
        "tmp",
        "torch",
        "is_tensor",
        "tmp",
        "transform",
        "tmp",
        "to",
        "dtype",
        "to",
        "device",
        "transform",
        "tmp",
        "isinstance",
        "data",
        "transform",
        "d",
        "torch",
        "is_tensor",
        "d",
        "d",
        "d",
        "data",
        "torch",
        "is_tensor",
        "data",
        "transform",
        "data",
        "data",
        "data",
        "DictTransform",
        "func",
        "apply_on",
        "kwargs",
        "apply_on",
        "apply_on",
        "kwargs",
        "func",
        "data",
        "tfm",
        "data",
        "tfms",
        "tfms",
        "data",
        "tfm",
        "tfms",
        "tfm",
        "data",
        "data",
        "DictTransform",
        "size",
        "mode",
        "kwargs",
        "kwargs",
        "isinstance",
        "size",
        "isinstance",
        "size",
        "size",
        "size",
        "mode",
        "x",
        "is_factor",
        "F",
        "interpolate",
        "make_5d",
        "x",
        "scale",
        "size",
        "mode",
        "mode",
        "squeeze",
        "F",
        "interpolate",
        "make_5d",
        "x",
        "size",
        "size",
        "mode",
        "mode",
        "squeeze",
        "DictTransform",
        "std_deviation",
        "mean",
        "kwargs",
        "std_deviation",
        "kwargs",
        "mean",
        "DictTransform",
        "kwargs",
        "data",
        "data",
        "min",
        "data",
        "max",
        "data",
        "torch",
        "randn_like",
        "data",
        "std_deviation",
        "mean",
        "torch",
        "clamp",
        "data",
        "min",
        "max",
        "DictTransform",
        "channels",
        "kernel_size",
        "sigma",
        "kwargs",
        "channels",
        "sigma",
        "sigma",
        "sigma",
        "kernel_size",
        "kwargs",
        "DictTransform",
        "kwargs",
        "torch",
        "meshgrid",
        "torch",
        "arange",
        "size",
        "dtype",
        "torch",
        "float32",
        "size",
        "kernel_size",
        "size",
        "std",
        "mgrid",
        "kernel_size",
        "sigma",
        "meshgrids",
        "size",
        "kernel",
        "std",
        "math",
        "sqrt",
        "math",
        "pi",
        "torch",
        "exp",
        "mgrid",
        "mean",
        "std",
        "kernel",
        "torch",
        "sum",
        "kernel",
        "kernel",
        "view",
        "kernel",
        "size",
        "kernel",
        "repeat",
        "channels",
        "kernel",
        "dim",
        "kernel",
        "to",
        "device",
        "kernel",
        "F",
        "conv3d",
        "kernel_size",
        "data",
        "F",
        "pad",
        "make_5d",
        "data",
        "pad",
        "mode",
        "conv",
        "vol",
        "weight",
        "weight",
        "groups",
        "channels",
        "padding",
        "DictTransform",
        "size",
        "position",
        "kwargs",
        "DictTransform",
        "kwargs",
        "size",
        "position",
        "position",
        "size",
        "position",
        "size",
        "position",
        "size",
        "position",
        "ValueError",
        "data",
        "get_center_crop",
        "data",
        "size",
        "data",
        "mid",
        "size",
        "size",
        "size",
        "size",
        "mid",
        "mid",
        "squeeze",
        "data",
        "mid",
        "size",
        "mid",
        "size",
        "mid",
        "size",
        "mid",
        "size",
        "mid",
        "size",
        "mid",
        "size",
        "data",
        "size",
        "position",
        "get_crop_around",
        "data",
        "torch",
        "Tensor",
        "data",
        "shape",
        "size",
        "get_crop_around",
        "data",
        "position",
        "size",
        "DictTransform",
        "permutations",
        "kwargs",
        "kwargs",
        "permutations",
        "permutations",
        "x",
        "torch",
        "randint",
        "len",
        "permutations",
        "x",
        "ndim",
        "d",
        "d",
        "len",
        "pre_shap",
        "permutations",
        "idx",
        "x",
        "permute",
        "pre_shap",
        "post_shap",
        "contiguous",
        "DictTransform",
        "flip_probability",
        "dims",
        "kwargs",
        "kwargs",
        "flip_probability",
        "isinstance",
        "dims",
        "torch",
        "is_tensor",
        "dims",
        "dims",
        "dtype",
        "torch",
        "len",
        "dims",
        "torch",
        "LongTensor",
        "dims",
        "x",
        "torch",
        "nonzero",
        "dims",
        "torch",
        "rand",
        "prob",
        "view",
        "x",
        "ndim",
        "tolist",
        "len",
        "idxs",
        "x",
        "torch",
        "flip",
        "x",
        "idxs",
        "contiguous"
    ],
    "literals": [
        "\"cpu\"",
        "\"vol\"",
        "\"mask\"",
        "f'Invalid data type for DictTransform: {type(data)}. Should be da dict (with keys to apply the transform on given through apply_on parameter), list, tuple or single tensor (applies to all tensors in these cases). Please modify your Dataset accordingly.'",
        "'trilinear'",
        "f'Invalid size argument ({size}). Use 3-tuple/list for a target size of a float for resizing by a factor.'",
        "\"device\"",
        "\"device\"",
        "'replicate'",
        "\"The size is larger than the image allows on that center position.\"",
        "\"Applies the Center Crop.\"",
        "\"Invalid dims\""
    ],
    "variables": [
        "device",
        "apply_on",
        "dtype",
        "tmp",
        "tmp",
        "data",
        "key",
        "data",
        "key",
        "data",
        "data",
        "tfm",
        "tfms",
        "data",
        "is_factor",
        "is_factor",
        "size",
        "mode",
        "std_deviation",
        "device",
        "mean",
        "min",
        "max",
        "data",
        "channels",
        "sigma",
        "kernel_size",
        "device",
        "kernel",
        "meshgrids",
        "mean",
        "kernel",
        "kernel",
        "kernel",
        "kernel",
        "weight",
        "conv",
        "pad",
        "vol",
        "size",
        "position",
        "size",
        "mid",
        "permutations",
        "permutations",
        "idx",
        "pre_shap",
        "post_shap",
        "prob",
        "dims",
        "idxs"
    ],
    "comments": [
        "",
        "BASE CLASS #############################################################",
        "",
        "Convert from NumPy",
        "If tmp is tensor, control type and device",
        "",
        "Transforms ##############################################################",
        "",
        "code from: https://discuss.pytorch.org/t/is-there-anyway-to-do-gaussian-filtering-for-an-image-2d-3d-in-pytorch/12351/10",
        "initialize conv layer.",
        "Make sure sum of values in gaussian kernel equals 1.",
        "Reshape to depthwise convolutional weight",
        "All Possible permutations for volume",
        "Choose permutation"
    ],
    "docstrings": [
        "\"\"\"\n    Super Class for the Transforms.\n\n\n    \"\"\"",
        "\"\"\"\n        :param device: The torch Code for the device on which the transformation should be executed. Possiblities are [\"cpu\", \"cuda\"].\n        :param apply_on: The keys of the volumes that should be transformed.\n        :param dtype: The torch type in which the data are. Possibilities are [torch.float16, torch.float32].\n        \"\"\"",
        "\"\"\" Transformation Method, must be overwritten by every SubClass.\"\"\"",
        "\"\"\"\n    Transformations for adding noise to images.\n    \"\"\"",
        "\"\"\"\n        :param std_deviation: The variance of the noise added to the  image.\n        :param mean: The mean of the noise.\n        :param kwargs: Arguments of the super class.\n        \"\"\"",
        "\"\"\"Applies the Noise onto the images. Variance is controlled by the noise_variance parameter.\"\"\"",
        "\"\"\"Transformation for adding Blur to images.\"\"\"",
        "\"\"\"\n        Initializing the Blur Transformation.\n        :param channels: Amount of channels of the input data.\n        :param kernel_size: Size of the convolution kernel.\n        :param sigma: Standard deviation.\n        \"\"\"",
        "\"\"\"Applies the Blur using a 3D Convolution.\"\"\"",
        "\"\"\"\n    Transformation for the cropping of 4D or 5D Volumes.\n    \"\"\"",
        "\"\"\"\n        :param size: Size of the crop.\n        :param position: Middle point of the cropped region.\n        :param kwargs: Arguments for super class.\n        \"\"\"",
        "\"\"\"Helper method for the crop.\"\"\"",
        "\"\"\"Helper method for the crop.\"\"\"",
        "''' Chooses one of the 8 random permutations for the volume axes '''",
        "''' Randomly choose one of the given permutations.\n        Args:\n            permutations (list of 3-tuples): Overrides the list of possible permutations to choose from.\n                The default is  [ (0, 1, 2), (0, 2, 1), (1, 0, 2), (1, 2, 0), (2, 0, 1), (2, 1, 0) ].\n                `permutations` must be a list or tuple of items that are compatible with torch.permute. Assume 0 to be the first spatial dimension, we account for a possible batch and channel dimension.\n                The permutation will then be chosen at random from the given list/tuple.\n        '''",
        "''' Flips dimensions with a given probability. (Random event occurs for each dimension)'''",
        "''' Flips dimensions of a tensor with a given `flip_probability`.\n        Args:\n            flip_probability (float): Probability of a dimension being flipped. Default 0.5.\n            dims (list of 3 ints): Dimensions that may be flipped are denoted with a 1, otherwise 0. [1,0,1] would randomly flip a volumes depth and width dimension, while never flipping its height dimension\n            kwargs: Keyword args for `DictTransform`\n        '''"
    ],
    "functions": [
        "transform",
        "__call__",
        "transform",
        "__call__",
        "transform",
        "transform",
        "transform",
        "transform",
        "get_crop_around",
        "get_center_crop",
        "transform",
        "transform"
    ],
    "classes": [
        "DictTransform",
        "LambdaDictTransform",
        "CompositeTransform",
        "Resize",
        "GaussianNoise",
        "GaussianBlur",
        "Crop",
        "RandPermute",
        "RandFlip"
    ]
}