{
    "identifiers": [
        "typing",
        "Dict",
        "logging",
        "joblib",
        "json",
        "click",
        "os",
        "uuid",
        "physarum",
        "physarum",
        "phyml",
        "sklearn_node",
        "sklearn_pipeline",
        "physarum",
        "phyml",
        "physarum",
        "features",
        "one_hot_encode",
        "physarum",
        "features",
        "get_unique_feature_values",
        "describe_features",
        "physarum",
        "features",
        "one_hot_encode",
        "physarum",
        "features",
        "get_unique_feature_values",
        "describe_features",
        "sklearnPipeline",
        "execute",
        "sklearnPipeline",
        "LogisticRegression",
        "SelectFromModel",
        "SelectKBest",
        "DataEncoder",
        "SimpleImputer",
        "ContinuousColumnSelector",
        "CategoricalColumnSelector",
        "customerapp",
        "shared",
        "MODEL_NAME",
        "customerapp",
        "shared",
        "FEATURES_NUMERIC",
        "FEATURES_CATEGORICAL",
        "TARGET",
        "phyml",
        "op",
        "sql",
        "output_dataset",
        "output_table",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "services",
        "warehouse",
        "select_into",
        "sql",
        "output_dataset",
        "output_table",
        "output_table",
        "phyml",
        "op",
        "bucket",
        "dataset_name",
        "table_name",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "pkg",
        "artifact_path",
        "table_name",
        "services",
        "warehouse",
        "export_csv",
        "bucket",
        "bucket_path",
        "dataset_name",
        "table_name",
        "bucket_path",
        "phyml",
        "op",
        "bucket",
        "csv_path",
        "artifact_name",
        "columns",
        "phyml",
        "get_package",
        "pkg",
        "get",
        "pkg",
        "services",
        "services",
        "lake",
        "download_csv",
        "bucket",
        "csv_path",
        "get_unique_feature_values",
        "training_df",
        "columns",
        "pkg",
        "add_artifact_json",
        "artifact_name",
        "unique_feature_values",
        "phyml",
        "op",
        "bucket",
        "csv_path",
        "artifact_name",
        "columns",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "services",
        "lake",
        "download_csv",
        "bucket",
        "csv_path",
        "describe_features",
        "training_df",
        "columns",
        "pkg",
        "add_artifact_json",
        "artifact_name",
        "unique_feature_values",
        "phyml",
        "op",
        "bucket",
        "csv_path",
        "analysis_path_categorical",
        "numeric_features",
        "target",
        "artifact_name",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "services",
        "lake",
        "download_string",
        "bucket",
        "analysis_path_categorical",
        "json",
        "loads",
        "json_features",
        "services",
        "lake",
        "download_csv",
        "bucket",
        "csv_path",
        "one_hot_encode",
        "training_df",
        "unique_feature_values",
        "throw_on_missing",
        "nf",
        "numeric_features",
        "training_df",
        "nf",
        "training_df",
        "target",
        "pkg",
        "add_artifact_dataframe",
        "artifact_name",
        "encoded_df",
        "sklearn_pipeline",
        "name",
        "description",
        "task",
        "parameter_tuning",
        "parameter_tuning_params",
        "train_path",
        "test_path",
        "output_path",
        "sep",
        "sklearn_node",
        "transformer",
        "CategoricalColumnSelector",
        "config",
        "sklearn_node",
        "transformer",
        "ContinuousColumnSelector",
        "config",
        "sklearn_node",
        "transformer",
        "SimpleImputer",
        "config",
        "after",
        "cat",
        "sklearn_node",
        "transformer",
        "DataEncoder",
        "config",
        "after",
        "imp",
        "sklearn_node",
        "transformer",
        "SimpleImputer",
        "config",
        "after",
        "cont",
        "sklearn_node",
        "transformer",
        "SelectKBest",
        "config",
        "after",
        "denc",
        "sklearn_node",
        "transformer",
        "SelectKBest",
        "config",
        "after",
        "imp2",
        "sklearn_node",
        "transformer",
        "SelectFromModel",
        "config",
        "after",
        "kbest",
        "kbest2",
        "sklearn_node",
        "transformer",
        "LogisticRegression",
        "config",
        "after",
        "fmodel",
        "sklearn_pipeline",
        "name",
        "description",
        "task",
        "parameter_tuning",
        "parameter_tuning_params",
        "train_path",
        "test_path",
        "output_path",
        "sep",
        "sklearn_node",
        "transformer",
        "CategoricalColumnSelector",
        "config",
        "sklearn_node",
        "transformer",
        "ContinuousColumnSelector",
        "config",
        "sklearn_node",
        "transformer",
        "SimpleImputer",
        "config",
        "after",
        "cat",
        "sklearn_node",
        "transformer",
        "DataEncoder",
        "config",
        "after",
        "imp",
        "sklearn_node",
        "transformer",
        "SimpleImputer",
        "config",
        "after",
        "cont",
        "sklearn_node",
        "transformer",
        "SelectKBest",
        "config",
        "after",
        "denc",
        "sklearn_node",
        "transformer",
        "SelectKBest",
        "config",
        "after",
        "imp2",
        "sklearn_node",
        "transformer",
        "SelectFromModel",
        "config",
        "after",
        "kbest",
        "kbest2",
        "sklearn_node",
        "transformer",
        "LogisticRegression",
        "config",
        "after",
        "fmodel",
        "cltv_pipeline",
        "propensity_pipeline",
        "phyml",
        "op",
        "pipeline_name",
        "bucket",
        "train_path",
        "test_path",
        "output_path",
        "class_column",
        "test_ratio",
        "version_params",
        "artifact_name",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "config",
        "pkg",
        "services",
        "services",
        "lake",
        "download_csv",
        "bucket",
        "train_path",
        "mlpipelines",
        "pipeline_name",
        "pipe",
        "train_path",
        "final_df",
        "test_path",
        "final_df",
        "output_path",
        "output_path",
        "execute",
        "pipe_dag",
        "class_column",
        "class_column",
        "test_ratio",
        "test_ratio",
        "pkg",
        "add_artifact_folder",
        "output_path",
        "phyml",
        "op",
        "pipeline_name",
        "bucket",
        "train_path",
        "test_path",
        "output_path",
        "class_column",
        "test_ratio",
        "version_params",
        "artifact_name",
        "phyml",
        "get_package",
        "pkg",
        "services",
        "config",
        "pkg",
        "services",
        "services",
        "lake",
        "download_csv",
        "bucket",
        "train_path",
        "mlpipelines",
        "pipeline_name",
        "pipe",
        "train_path",
        "final_df",
        "test_path",
        "final_df",
        "output_path",
        "output_path",
        "execute",
        "pipe_dag",
        "class_column",
        "class_column",
        "test_ratio",
        "test_ratio",
        "pkg",
        "artifact_folder",
        "output_path",
        "services",
        "lake",
        "copy_local_directory",
        "bucket",
        "bucket_path",
        "output_path",
        "pkg",
        "add_artifact_folder",
        "output_path"
    ],
    "literals": [
        "\".csv\"",
        "\"mypipeline2\"",
        "\"sklearn training one way\"",
        "\"Classification\"",
        "\"\\t\"",
        "\"unique_counts\"",
        "\"continuous_columns\"",
        "\"unique_counts\"",
        "\"strategy\"",
        "\"most_frequent\"",
        "\"columns\"",
        "\"strategy\"",
        "\"mean\"",
        "\"score_func\"",
        "\"f_regression\"",
        "\"k\"",
        "\"all\"",
        "\"score_func\"",
        "\"f_regression\"",
        "\"k\"",
        "\"all\"",
        "\"estimator\"",
        "\"LogisticRegression\"",
        "\"estimator_params\"",
        "\"threshold\"",
        "\"C\"",
        "\"tol\"",
        "\"penalty\"",
        "\"l1\"",
        "\"l2\"",
        "\"mypipeline3\"",
        "\"sklearn training another way\"",
        "\"Classification\"",
        "\"\\t\"",
        "\"unique_counts\"",
        "\"continuous_columns\"",
        "\"unique_counts\"",
        "\"strategy\"",
        "\"most_frequent\"",
        "\"columns\"",
        "\"strategy\"",
        "\"mean\"",
        "\"score_func\"",
        "\"f_regression\"",
        "\"k\"",
        "\"all\"",
        "\"score_func\"",
        "\"f_regression\"",
        "\"k\"",
        "\"all\"",
        "\"estimator\"",
        "\"LogisticRegression\"",
        "\"estimator_params\"",
        "\"threshold\"",
        "\"C\"",
        "\"tol\"",
        "\"penalty\"",
        "\"l1\"",
        "\"l2\"",
        "\"cltv_pipeline\"",
        "\"propensity_pipeline\"",
        "\"data loaded to bucket\""
    ],
    "variables": [
        "pkg",
        "services",
        "pkg",
        "services",
        "bucket_path",
        "bucket_url",
        "pkg",
        "pkg_data",
        "services",
        "training_df",
        "unique_feature_values",
        "pkg",
        "services",
        "training_df",
        "unique_feature_values",
        "pkg",
        "services",
        "json_features",
        "unique_feature_values",
        "training_df",
        "encoded_df",
        "encoded_df",
        "nf",
        "encoded_df",
        "target",
        "cat",
        "cont",
        "imp",
        "denc",
        "imp2",
        "kbest",
        "kbest2",
        "fmodel",
        "logg",
        "cat",
        "cont",
        "imp",
        "denc",
        "imp2",
        "kbest",
        "kbest2",
        "fmodel",
        "logg",
        "mlpipelines",
        "pkg",
        "config",
        "services",
        "final_df",
        "pipe",
        "pipe_dag",
        "pkg",
        "config",
        "services",
        "final_df",
        "pipe",
        "pipe_dag",
        "bucket_path",
        "bucket_url"
    ],
    "comments": [
        "general python library imports",
        "from xgboost import XGBClassifier",
        "core physarum library imports",
        "application specific imports",
        "from customerapp.shared import version_params",
        "from customerapp.shared import data, version_params",
        "a kind fo hack as the initilaizing service was causing error (to be fixed later)",
        "we can also pass it using phyml.pass_context, as essentially we need only the",
        "services from the package",
        "print(\"training df is obtained\")",
        "Add the artifact we have just produced",
        "Add the artifact we have just produced",
        "Add in our categorical features via one-hot encoding",
        "Add in all the numeric columns",
        "Add in the target column to our new encoded data frame",
        "Add the encoded dataframe as an artifact",
        "@phyml.op()",
        "def churn_pipeline_op1(bucket: str,",
        "train_path: str,",
        "test_path: str,",
        "output_path: str,",
        "class_column: str,",
        "test_ratio=0.3,",
        "version_params = None,",
        "artifact_name = None):",
        "# will be provide by feature store",
        "pkg = phyml.get_package()",
        "services = pkg.services",
        "# reolving the bucket_path to local_path",
        "train_path = services.lake.download_csv_local(bucket, train_path)",
        "if test_path is not None:",
        "test_path = services.lake.download_csv(bucket, test_path)",
        "# we cannot hardcode names here; will be passed from above",
        "@sklearn_pipeline(name = \"mypipeline\", description = \"sklearn training\" , task = \"Classification\", parameter_tuning = None, parameter_tuning_params = {})",
        "def my_pipeline(train_path=train_path, test_path=test_path, output_path=output_path, sep=\"\\t\"):",
        "# cat = sklearn_node(transformer = CategoricalColumnSelector , config=  {\"categorical_columns\": [\"a\", \"c\", \"f\"] , \"unique_counts\" : 20}   )",
        "cat = sklearn_node(transformer = CategoricalColumnSelector , config=  {\"unique_counts\" : 20}   )",
        "cont = sklearn_node(transformer = ContinuousColumnSelector, config=  { \"continuous_columns\" : None, \"unique_counts\" : 20} )",
        "imp = sklearn_node(transformer = SimpleImputer, config=    {  \"strategy\": \"most_frequent\"}  ).after(cat)",
        "denc = sklearn_node(transformer = DataEncoder, config=    { \"columns\" : None} ).after(imp)",
        "imp2 = sklearn_node(transformer = SimpleImputer, config=   { \"strategy\": \"mean\"} ).after(cont)",
        "kbest = sklearn_node(transformer = SelectKBest, config=   { \"score_func\" : \"f_regression\" ,  \"k\": \"all\"} ).after(denc)",
        "kbest2 = sklearn_node(transformer = SelectKBest, config=   { \"score_func\" : \"f_regression\" ,  \"k\": \"all\" } ).after(imp2)",
        "fmodel = sklearn_node(transformer = SelectFromModel, config=   {\"estimator\" : \"LogisticRegression\"  , \"estimator_params\" : {} ,  \"threshold\" : None } ).after(kbest,kbest2)",
        "logg = sklearn_node(transformer = LogisticRegression, config=  {\"C\":[1 , 3]   ,\"tol\":[ 0.0001 , 0.001 ] ,\"penalty\": [\"l1\" , \"l2\"]  } ).after(fmodel)",
        "# get the dag and execute",
        "pipe = my_pipeline()",
        "execute(pipe, class_column=class_column, test_ratio=test_ratio)",
        "bucket_path = pkg.artifact_folder(output_path)",
        "bucket_url = services.lake.copy_local_directory(bucket, bucket_path, output_path)",
        "print(\"data loaded to bucket\")",
        "# artifact_tmp_path = os.path.join(output_path, \"model.joblib\")",
        "return bucket_path",
        "another way of writing the pipeline",
        "cat = sklearn_node(transformer = CategoricalColumnSelector , config=  {\"unique_counts\" : 20}   )",
        "dynamically pick the node based on the type of pipeline",
        "In this case we think of Sklearn pipeline as a \"SQL query\" which can run itself against any data/table/database (tables are parameterized)",
        "resolving the bucket_path to local_path",
        "pkg = phyml.get_package()",
        "print(pkg)",
        "services = pkg.services",
        "train_path = services.lake.download_csv_local(bucket, train_path)",
        "if test_path is not None:",
        "test_path = services.lake.download_csv_local(bucket, test_path)",
        "# get the required pipe object",
        "pipe = mlpipelines[pipeline_name]",
        "# get the dag and execute",
        "pipe_dag = pipe(train_path=train_path, test_path=test_path, output_path=output_path)",
        "execute(pipe_dag, class_column=class_column, test_ratio=test_ratio)",
        "print(\"data loaded to bucket\")",
        "we can move this code to execute itself (coz the folder structure cannot be arbirtary)",
        "order will be required for model_container to effectively work",
        "bucket_path = pkg.artifact_folder(output_path)",
        "bucket_url = services.lake.copy_local_directory(bucket, bucket_path, output_path)",
        "print(\"print the required path\")",
        "print(bucket)",
        "print(bucket_path)",
        "print(output_path)",
        "services.lake.copy_local_directory_to_gcs(bucket_name, bucket_path, local_path)",
        "artifact_tmp_path = os.path.join(output_path, \"model.joblib\")",
        "return bucket_path",
        "resolving the bucket_path to local_path",
        "pkg = phyml.get_package()",
        "services = pkg.services",
        "train_path = services.lake.download_csv_local(bucket, train_path)",
        "if test_path is not None:",
        "test_path = services.lake.download_csv_local(bucket, test_path)",
        "# get the required pipe object",
        "pipe = mlpipelines[pipeline_name]",
        "# get the dag and execute",
        "pipe_dag = pipe(train_path=train_path, test_path=test_path, output_path=output_path)",
        "execute(pipe_dag, class_column=class_column, test_ratio=test_ratio)",
        "artifact_tmp_path = os.path.join(output_path, \"model.joblib\")",
        "@phyml.op()",
        "def custom_model(bucket: str, train_path: str, target: str, artifact_name: str):",
        "pkg = phyml.get_package()",
        "config = pkg.services.config",
        "services = pkg.services",
        "final_df = services.lake.download_csv(bucket, train_path)",
        "targets = final_df[target]",
        "feature_matrix = final_df.values",
        "classifier = XGBClassifier()",
        "model = classifier.fit(feature_matrix, targets, verbose=True)",
        "filename = uuid.uuid4()",
        "tmp_path = os.path.join(\"/tmp/\", f\"{filename}.joblib\")",
        "joblib.dump(model, tmp_path)",
        "artifact_path = pkg.add_artifact_file(artifact_name, tmp_path)",
        "# os.remove(tmp_path)",
        "return artifact_path",
        "return bucket_path",
        "@phyml.op()",
        "def download_data(bucket: str, bucket_path: str, artifact_name: str):",
        "pkg = phyml.get_package()",
        "pkg_data = pkg.get()",
        "services = pkg.services",
        "tmp_path = services.lake.download_csv_local(bucket, bucket_path)",
        "artifact_path = pkg.add_artifact_file(artifact_name, tmp_path)",
        "# return artifact_path",
        "return tmp_path",
        "Or we can do it here"
    ],
    "docstrings": [],
    "functions": [
        "select_into",
        "export_csv",
        "analyze_categorical_features",
        "analyze_numeric_features",
        "build_matrix",
        "cltv_pipeline",
        "propensity_pipeline",
        "sklearn_pipeline_op2",
        "sklearn_pipeline_op3"
    ],
    "classes": []
}