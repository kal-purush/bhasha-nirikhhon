{
    "identifiers": [
        "os",
        "pygments",
        "lexer",
        "Lexer",
        "pygments",
        "token",
        "Keyword",
        "Name",
        "Comment",
        "Operator",
        "Other",
        "Generic",
        "Whitespace",
        "Text",
        "Punctuation",
        "re",
        "subprocess",
        "Text",
        "Text",
        "Name",
        "Variable",
        "Generic",
        "Name",
        "Punctuation",
        "Name",
        "Constant",
        "Comment",
        "DEFAULT",
        "DEFAULT",
        "Operator",
        "Escape",
        "Single",
        "Punctuation",
        "Other",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "DEFAULT",
        "text",
        "role",
        "text",
        "isspace",
        "Text",
        "Whitespace",
        "role",
        "Single",
        "text",
        "startswith",
        "ROLE_TO_TOKEN",
        "role",
        "code",
        "offset",
        "subprocess",
        "Popen",
        "stdin",
        "subprocess",
        "PIPE",
        "stdout",
        "subprocess",
        "PIPE",
        "universal_newlines",
        "proc",
        "communicate",
        "code",
        "line",
        "stdout",
        "splitlines",
        "line",
        "split",
        "start",
        "end",
        "code",
        "start",
        "end",
        "token_for_text_and_role",
        "value",
        "role",
        "result",
        "append",
        "start",
        "offset",
        "tok",
        "value",
        "result",
        "Lexer",
        "input_text",
        "s",
        "startswith",
        "s",
        "input_text",
        "splitlines",
        "tokenize_fish_command",
        "input_text",
        "re",
        "compile",
        "re",
        "MULTILINE",
        "m",
        "regex",
        "finditer",
        "input_text",
        "m",
        "group",
        "result",
        "append",
        "m",
        "start",
        "Generic",
        "Prompt",
        "m",
        "group",
        "result",
        "extend",
        "tokenize_fish_command",
        "m",
        "group",
        "m",
        "start",
        "result",
        "append",
        "m",
        "start",
        "OUTPUT_TOKEN",
        "m",
        "group",
        "result"
    ],
    "literals": [
        "\"normal\"",
        "\"error\"",
        "\"command\"",
        "\"statement_terminator\"",
        "\"param\"",
        "\"comment\"",
        "\"match\"",
        "\"search_match\"",
        "\"operat\"",
        "\"escape\"",
        "\"quote\"",
        "\"redirection\"",
        "\"autosuggestion\"",
        "\"selection\"",
        "\"pager_progress\"",
        "\"pager_background\"",
        "\"pager_prefix\"",
        "\"pager_completion\"",
        "\"pager_description\"",
        "\"pager_secondary_background\"",
        "\"pager_secondary_prefix\"",
        "\"pager_secondary_completion\"",
        "\"pager_secondary_description\"",
        "\"pager_selected_background\"",
        "\"pager_selected_prefix\"",
        "\"pager_selected_completion\"",
        "\"pager_selected_description\"",
        "\"quote\"",
        "\"'\"",
        "\"fish_indent\"",
        "\"--pygments\"",
        "\",\"",
        "\"FishIndentLexer\"",
        "\"fish\"",
        "\"fish-docs-samples\"",
        "\"*.fish\"",
        "\">\"",
        "r\"^(>\\s*)?(.*\\n?)\""
    ],
    "variables": [
        "OUTPUT_TOKEN",
        "DEFAULT",
        "ROLE_TO_TOKEN",
        "proc",
        "stdout",
        "_",
        "result",
        "start",
        "end",
        "role",
        "start",
        "end",
        "value",
        "tok",
        "name",
        "aliases",
        "filenames",
        "result",
        "result",
        "regex"
    ],
    "comments": [
        "This is a plugin for pygments that shells out to fish_indent.",
        "Example of how to use this:",
        "env PATH=\"/dir/containing/fish/indent/:$PATH\" pygmentize -f terminal256 -l /path/to/fish_indent_lexer.py:FishIndentLexer -x ~/test.fish",
        "The token type representing output to the console.",
        "A fallback token type.",
        "Mapping from fish token types to Pygments types.",
        "note, may be changed to double dynamically",
        "?",
        "in practice won't be generated",
        "Here fish will return 'normal' or 'statement_terminator' for newline.",
        "Check for single or double.",
        "No prompt, just tokenize everything.",
        "We have a prompt line.",
        "Use a regexp because it will maintain string indexes for us.",
        "Prompt line; highlight via fish syntax.",
        "Non-prompt line representing output from a command."
    ],
    "docstrings": [
        "\"\"\" Return the pygments token for some input text and a fish role\n    \n        This applies any special cases of ROLE_TO_TOKEN.\n    \"\"\"",
        "\"\"\" Tokenize some fish code, offset in a parent string, by shelling\n        out to fish_indent.\n        \n        fish_indent will output a list of csv lines: start,end,type.\n\n        This function returns a list of (start, tok, value) tuples, as\n        Pygments expects.\n    \"\"\"",
        "\"\"\" Return a list of (start, tok, value) tuples.\n\n            start is the index into the string\n            tok is the token type (as above)\n            value is the string contents of the token\n        \"\"\""
    ],
    "functions": [
        "token_for_text_and_role",
        "tokenize_fish_command",
        "get_tokens_unprocessed"
    ],
    "classes": [
        "FishIndentLexer"
    ]
}