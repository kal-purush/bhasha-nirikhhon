{
    "identifiers": [
        "absolute_import",
        "division",
        "print_function",
        "os",
        "nltk",
        "numpy",
        "np",
        "pickle",
        "random",
        "filename",
        "os",
        "path",
        "join",
        "filename",
        "format",
        "dataset_path",
        "open",
        "dataset_path",
        "handle",
        "pickle",
        "load",
        "handle",
        "data",
        "data",
        "data",
        "word2id",
        "id2word",
        "trainingSamples",
        "samples",
        "Batch",
        "len",
        "sample",
        "sample",
        "samples",
        "len",
        "sample",
        "sample",
        "samples",
        "max",
        "batch",
        "encoder_inputs_length",
        "max",
        "batch",
        "decoder_targets_length",
        "sample",
        "samples",
        "reversed",
        "sample",
        "padToken",
        "max_source_length",
        "len",
        "source",
        "batch",
        "encoder_inputs",
        "append",
        "pad",
        "source",
        "sample",
        "padToken",
        "max_target_length",
        "len",
        "target",
        "batch",
        "decoder_targets",
        "append",
        "target",
        "pad",
        "batch",
        "data",
        "batch_size",
        "random",
        "shuffle",
        "data",
        "len",
        "data",
        "i",
        "data_len",
        "batch_size",
        "data",
        "i",
        "min",
        "i",
        "batch_size",
        "data_len",
        "samples",
        "genNextSamples",
        "createBatch",
        "samples",
        "batches",
        "append",
        "batch",
        "batches",
        "sentence",
        "word2id",
        "sentence",
        "nltk",
        "word_tokenize",
        "sentence",
        "len",
        "tokens",
        "token",
        "tokens",
        "wordIds",
        "append",
        "word2id",
        "get",
        "token",
        "unknownToken",
        "createBatch",
        "wordIds",
        "batch"
    ],
    "literals": [
        "'Loading dataset from {}'",
        "'rb'",
        "'word2id'",
        "'id2word'",
        "'trainingSamples'",
        "''"
    ],
    "variables": [
        "padToken",
        "goToken",
        "eosToken",
        "unknownToken",
        "encoder_inputs",
        "encoder_inputs_length",
        "decoder_targets",
        "decoder_targets_length",
        "dataset_path",
        "data",
        "word2id",
        "id2word",
        "trainingSamples",
        "batch",
        "batch",
        "encoder_inputs_length",
        "batch",
        "decoder_targets_length",
        "max_source_length",
        "max_target_length",
        "source",
        "pad",
        "target",
        "pad",
        "batches",
        "data_len",
        "batch",
        "tokens",
        "wordIds",
        "batch"
    ],
    "comments": [
        "batch类，里面包含了encoder输入，decoder输入，decoder标签，decoder样本长度mask",
        "Warning: If adding something here, also modifying saveDataset",
        "将source进行反序并PAD值本batch的最大长度",
        "将target进行PAD，并添加END符号",
        "batch.target_inputs.append([goToken] + target + pad[:-1])",
        "每个epoch之前都要进行样本的shuffle",
        "分词",
        "将每个单词转化为id",
        "调用createBatch构造batch"
    ],
    "docstrings": [
        "'''\n    读取样本数据\n    :param filename: 文件路径，是一个字典，包含word2id、id2word分别是单词与索引对应的字典和反序字典，\n                    trainingSamples样本数据，每一条都是QA对\n    :return: word2id, id2word, trainingSamples\n    '''",
        "'''\n    根据给出的samples（就是一个batch的数据），进行padding并构造成placeholder所需要的数据形式\n    :param samples: 一个batch的样本数据，列表，每个元素都是[question， answer]的形式，id\n    :return: 处理完之后可以直接传入feed_dict的数据格式\n    '''",
        "'''\n    根据读取出来的所有数据和batch_size将原始数据分成不同的小batch。对每个batch索引的样本调用createBatch函数进行处理\n    :param data: loadDataset函数读取之后的trainingSamples，就是QA对的列表\n    :param batch_size: batch大小\n    :param en_de_seq_len: 列表，第一个元素表示source端序列的最大长度，第二个元素表示target端序列的最大长度\n    :return: 列表，每个元素都是一个batch的样本数据，可直接传入feed_dict进行训练\n    '''",
        "'''\n    测试的时候将用户输入的句子转化为可以直接feed进模型的数据，现将句子转化成id，然后调用createBatch处理\n    :param sentence: 用户输入的句子\n    :param word2id: 单词与id之间的对应关系字典\n    :param en_de_seq_len: 列表，第一个元素表示source端序列的最大长度，第二个元素表示target端序列的最大长度\n    :return: 处理之后的数据，可直接feed进模型进行预测\n    '''"
    ],
    "functions": [
        "loadDataset",
        "createBatch",
        "getBatches",
        "genNextSamples",
        "sentence2enco"
    ],
    "classes": [
        "Batch"
    ]
}