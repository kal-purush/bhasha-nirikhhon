{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "torch",
        "utils",
        "checkpoint",
        "checkpoint",
        "numpy",
        "np",
        "typing",
        "Optional",
        "x",
        "drop_prob",
        "training",
        "drop_prob",
        "training",
        "x",
        "drop_prob",
        "x",
        "shape",
        "x",
        "ndim",
        "keep_prob",
        "torch",
        "rand",
        "shape",
        "dtype",
        "x",
        "dtype",
        "device",
        "x",
        "device",
        "random_tensor",
        "floor_",
        "x",
        "div",
        "keep_prob",
        "random_tensor",
        "output",
        "nn",
        "Module",
        "drop_prob",
        "DropPath",
        "drop_prob",
        "x",
        "drop_path_f",
        "x",
        "drop_prob",
        "training",
        "nn",
        "Module",
        "patch_size",
        "in_chans",
        "num_classes",
        "embed_dim",
        "depths",
        "num_heads",
        "window_size",
        "mlp_ratio",
        "qkv_bias",
        "drop_rate",
        "attn_drop_rate",
        "drop_path_rate",
        "norm_layer",
        "nn",
        "LayerNorm",
        "patch_norm",
        "use_checkpoint",
        "conv_depths",
        "conv_dims",
        "conv_drop_path_rate",
        "conv_layer_scale_init_value",
        "conv_head_init_scale",
        "kwargs",
        "nn",
        "ModuleList",
        "nn",
        "Sequential",
        "nn",
        "Conv2d",
        "in_chans",
        "conv_dims",
        "kernel_size",
        "stride",
        "LayerNorm",
        "conv_dims",
        "eps",
        "data_format",
        "downsample_layers",
        "append",
        "stem",
        "i",
        "nn",
        "Sequential",
        "LayerNorm",
        "conv_dims",
        "i",
        "eps",
        "data_format",
        "nn",
        "Conv2d",
        "conv_dims",
        "i",
        "conv_dims",
        "i",
        "kernel_size",
        "stride",
        "downsample_layers",
        "append",
        "downsample_layer",
        "nn",
        "ModuleList",
        "x",
        "item",
        "x",
        "torch",
        "linspace",
        "conv_drop_path_rate",
        "sum",
        "conv_depths",
        "i",
        "nn",
        "Sequential",
        "Block",
        "dim",
        "conv_dims",
        "i",
        "drop_rate",
        "dp_rates",
        "cur",
        "j",
        "layer_scale_init_value",
        "conv_layer_scale_init_value",
        "j",
        "conv_depths",
        "i",
        "stages",
        "append",
        "stage",
        "cur",
        "conv_depths",
        "i",
        "nn",
        "LayerNorm",
        "conv_dims",
        "eps",
        "nn",
        "Linear",
        "conv_dims",
        "num_classes",
        "conv_head",
        "weight",
        "data",
        "mul_",
        "conv_head_init_scale",
        "conv_head",
        "bias",
        "data",
        "mul_",
        "conv_head_init_scale",
        "num_classes",
        "len",
        "depths",
        "embed_dim",
        "patch_norm",
        "embed_dim",
        "num_layers",
        "mlp_ratio",
        "PatchEmbed",
        "patch_size",
        "patch_size",
        "in_c",
        "in_chans",
        "embed_dim",
        "embed_dim",
        "norm_layer",
        "norm_layer",
        "patch_norm",
        "nn",
        "Dropout",
        "p",
        "drop_rate",
        "x",
        "item",
        "x",
        "torch",
        "linspace",
        "drop_path_rate",
        "sum",
        "depths",
        "BasicLayer",
        "dim",
        "embed_dim",
        "i_layer",
        "depth",
        "depths",
        "i_layer",
        "num_heads",
        "num_heads",
        "i_layer",
        "window_size",
        "window_size",
        "mlp_ratio",
        "mlp_ratio",
        "qkv_bias",
        "qkv_bias",
        "drop",
        "drop_rate",
        "attn_drop",
        "attn_drop_rate",
        "drop_path",
        "dpr",
        "sum",
        "depths",
        "i_layer",
        "sum",
        "depths",
        "i_layer",
        "norm_layer",
        "norm_layer",
        "downsample",
        "PatchMerging",
        "i_layer",
        "use_checkpoint",
        "use_checkpoint",
        "BasicLayer",
        "dim",
        "embed_dim",
        "i_layer",
        "depth",
        "depths",
        "i_layer",
        "num_heads",
        "num_heads",
        "i_layer",
        "window_size",
        "window_size",
        "mlp_ratio",
        "mlp_ratio",
        "qkv_bias",
        "qkv_bias",
        "drop",
        "drop_rate",
        "attn_drop",
        "attn_drop_rate",
        "drop_path",
        "dpr",
        "sum",
        "depths",
        "i_layer",
        "sum",
        "depths",
        "i_layer",
        "norm_layer",
        "norm_layer",
        "downsample",
        "PatchMerging",
        "i_layer",
        "use_checkpoint",
        "use_checkpoint",
        "BasicLayer",
        "dim",
        "embed_dim",
        "i_layer",
        "depth",
        "depths",
        "i_layer",
        "num_heads",
        "num_heads",
        "i_layer",
        "window_size",
        "window_size",
        "mlp_ratio",
        "mlp_ratio",
        "qkv_bias",
        "qkv_bias",
        "drop",
        "drop_rate",
        "attn_drop",
        "attn_drop_rate",
        "drop_path",
        "dpr",
        "sum",
        "depths",
        "i_layer",
        "sum",
        "depths",
        "i_layer",
        "norm_layer",
        "norm_layer",
        "downsample",
        "PatchMerging",
        "i_layer",
        "use_checkpoint",
        "use_checkpoint",
        "BasicLayer",
        "dim",
        "embed_dim",
        "i_layer",
        "depth",
        "depths",
        "i_layer",
        "num_heads",
        "num_heads",
        "i_layer",
        "window_size",
        "window_size",
        "mlp_ratio",
        "mlp_ratio",
        "qkv_bias",
        "qkv_bias",
        "drop",
        "drop_rate",
        "attn_drop",
        "attn_drop_rate",
        "drop_path",
        "dpr",
        "sum",
        "depths",
        "i_layer",
        "sum",
        "depths",
        "i_layer",
        "norm_layer",
        "norm_layer",
        "downsample",
        "PatchMerging",
        "i_layer",
        "use_checkpoint",
        "use_checkpoint",
        "norm_layer",
        "num_features",
        "nn",
        "AdaptiveAvgPool1d",
        "nn",
        "Linear",
        "num_features",
        "num_classes",
        "num_classes",
        "nn",
        "Identity",
        "apply",
        "_init_weights",
        "HFF_block",
        "ch_1",
        "ch_2",
        "r_2",
        "ch_int",
        "ch_out",
        "drop_rate",
        "drop_rate",
        "HFF_block",
        "ch_1",
        "ch_2",
        "r_2",
        "ch_int",
        "ch_out",
        "drop_rate",
        "drop_rate",
        "HFF_block",
        "ch_1",
        "ch_2",
        "r_2",
        "ch_int",
        "ch_out",
        "drop_rate",
        "drop_rate",
        "HFF_block",
        "ch_1",
        "ch_2",
        "r_2",
        "ch_int",
        "ch_out",
        "drop_rate",
        "drop_rate",
        "m",
        "isinstance",
        "m",
        "nn",
        "Linear",
        "nn",
        "init",
        "trunc_normal_",
        "m",
        "weight",
        "std",
        "isinstance",
        "m",
        "nn",
        "Linear",
        "m",
        "bias",
        "nn",
        "init",
        "constant_",
        "m",
        "bias",
        "isinstance",
        "m",
        "nn",
        "LayerNorm",
        "nn",
        "init",
        "constant_",
        "m",
        "bias",
        "nn",
        "init",
        "constant_",
        "m",
        "weight",
        "isinstance",
        "m",
        "nn",
        "Conv2d",
        "nn",
        "Linear",
        "nn",
        "init",
        "trunc_normal_",
        "m",
        "weight",
        "std",
        "nn",
        "init",
        "constant_",
        "m",
        "bias",
        "imgs",
        "patch_embed",
        "imgs",
        "pos_drop",
        "x_s",
        "layers1",
        "x_s",
        "H",
        "W",
        "layers2",
        "x_s_1",
        "H",
        "W",
        "layers3",
        "x_s_2",
        "H",
        "W",
        "layers4",
        "x_s_3",
        "H",
        "W",
        "torch",
        "transpose",
        "x_s_1",
        "x_s_1",
        "view",
        "x_s_1",
        "shape",
        "torch",
        "transpose",
        "x_s_2",
        "x_s_2",
        "view",
        "x_s_2",
        "shape",
        "torch",
        "transpose",
        "x_s_3",
        "x_s_3",
        "view",
        "x_s_3",
        "shape",
        "torch",
        "transpose",
        "x_s_4",
        "x_s_4",
        "view",
        "x_s_4",
        "shape",
        "downsample_layers",
        "imgs",
        "stages",
        "x_c",
        "downsample_layers",
        "x_c_1",
        "stages",
        "x_c",
        "downsample_layers",
        "x_c_2",
        "stages",
        "x_c",
        "downsample_layers",
        "x_c_3",
        "stages",
        "x_c",
        "fu1",
        "x_c_1",
        "x_s_1",
        "fu2",
        "x_c_2",
        "x_s_2",
        "x_f_1",
        "fu3",
        "x_c_3",
        "x_s_3",
        "x_f_2",
        "fu4",
        "x_c_4",
        "x_s_4",
        "x_f_3",
        "conv_norm",
        "x_f_4",
        "mean",
        "conv_head",
        "x_fu",
        "x_fu",
        "nn",
        "Module",
        "normalized_shape",
        "eps",
        "data_format",
        "nn",
        "Parameter",
        "torch",
        "ones",
        "normalized_shape",
        "requires_grad",
        "nn",
        "Parameter",
        "torch",
        "zeros",
        "normalized_shape",
        "requires_grad",
        "eps",
        "data_format",
        "data_format",
        "ValueError",
        "data_format",
        "normalized_shape",
        "x",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "data_format",
        "F",
        "layer_norm",
        "x",
        "normalized_shape",
        "weight",
        "bias",
        "eps",
        "data_format",
        "x",
        "mean",
        "keepdim",
        "x",
        "mean",
        "pow",
        "mean",
        "keepdim",
        "x",
        "mean",
        "torch",
        "sqrt",
        "eps",
        "weight",
        "x",
        "bias",
        "x",
        "nn",
        "Module",
        "dim",
        "drop_rate",
        "layer_scale_init_value",
        "nn",
        "Conv2d",
        "dim",
        "dim",
        "kernel_size",
        "padding",
        "groups",
        "dim",
        "LayerNorm",
        "dim",
        "eps",
        "data_format",
        "nn",
        "Linear",
        "dim",
        "dim",
        "nn",
        "GELU",
        "nn",
        "Linear",
        "dim",
        "dim",
        "nn",
        "Parameter",
        "layer_scale_init_value",
        "torch",
        "ones",
        "dim",
        "requires_grad",
        "layer_scale_init_value",
        "DropPath",
        "drop_rate",
        "drop_rate",
        "nn",
        "Identity",
        "x",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "x",
        "dwconv",
        "x",
        "x",
        "permute",
        "norm",
        "x",
        "pwconv1",
        "x",
        "act",
        "x",
        "pwconv2",
        "x",
        "gamma",
        "gamma",
        "x",
        "x",
        "permute",
        "shortcut",
        "drop_path",
        "x",
        "x",
        "nn",
        "Module",
        "ch_1",
        "ch_2",
        "r_2",
        "ch_int",
        "ch_out",
        "drop_rate",
        "HFF_block",
        "nn",
        "AdaptiveMaxPool2d",
        "nn",
        "AdaptiveAvgPool2d",
        "nn",
        "Sequential",
        "nn",
        "Conv2d",
        "ch_2",
        "ch_2",
        "r_2",
        "bias",
        "nn",
        "ReLU",
        "nn",
        "Conv2d",
        "ch_2",
        "r_2",
        "ch_2",
        "bias",
        "nn",
        "Sigmoid",
        "Conv",
        "bn",
        "relu",
        "bias",
        "Conv",
        "ch_1",
        "ch_int",
        "bn",
        "relu",
        "Conv",
        "ch_2",
        "ch_int",
        "bn",
        "relu",
        "nn",
        "AvgPool2d",
        "stride",
        "Conv",
        "ch_int",
        "ch_int",
        "bn",
        "relu",
        "Conv",
        "ch_int",
        "ch_int",
        "bn",
        "relu",
        "Conv",
        "ch_int",
        "ch_int",
        "bn",
        "relu",
        "nn",
        "ReLU",
        "inplace",
        "Residual",
        "ch_1",
        "ch_2",
        "ch_int",
        "ch_out",
        "nn",
        "Dropout2d",
        "drop_rate",
        "drop_rate",
        "l",
        "g",
        "f",
        "W_l",
        "l",
        "W_g",
        "g",
        "f",
        "Updim",
        "f",
        "Avg",
        "W_f",
        "W3",
        "torch",
        "cat",
        "W_f",
        "W_local",
        "W_global",
        "W",
        "torch",
        "cat",
        "W_local",
        "W_global",
        "l",
        "torch",
        "max",
        "l",
        "dim",
        "keepdim",
        "torch",
        "mean",
        "l",
        "dim",
        "keepdim",
        "torch",
        "cat",
        "max_result",
        "avg_result",
        "spatial",
        "result",
        "sigmoid",
        "l",
        "l_jump",
        "g",
        "maxpool",
        "g",
        "avgpool",
        "g",
        "se",
        "max_result",
        "se",
        "avg_result",
        "sigmoid",
        "max_out",
        "avg_out",
        "sigmoid",
        "output",
        "g_jump",
        "residual",
        "torch",
        "cat",
        "g",
        "l",
        "X_f",
        "drop_rate",
        "dropout",
        "fuse",
        "fuse",
        "nn",
        "Module",
        "inp_dim",
        "out_dim",
        "kernel_size",
        "stride",
        "bn",
        "relu",
        "bias",
        "Conv",
        "inp_dim",
        "nn",
        "Conv2d",
        "inp_dim",
        "out_dim",
        "kernel_size",
        "stride",
        "padding",
        "kernel_size",
        "bias",
        "bias",
        "relu",
        "nn",
        "ReLU",
        "inplace",
        "bn",
        "nn",
        "BatchNorm2d",
        "out_dim",
        "x",
        "x",
        "size",
        "inp_dim",
        "format",
        "x",
        "size",
        "inp_dim",
        "conv",
        "x",
        "bn",
        "bn",
        "x",
        "relu",
        "relu",
        "x",
        "x",
        "nn",
        "Module",
        "inp_dim",
        "out_dim",
        "Residual",
        "nn",
        "ReLU",
        "inplace",
        "nn",
        "BatchNorm2d",
        "inp_dim",
        "Conv",
        "inp_dim",
        "out_dim",
        "relu",
        "nn",
        "BatchNorm2d",
        "out_dim",
        "Conv",
        "out_dim",
        "out_dim",
        "relu",
        "nn",
        "BatchNorm2d",
        "out_dim",
        "Conv",
        "out_dim",
        "out_dim",
        "relu",
        "Conv",
        "inp_dim",
        "out_dim",
        "relu",
        "inp_dim",
        "out_dim",
        "x",
        "need_skip",
        "skip_layer",
        "x",
        "x",
        "bn1",
        "x",
        "relu",
        "conv1",
        "bn2",
        "relu",
        "conv2",
        "bn3",
        "relu",
        "conv3",
        "residual",
        "nn",
        "Module",
        "in_channels",
        "out_channels",
        "nn",
        "Sequential",
        "nn",
        "Conv2d",
        "in_channels",
        "out_channels",
        "kernel_size",
        "padding",
        "nn",
        "BatchNorm2d",
        "out_channels",
        "nn",
        "ReLU",
        "inplace",
        "nn",
        "Conv2d",
        "out_channels",
        "out_channels",
        "kernel_size",
        "padding",
        "nn",
        "BatchNorm2d",
        "out_channels",
        "nn",
        "Sequential",
        "nn",
        "Conv2d",
        "in_channels",
        "out_channels",
        "kernel_size",
        "padding",
        "nn",
        "BatchNorm2d",
        "out_channels",
        "nn",
        "ReLU",
        "inplace",
        "x",
        "relu",
        "double_conv",
        "x",
        "identity",
        "x",
        "nn",
        "Module",
        "in_features",
        "hidden_features",
        "out_features",
        "act_layer",
        "nn",
        "GELU",
        "drop",
        "out_features",
        "in_features",
        "hidden_features",
        "in_features",
        "nn",
        "Linear",
        "in_features",
        "hidden_features",
        "act_layer",
        "nn",
        "Dropout",
        "drop",
        "nn",
        "Linear",
        "hidden_features",
        "out_features",
        "nn",
        "Dropout",
        "drop",
        "x",
        "fc1",
        "x",
        "act",
        "x",
        "drop1",
        "x",
        "fc2",
        "x",
        "drop2",
        "x",
        "x",
        "nn",
        "Module",
        "dim",
        "window_size",
        "num_heads",
        "qkv_bias",
        "attn_drop",
        "proj_drop",
        "dim",
        "window_size",
        "num_heads",
        "dim",
        "num_heads",
        "head_dim",
        "nn",
        "Parameter",
        "torch",
        "zeros",
        "window_size",
        "window_size",
        "num_heads",
        "torch",
        "arange",
        "window_size",
        "torch",
        "arange",
        "window_size",
        "torch",
        "stack",
        "torch",
        "meshgrid",
        "coords_h",
        "coords_w",
        "indexing",
        "torch",
        "flatten",
        "coords",
        "coords_flatten",
        "coords_flatten",
        "relative_coords",
        "permute",
        "contiguous",
        "relative_coords",
        "window_size",
        "relative_coords",
        "window_size",
        "relative_coords",
        "window_size",
        "relative_coords",
        "sum",
        "register_buffer",
        "relative_position_index",
        "nn",
        "Linear",
        "dim",
        "dim",
        "bias",
        "qkv_bias",
        "nn",
        "Dropout",
        "attn_drop",
        "nn",
        "Linear",
        "dim",
        "dim",
        "nn",
        "Dropout",
        "proj_drop",
        "nn",
        "init",
        "trunc_normal_",
        "relative_position_bias_table",
        "std",
        "nn",
        "Softmax",
        "dim",
        "x",
        "mask",
        "Optional",
        "torch",
        "Tensor",
        "x",
        "shape",
        "qkv",
        "x",
        "reshape",
        "B_",
        "N",
        "num_heads",
        "C",
        "num_heads",
        "permute",
        "qkv",
        "unbind",
        "q",
        "scale",
        "q",
        "k",
        "transpose",
        "relative_position_bias_table",
        "relative_position_index",
        "view",
        "view",
        "window_size",
        "window_size",
        "window_size",
        "window_size",
        "relative_position_bias",
        "permute",
        "contiguous",
        "attn",
        "relative_position_bias",
        "unsqueeze",
        "mask",
        "mask",
        "shape",
        "attn",
        "view",
        "B_",
        "nW",
        "nW",
        "num_heads",
        "N",
        "N",
        "mask",
        "unsqueeze",
        "unsqueeze",
        "attn",
        "view",
        "num_heads",
        "N",
        "N",
        "softmax",
        "attn",
        "softmax",
        "attn",
        "attn_drop",
        "attn",
        "attn",
        "v",
        "transpose",
        "reshape",
        "B_",
        "N",
        "C",
        "proj",
        "x",
        "proj_drop",
        "x",
        "x",
        "nn",
        "Module",
        "dim",
        "num_heads",
        "window_size",
        "shift_size",
        "mlp_ratio",
        "qkv_bias",
        "drop",
        "attn_drop",
        "drop_path",
        "act_layer",
        "nn",
        "GELU",
        "norm_layer",
        "nn",
        "LayerNorm",
        "dim",
        "num_heads",
        "window_size",
        "shift_size",
        "mlp_ratio",
        "shift_size",
        "window_size",
        "norm_layer",
        "dim",
        "WindowAttention",
        "dim",
        "window_size",
        "window_size",
        "window_size",
        "num_heads",
        "num_heads",
        "qkv_bias",
        "qkv_bias",
        "attn_drop",
        "attn_drop",
        "proj_drop",
        "drop",
        "DropPath",
        "drop_path",
        "drop_path",
        "nn",
        "Identity",
        "norm_layer",
        "dim",
        "dim",
        "mlp_ratio",
        "Mlp",
        "in_features",
        "dim",
        "hidden_features",
        "mlp_hidden_dim",
        "act_layer",
        "act_layer",
        "drop",
        "drop",
        "x",
        "attn_mask",
        "H",
        "W",
        "x",
        "shape",
        "L",
        "H",
        "W",
        "x",
        "norm1",
        "x",
        "x",
        "view",
        "B",
        "H",
        "W",
        "C",
        "window_size",
        "W",
        "window_size",
        "window_size",
        "window_size",
        "H",
        "window_size",
        "window_size",
        "F",
        "pad",
        "x",
        "pad_l",
        "pad_r",
        "pad_t",
        "pad_b",
        "x",
        "shape",
        "shift_size",
        "torch",
        "roll",
        "x",
        "shifts",
        "shift_size",
        "shift_size",
        "dims",
        "x",
        "window_partition",
        "shifted_x",
        "window_size",
        "x_windows",
        "view",
        "window_size",
        "window_size",
        "C",
        "attn",
        "x_windows",
        "mask",
        "attn_mask",
        "attn_windows",
        "view",
        "window_size",
        "window_size",
        "C",
        "window_reverse",
        "attn_windows",
        "window_size",
        "Hp",
        "Wp",
        "shift_size",
        "torch",
        "roll",
        "shifted_x",
        "shifts",
        "shift_size",
        "shift_size",
        "dims",
        "shifted_x",
        "pad_r",
        "pad_b",
        "x",
        "H",
        "W",
        "contiguous",
        "x",
        "view",
        "B",
        "H",
        "W",
        "C",
        "shortcut",
        "drop_path",
        "x",
        "x",
        "drop_path",
        "mlp",
        "norm2",
        "x",
        "x",
        "nn",
        "Module",
        "dim",
        "depth",
        "num_heads",
        "window_size",
        "mlp_ratio",
        "qkv_bias",
        "drop",
        "attn_drop",
        "drop_path",
        "norm_layer",
        "nn",
        "LayerNorm",
        "downsample",
        "use_checkpoint",
        "dim",
        "depth",
        "window_size",
        "use_checkpoint",
        "window_size",
        "nn",
        "ModuleList",
        "SwinTransformerBlock",
        "dim",
        "dim",
        "num_heads",
        "num_heads",
        "window_size",
        "window_size",
        "shift_size",
        "i",
        "shift_size",
        "mlp_ratio",
        "mlp_ratio",
        "qkv_bias",
        "qkv_bias",
        "drop",
        "drop",
        "attn_drop",
        "attn_drop",
        "drop_path",
        "drop_path",
        "i",
        "isinstance",
        "drop_path",
        "drop_path",
        "norm_layer",
        "norm_layer",
        "i",
        "depth",
        "downsample",
        "downsample",
        "dim",
        "dim",
        "norm_layer",
        "norm_layer",
        "x",
        "H",
        "W",
        "np",
        "ceil",
        "H",
        "window_size",
        "window_size",
        "np",
        "ceil",
        "W",
        "window_size",
        "window_size",
        "torch",
        "zeros",
        "Hp",
        "Wp",
        "device",
        "x",
        "device",
        "window_size",
        "window_size",
        "shift_size",
        "shift_size",
        "window_size",
        "window_size",
        "shift_size",
        "shift_size",
        "h",
        "h_slices",
        "w",
        "w_slices",
        "cnt",
        "cnt",
        "window_partition",
        "img_mask",
        "window_size",
        "mask_windows",
        "view",
        "window_size",
        "window_size",
        "mask_windows",
        "unsqueeze",
        "mask_windows",
        "unsqueeze",
        "attn_mask",
        "masked_fill",
        "attn_mask",
        "masked_fill",
        "attn_mask",
        "attn_mask",
        "x",
        "H",
        "W",
        "downsample",
        "downsample",
        "x",
        "H",
        "W",
        "H",
        "W",
        "create_mask",
        "x",
        "H",
        "W",
        "blk",
        "blocks",
        "blk",
        "H",
        "blk",
        "W",
        "H",
        "W",
        "torch",
        "jit",
        "is_scripting",
        "use_checkpoint",
        "checkpoint",
        "checkpoint",
        "blk",
        "x",
        "attn_mask",
        "blk",
        "x",
        "attn_mask",
        "x",
        "H",
        "W",
        "x",
        "window_size",
        "x",
        "shape",
        "x",
        "view",
        "B",
        "H",
        "window_size",
        "window_size",
        "W",
        "window_size",
        "window_size",
        "C",
        "x",
        "permute",
        "contiguous",
        "view",
        "window_size",
        "window_size",
        "C",
        "windows",
        "windows",
        "window_size",
        "H",
        "W",
        "windows",
        "shape",
        "H",
        "W",
        "window_size",
        "window_size",
        "windows",
        "view",
        "B",
        "H",
        "window_size",
        "W",
        "window_size",
        "window_size",
        "window_size",
        "x",
        "permute",
        "contiguous",
        "view",
        "B",
        "H",
        "W",
        "x",
        "nn",
        "Module",
        "patch_size",
        "in_c",
        "embed_dim",
        "norm_layer",
        "patch_size",
        "patch_size",
        "patch_size",
        "in_c",
        "embed_dim",
        "nn",
        "Conv2d",
        "in_c",
        "embed_dim",
        "kernel_size",
        "patch_size",
        "stride",
        "patch_size",
        "norm_layer",
        "embed_dim",
        "norm_layer",
        "nn",
        "Identity",
        "x",
        "x",
        "shape",
        "H",
        "patch_size",
        "W",
        "patch_size",
        "pad_input",
        "F",
        "pad",
        "x",
        "patch_size",
        "W",
        "patch_size",
        "patch_size",
        "H",
        "patch_size",
        "proj",
        "x",
        "x",
        "shape",
        "x",
        "flatten",
        "transpose",
        "norm",
        "x",
        "x",
        "H",
        "W",
        "nn",
        "Module",
        "dim",
        "norm_layer",
        "nn",
        "LayerNorm",
        "dim",
        "dim",
        "nn",
        "Linear",
        "dim",
        "dim",
        "bias",
        "norm_layer",
        "dim",
        "x",
        "H",
        "W",
        "x",
        "shape",
        "L",
        "H",
        "W",
        "x",
        "view",
        "B",
        "H",
        "W",
        "C",
        "H",
        "W",
        "pad_input",
        "F",
        "pad",
        "x",
        "W",
        "H",
        "x",
        "x",
        "x",
        "x",
        "torch",
        "cat",
        "x0",
        "x1",
        "x2",
        "x3",
        "x",
        "view",
        "B",
        "C",
        "norm",
        "x",
        "reduction",
        "x",
        "x"
    ],
    "literals": [
        "\"channels_first\"",
        "\"channels_first\"",
        "r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n    with shape (batch_size, channels, height, width).\n    \"\"\"",
        "\"channels_last\"",
        "\"channels_last\"",
        "\"channels_first\"",
        "f\"not support data format '{self.data_format}'\"",
        "\"channels_last\"",
        "\"channels_first\"",
        "r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n\n    Args:\n        dim (int): Number of input channels.\n        drop_rate (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"",
        "\"channels_last\"",
        "\"{} {}\"",
        "r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"",
        "\"ij\"",
        "\"relative_position_index\"",
        "r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"",
        "\"shift_size must in 0-window_size\"",
        "\"input feature has wrong size\"",
        "r\"\"\" Patch Merging Layer.\n\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"",
        "\"input feature has wrong size\""
    ],
    "variables": [
        "keep_prob",
        "shape",
        "random_tensor",
        "output",
        "drop_prob",
        "downsample_layers",
        "stem",
        "downsample_layer",
        "stages",
        "dp_rates",
        "cur",
        "stage",
        "conv_norm",
        "conv_head",
        "num_classes",
        "num_layers",
        "embed_dim",
        "patch_norm",
        "num_features",
        "mlp_ratio",
        "patch_embed",
        "pos_drop",
        "dpr",
        "i_layer",
        "layers1",
        "i_layer",
        "layers2",
        "i_layer",
        "layers3",
        "i_layer",
        "layers4",
        "norm",
        "avgpool",
        "head",
        "fu1",
        "fu2",
        "fu3",
        "fu4",
        "x_s",
        "H",
        "W",
        "x_s",
        "x_s_1",
        "H",
        "W",
        "x_s_2",
        "H",
        "W",
        "x_s_3",
        "H",
        "W",
        "x_s_4",
        "H",
        "W",
        "x_s_1",
        "x_s_1",
        "x_s_2",
        "x_s_2",
        "x_s_3",
        "x_s_3",
        "x_s_4",
        "x_s_4",
        "x_c",
        "x_c_1",
        "x_c",
        "x_c_2",
        "x_c",
        "x_c_3",
        "x_c",
        "x_c_4",
        "x_f_1",
        "x_f_2",
        "x_f_3",
        "x_f_4",
        "x_fu",
        "x_fu",
        "weight",
        "bias",
        "eps",
        "data_format",
        "normalized_shape",
        "mean",
        "x",
        "x",
        "dwconv",
        "norm",
        "pwconv1",
        "act",
        "pwconv2",
        "gamma",
        "drop_path",
        "shortcut",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "maxpool",
        "avgpool",
        "se",
        "sigmoid",
        "spatial",
        "W_l",
        "W_g",
        "Avg",
        "Updim",
        "W",
        "W3",
        "relu",
        "residual",
        "dropout",
        "drop_rate",
        "W_local",
        "W_global",
        "W_f",
        "W_f",
        "X_f",
        "X_f",
        "l_jump",
        "max_result",
        "_",
        "avg_result",
        "result",
        "l",
        "l",
        "g_jump",
        "max_result",
        "avg_result",
        "max_out",
        "avg_out",
        "output",
        "g",
        "fuse",
        "inp_dim",
        "conv",
        "relu",
        "bn",
        "relu",
        "bn",
        "x",
        "x",
        "x",
        "relu",
        "bn1",
        "conv1",
        "bn2",
        "conv2",
        "bn3",
        "conv3",
        "skip_layer",
        "need_skip",
        "need_skip",
        "residual",
        "residual",
        "double_conv",
        "identity",
        "relu",
        "out_features",
        "hidden_features",
        "fc1",
        "act",
        "drop1",
        "fc2",
        "drop2",
        "x",
        "x",
        "x",
        "x",
        "x",
        "dim",
        "window_size",
        "num_heads",
        "head_dim",
        "scale",
        "relative_position_bias_table",
        "coords_h",
        "coords_w",
        "coords",
        "coords_flatten",
        "relative_coords",
        "relative_coords",
        "relative_position_index",
        "qkv",
        "attn_drop",
        "proj",
        "proj_drop",
        "softmax",
        "B_",
        "N",
        "C",
        "qkv",
        "q",
        "k",
        "v",
        "q",
        "attn",
        "relative_position_bias",
        "relative_position_bias",
        "attn",
        "nW",
        "attn",
        "attn",
        "attn",
        "attn",
        "attn",
        "x",
        "x",
        "x",
        "dim",
        "num_heads",
        "window_size",
        "shift_size",
        "mlp_ratio",
        "norm1",
        "attn",
        "drop_path",
        "norm2",
        "mlp_hidden_dim",
        "mlp",
        "H",
        "W",
        "B",
        "L",
        "C",
        "shortcut",
        "x",
        "x",
        "pad_l",
        "pad_t",
        "pad_r",
        "pad_b",
        "x",
        "_",
        "Hp",
        "Wp",
        "_",
        "shifted_x",
        "shifted_x",
        "attn_mask",
        "x_windows",
        "x_windows",
        "attn_windows",
        "attn_windows",
        "shifted_x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "x",
        "dim",
        "depth",
        "window_size",
        "use_checkpoint",
        "shift_size",
        "blocks",
        "downsample",
        "downsample",
        "Hp",
        "Wp",
        "img_mask",
        "h_slices",
        "w_slices",
        "cnt",
        "img_mask",
        "h",
        "w",
        "mask_windows",
        "mask_windows",
        "attn_mask",
        "attn_mask",
        "x",
        "H",
        "W",
        "attn_mask",
        "x",
        "x",
        "B",
        "H",
        "W",
        "C",
        "x",
        "windows",
        "B",
        "x",
        "x",
        "patch_size",
        "patch_size",
        "in_chans",
        "embed_dim",
        "proj",
        "norm",
        "_",
        "_",
        "H",
        "W",
        "pad_input",
        "x",
        "x",
        "_",
        "_",
        "H",
        "W",
        "x",
        "x",
        "dim",
        "dim",
        "reduction",
        "norm",
        "B",
        "L",
        "C",
        "x",
        "pad_input",
        "x",
        "x0",
        "x1",
        "x2",
        "x3",
        "x",
        "x",
        "x",
        "x"
    ],
    "comments": [
        "work with diff dim tensors, not just 2D ConvNets",
        "binarize",
        "ConvNeXt Branch Setting #######",
        "stem + 3 stage downsample",
        "stage2-4 downsample",
        "4 feature resolution stages, each consisting of multiple blocks",
        "Build stacks of blocks in each stage",
        "final norm layer",
        "Swin Transformer Branch Setting ######",
        "The channels of stage4 output feature matrix",
        "split image into non-overlapping patches",
        "stochastic depth",
        "stochastic depth decay rule",
        "Hierachical Feature Fusion Block Setting #######",
        "Swin Transformer Branch ######",
        "[B,L,C] ---> [B,C,H,W]",
        "ConvNeXt Branch ######",
        "Hierachical Feature Fusion Path ######",
        "global average pooling, (N, C, H, W) -> (N, C)",
        "ConvNeXt Component #####",
        "[batch_size, channels, height, width]",
        "depthwise conv",
        "pointwise/1x1 convs, implemented with linear layers",
        "[N, C, H, W] -> [N, H, W, C]",
        "[N, H, W, C] -> [N, C, H, W]",
        "Hierachical Feature Fusion Block",
        "local feature from ConvNeXt",
        "global feature from SwinTransformer",
        "spatial attention for ConvNeXt branch",
        "channel attetion for transformer branch",
        "SwinTransformer",
        "[Mh, Mw]",
        "define a parameter table of relative position bias",
        "[2*Mh-1 * 2*Mw-1, nH]",
        "get pair-wise relative position index for each token inside the window",
        "[2, Mh, Mw]",
        "[2, Mh*Mw]",
        "[2, Mh*Mw, 1] - [2, 1, Mh*Mw]",
        "[2, Mh*Mw, Mh*Mw]",
        "[Mh*Mw, Mh*Mw, 2]",
        "shift to start from 0",
        "[Mh*Mw, Mh*Mw]",
        "[batch_size*num_windows, Mh*Mw, total_embed_dim]",
        "qkv(): -> [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]",
        "reshape: -> [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]",
        "permute: -> [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]",
        "[batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]",
        "make torchscript happy (cannot use tensor as tuple)",
        "transpose: -> [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]",
        "@: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]",
        "relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -> [Mh*Mw,Mh*Mw,nH]",
        "[nH, Mh*Mw, Mh*Mw]",
        "mask: [nW, Mh*Mw, Mh*Mw]",
        "num_windows",
        "attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]",
        "mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]",
        "@: multiply -> [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]",
        "transpose: -> [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]",
        "reshape: -> [batch_size*num_windows, Mh*Mw, total_embed_dim]",
        "cyclic shift",
        "partition windows",
        "[nW*B, Mh, Mw, C]",
        "[nW*B, Mh*Mw, C]",
        "W-MSA/SW-MSA",
        "[nW*B, Mh*Mw, C]",
        "merge windows",
        "[nW*B, Mh, Mw, C]",
        "[B, H', W', C]",
        "reverse cyclic shift",
        "build blocks",
        "patch merging layer",
        "calculate attention mask for SW-MSA",
        "[1, Hp, Wp, 1]",
        "[nW, Mh, Mw, 1]",
        "[nW, Mh*Mw]",
        "[nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]",
        "[nW, Mh*Mw, Mh*Mw]",
        "patch merging stage2 in [6,3136,96] out [6,784,192]",
        "[nW, Mh*Mw, Mh*Mw]",
        "swin block",
        "permute: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H//Mh, W//Mh, Mw, Mw, C]",
        "view: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B*num_windows, Mh, Mw, C]",
        "view: [B*num_windows, Mh, Mw, C] -> [B, H//Mh, W//Mw, Mh, Mw, C]",
        "permute: [B, H//Mh, W//Mw, Mh, Mw, C] -> [B, H//Mh, Mh, W//Mw, Mw, C]",
        "view: [B, H//Mh, Mh, W//Mw, Mw, C] -> [B, H, W, C]",
        "padding",
        "如果输入图片的H，W不是patch_size的整数倍，需要进行padding",
        "to pad the last 3 dimensions,",
        "(W_left, W_right, H_top,H_bottom, C_front, C_back)",
        "下采样patch_size倍",
        "flatten: [B, C, H, W] -> [B, C, HW]",
        "transpose: [B, C, HW] -> [B, HW, C]",
        "padding",
        "to pad the last 3 dimensions, starting from the last dimension and moving forward.",
        "(C_front, C_back, W_left, W_right, H_top, H_bottom)",
        "[B, H/2, W/2, C]",
        "[B, H/2, W/2, C]",
        "[B, H/2, W/2, C]",
        "[B, H/2, W/2, C]",
        "[B, H/2, W/2, 4*C]",
        "[B, H/2*W/2, 4*C]",
        "[B, H/2*W/2, 2*C]"
    ],
    "docstrings": [
        "\"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"",
        "\"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"",
        "\"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n    \"\"\"",
        "\"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, Mh*Mw, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"",
        "\"\"\"\n    A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n    \"\"\"",
        "\"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size(M)\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"",
        "\"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size(M)\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "\"\"\"\n    2D Image to Patch Embedding\n    \"\"\"",
        "\"\"\"\n        x: B, H*W, C\n        \"\"\""
    ],
    "functions": [
        "drop_path_f",
        "forward",
        "_init_weights",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "forward",
        "create_mask",
        "forward",
        "window_partition",
        "window_reverse",
        "forward",
        "forward"
    ],
    "classes": [
        "DropPath",
        "main_model",
        "LayerNorm",
        "Block",
        "HFF_block",
        "Conv",
        "Residual",
        "DoubleConv",
        "Mlp",
        "WindowAttention",
        "SwinTransformerBlock",
        "BasicLayer",
        "PatchEmbed",
        "PatchMerging"
    ]
}