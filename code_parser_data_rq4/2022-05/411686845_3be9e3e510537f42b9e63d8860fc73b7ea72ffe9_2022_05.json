{
    "identifiers": [
        "model",
        "numpy",
        "np",
        "obs",
        "action",
        "reward",
        "numberactions",
        "numberpos",
        "numbervelo",
        "obs",
        "action",
        "reward",
        "numberactions",
        "model",
        "env",
        "observation_space",
        "high",
        "model",
        "env",
        "observation_space",
        "low",
        "np",
        "array",
        "np",
        "round",
        "num_states",
        "astype",
        "num_states",
        "np",
        "random",
        "uniform",
        "low",
        "high",
        "size",
        "num_states",
        "num_states",
        "model",
        "env",
        "action_space",
        "n",
        "qtable",
        "num_states",
        "np",
        "array",
        "i",
        "model",
        "env",
        "action_space",
        "n",
        "j",
        "num_states",
        "epsilon",
        "np",
        "random",
        "random",
        "epsilon",
        "np",
        "argmax",
        "qtable",
        "obs",
        "obs",
        "np",
        "random",
        "choice",
        "numberactions",
        "alpha",
        "gamma",
        "epsilon",
        "obs",
        "qtable",
        "oldstate",
        "oldstate",
        "action",
        "reward",
        "done",
        "model",
        "env",
        "step",
        "action",
        "done",
        "observation",
        "discretize_state",
        "observation",
        "oldq",
        "round",
        "alpha",
        "oldq",
        "discretize_state",
        "observation",
        "oldq",
        "round",
        "alpha",
        "reward",
        "gamma",
        "np",
        "max",
        "qtable",
        "obs",
        "obs",
        "oldq",
        "qtable",
        "oldstate",
        "oldstate",
        "action",
        "newq",
        "observation",
        "numbervelo",
        "numberpos",
        "observation",
        "model",
        "env",
        "observation_space",
        "low",
        "np",
        "array",
        "np",
        "round",
        "state_adj",
        "astype"
    ],
    "literals": [],
    "variables": [
        "obs",
        "action",
        "reward",
        "numberactions",
        "done",
        "num_states",
        "num_states",
        "qtable",
        "action",
        "action",
        "oldstate",
        "oldq",
        "observation",
        "info",
        "newq",
        "newq",
        "state_adj",
        "obs"
    ],
    "comments": [
        "print(\"HELLO\")",
        "diskretize from another programm",
        "terminal state should have value zero",
        "self.qtable = np.array([[[ 0.5 for k in range(numberactions)] for j in range(numbervelo)] for i in range(numberpos)], float)",
        "self.qtable = np.random.uniform(low=-1, high=1,size=(numberpos, numbervelo, numberactions))",
        "self.qtable = [[[ None for k in range(numbervelo)] for j in range(numberpos)] for i in [0,1,2]] hier Actions außerhalb, für Vergleich effizienter innerhalb",
        "epsilon greedy policy",
        "take a random action",
        "qlearning",
        "step, action decided by act()",
        "terminal state",
        "in terminal state reward is zero and qtable value is initialized zero",
        "from another project"
    ],
    "docstrings": [
        "\"\"\"\n    Ursprüngliche Diskretisierung\n    \n    def discretize_pos(observation, numberpos = 20):#von -1.2 bis 0.6\n        # Input Array of length two with two observations\n        # Output discretized value of position\n        if observation[0] < -1:#eigentlich -1,2\n            return 0\n        if observation[0] > 0.5:#eigentlich 0.5\n            return numberpos - 1\n        else:\n            return round(((observation[0]+1)/1.5)*(numberpos-2))\n\n\n    def discretize_velo(observation, numbervelo = 15):#von -0.07 bis 0.07\n        # Input Array of length two with two observations\n        # Output discretized value of velocity\n        if observation[1] < -0.05:\n            return 0\n        if observation[1] > 0.05:\n            return numbervelo - 1\n        else:\n            return round(((observation[1]+0.05)/0.1)*(numbervelo-2))\n            \n    def maxaction(self):#if there are actions with equal values a random action of those will be choosen\n        smallq = self.qtable[self.obs[0]][self.obs[1]]\n        listmaxactions = np.argwhere(smallq == np.amax(smallq))\n        if len(listmaxactions) == 1:\n            return listmaxactions[0][0]\n        else:\n            listmaxactions2 = np.array([ob[0] for ob in listmaxactions])\n            return np.random.choice(listmaxactions2)\n    \n\n    def discretize_state(self, observation, numbervelo = 15,numberpos = 20):\n        #self.obs[0] = QLearnAgent.discretize_pos(observation, numberpos)\n        #self.obs[1] = QLearnAgent.discretize_velo(observation, numbervelo)\n    \n    \"\"\""
    ],
    "functions": [
        "act",
        "learn",
        "discretize_state"
    ],
    "classes": [
        "QLearnAgent"
    ]
}