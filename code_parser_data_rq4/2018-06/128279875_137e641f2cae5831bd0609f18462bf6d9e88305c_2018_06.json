{
    "identifiers": [
        "subprocess",
        "re",
        "os",
        "sys",
        "cmd",
        "subprocess",
        "Popen",
        "cmd",
        "stdout",
        "subprocess",
        "PIPE",
        "shell",
        "communicate",
        "output",
        "decode",
        "run_command",
        "re",
        "compile",
        "line",
        "output",
        "strip",
        "split",
        "gpu_regex",
        "match",
        "line",
        "m",
        "line",
        "result",
        "append",
        "m",
        "group",
        "result",
        "run_command",
        "output",
        "output",
        "find",
        "re",
        "compile",
        "gpu_output",
        "split",
        "gpu_id",
        "gpu_id",
        "list_available_gpus",
        "row",
        "gpu_output",
        "split",
        "memory_regex",
        "search",
        "row",
        "m",
        "m",
        "group",
        "m",
        "group",
        "result",
        "gpu_id",
        "gpu_memory",
        "result",
        "memory",
        "gpu_id",
        "gpu_id",
        "memory",
        "gpu_memory_map",
        "items",
        "sorted",
        "memory_gpu_map",
        "best_gpu",
        "sys",
        "modules",
        "pick_gpu_lowest_memory",
        "gpu_id",
        "os",
        "environ",
        "os",
        "environ",
        "gpu_id",
        "sys",
        "modules",
        "os",
        "environ"
    ],
    "literals": [
        "\"ascii\"",
        "\"nvidia-smi -L\"",
        "r\"GPU (?P<gpu_id>\\d+):\"",
        "\"\\n\"",
        "\"Couldnt parse \"",
        "\"gpu_id\"",
        "\"nvidia-smi\"",
        "\"GPU Memory\"",
        "r\"[|]\\s+?(?P<gpu_id>\\d+)\\D+?(?P<pid>\\d+).+[ ](?P<gpu_memory>\\d+)MiB\"",
        "\"\\n\"",
        "\"\\n\"",
        "\"gpu_id\"",
        "\"gpu_memory\"",
        "'tensorflow'",
        "\"GPU setup must happen before importing TensorFlow\"",
        "\"Picking GPU \"",
        "\"CUDA_DEVICE_ORDER\"",
        "\"PCI_BUS_ID\"",
        "\"CUDA_VISIBLE_DEVICES\"",
        "'tensorflow'",
        "\"Warning, GPU setup must happen before importing TensorFlow\"",
        "\"CUDA_VISIBLE_DEVICES\"",
        "''"
    ],
    "variables": [
        "output",
        "output",
        "gpu_regex",
        "result",
        "m",
        "output",
        "gpu_output",
        "memory_regex",
        "rows",
        "result",
        "m",
        "gpu_id",
        "gpu_memory",
        "memory_gpu_map",
        "best_memory",
        "best_gpu",
        "gpu_id"
    ],
    "comments": [
        "https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py",
        "You can then put it in utils.py and set GPU in your TensorFlow script before first tensorflow import. IE",
        "import utils",
        "import os",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(utils.pick_gpu_lowest_memory())",
        "import tensorflow",
        "GPU picking",
        "http://stackoverflow.com/a/41638727/419116",
        "Nvidia-smi GPU memory parsing.",
        "Tested on nvidia-smi 370.23",
        "lines of the form GPU 0: TITAN X",
        "lines of the form",
        "|    0      8734    C   python                                       11705MiB |"
    ],
    "docstrings": [
        "\"\"\"Run command, return output as string.\"\"\"",
        "\"\"\"Returns list of available GPU ids.\"\"\"",
        "\"\"\"Returns map of GPU id to memory allocated on that GPU.\"\"\"",
        "\"\"\"Returns GPU with the least allocated memory\"\"\""
    ],
    "functions": [
        "run_command",
        "list_available_gpus",
        "gpu_memory_map",
        "pick_gpu_lowest_memory",
        "setup_one_gpu",
        "setup_no_gpu"
    ],
    "classes": []
}