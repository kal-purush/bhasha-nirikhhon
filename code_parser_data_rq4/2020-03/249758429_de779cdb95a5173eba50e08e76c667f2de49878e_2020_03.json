{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "autograd",
        "Variable",
        "torch",
        "nn",
        "functional",
        "F",
        "nn",
        "Module",
        "batch_size",
        "output_size",
        "hidden_size",
        "vocab_size",
        "embedding_length",
        "weights",
        "RNN",
        "batch_size",
        "output_size",
        "hidden_size",
        "vocab_size",
        "embedding_length",
        "nn",
        "Embedding",
        "vocab_size",
        "embedding_length",
        "word_embeddings",
        "nn",
        "Parameter",
        "weights",
        "requires_grad",
        "nn",
        "RNN",
        "embedding_length",
        "hidden_size",
        "num_layers",
        "bidirectional",
        "nn",
        "Linear",
        "hidden_size",
        "output_size",
        "input_sentences",
        "batch_size",
        "word_embeddings",
        "input_sentences",
        "input",
        "permute",
        "batch_size",
        "Variable",
        "torch",
        "zeros",
        "batch_size",
        "hidden_size",
        "Variable",
        "torch",
        "zeros",
        "batch_size",
        "hidden_size",
        "rnn",
        "input",
        "h_0",
        "h_n",
        "permute",
        "h_n",
        "contiguous",
        "view",
        "h_n",
        "size",
        "h_n",
        "size",
        "h_n",
        "size",
        "label",
        "h_n",
        "logits"
    ],
    "literals": [],
    "variables": [
        "batch_size",
        "output_size",
        "hidden_size",
        "vocab_size",
        "embedding_length",
        "word_embeddings",
        "weight",
        "rnn",
        "label",
        "input",
        "input",
        "h_0",
        "h_0",
        "output",
        "h_n",
        "h_n",
        "h_n",
        "logits"
    ],
    "comments": [
        "4 = num_layers*num_directions",
        "h_n.size() = (4, batch_size, hidden_size)",
        "h_n.size() = (batch_size, 4, hidden_size)",
        "h_n.size() = (batch_size, 4*hidden_size)",
        "logits.size() = (batch_size, output_size)"
    ],
    "docstrings": [
        "\"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 2 = (pos, neg)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        \"\"\"",
        "\"\"\"\n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n        logits.size() = (batch_size, output_size)\n\n        \"\"\""
    ],
    "functions": [
        "forward"
    ],
    "classes": [
        "RNN"
    ]
}