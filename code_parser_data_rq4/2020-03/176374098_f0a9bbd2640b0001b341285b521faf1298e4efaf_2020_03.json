{
    "identifiers": [
        "contextlib",
        "threading",
        "threading",
        "local",
        "contextlib",
        "contextmanager",
        "quantize",
        "should_quantize",
        "quantize",
        "quantize",
        "backup",
        "getattr",
        "_quantized_scope",
        "contextlib",
        "contextmanager",
        "metrics",
        "metric",
        "metrics",
        "metric",
        "_available_metrics",
        "ValueError",
        "metric",
        "_available_metrics",
        "_global_training_metrics",
        "copy",
        "_global_training_metrics",
        "update",
        "metrics",
        "_global_training_metrics",
        "_global_training_metrics",
        "clear",
        "_global_training_metrics",
        "update",
        "backup",
        "_global_training_metrics"
    ],
    "literals": [
        "\"metrics_scope\"",
        "\"quantized_scope\"",
        "\"get_training_metrics\"",
        "\"should_quantize\"",
        "\"should_quantize\"",
        "\"flip_ratio\"",
        "f\"Unknown training metric '{metric}'. Available metrics: {_available_metrics}.\""
    ],
    "variables": [
        "__all__",
        "_quantized_scope",
        "_quantized_scope",
        "should_quantize",
        "backup",
        "_quantized_scope",
        "should_quantize",
        "_quantized_scope",
        "should_quantize",
        "_global_training_metrics",
        "_available_metrics",
        "backup"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\"Context managers that configure global behaviour of Larq.\"\"\"",
        "\"\"\"A context manager to define the behaviour of `QuantizedVariable`.\n\n    !!! example\n        ```python\n        model.save(\"full_precision_model.h5\")  # save full precision latent weights\n        fp_weights = model.get_weights()  # get latent weights\n\n        with larq.context.quantized_scope(True):\n            model.save(\"binary_model.h5\")  # save binarized weights\n            weights = model.get_weights()  # get binarized weights\n        ```\n\n    # Arguments\n    quantize: If `should_quantize` is `True`, `QuantizedVariable` will return their\n        quantized value in the forward pass. If `False`, `QuantizedVariable` will act\n        as a latent variable.\n    \"\"\"",
        "\"\"\"Returns the current quantized scope.\"\"\"",
        "\"\"\"A context manager to set the training metrics to be used in quantizers.\n\n    !!! example\n        ```python\n        with larq.context.metrics_scope([\"flip_ratio\"]):\n            model = tf.keras.models.Sequential(\n                [larq.layers.QuantDense(3, kernel_quantizer=\"ste_sign\", input_shape=(32,))]\n            )\n        model.compile(loss=\"mse\", optimizer=\"sgd\")\n        ```\n\n    # Arguments\n    metrics: Iterable of metrics to add to quantizers defined inside this context.\n        Currently only the `flip_ratio` metric is available.\n    \"\"\"",
        "\"\"\"Retrieves a live reference to the training metrics in the current scope.\n\n    Updating and clearing training metrics using `larq.context.metrics_scope` is\n    preferred, but `get_training_metrics` can be used to directly access them.\n\n    !!! example\n        ```python\n        get_training_metrics().clear()\n        get_training_metrics().add(\"flip_ratio\")\n        ```\n\n    # Returns\n    A set of training metrics in the current scope.\n    \"\"\""
    ],
    "functions": [
        "quantized_scope",
        "should_quantize",
        "metrics_scope",
        "get_training_metrics"
    ],
    "classes": []
}