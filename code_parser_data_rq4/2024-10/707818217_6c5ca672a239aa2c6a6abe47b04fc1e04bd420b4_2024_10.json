{
    "identifiers": [
        "dataclasses",
        "dataclass",
        "typing",
        "Optional",
        "torch",
        "dataclass",
        "torch",
        "Tensor",
        "torch",
        "Tensor",
        "Optional",
        "torch",
        "Tensor",
        "Optional",
        "torch",
        "Tensor",
        "input_lengths",
        "cache_lengths",
        "cu_seqlen_q",
        "max_q",
        "max_k",
        "input_lengths",
        "cache_lengths",
        "input_lengths",
        "device",
        "input_lengths",
        "shape",
        "cu_seqlen_q",
        "torch",
        "arange",
        "shape",
        "device",
        "device",
        "dtype",
        "torch",
        "int32",
        "max_q",
        "max_k",
        "torch",
        "zeros",
        "shape",
        "device",
        "device",
        "dtype",
        "torch",
        "int32",
        "input_lengths",
        "cache_lengths",
        "torch",
        "cumsum",
        "total",
        "cu_seqlen_k",
        "cu_seqlen_q",
        "cu_seqlen_k",
        "max_q",
        "max_k",
        "max",
        "torch",
        "clamp",
        "input_lengths",
        "max",
        "max"
    ],
    "literals": [],
    "variables": [
        "input_lengths",
        "cache_lengths",
        "cu_seqlen_q",
        "cu_seqlen_k",
        "max_q",
        "max_k",
        "input_lengths",
        "cache_lengths",
        "device",
        "shape",
        "cu_seqlen_q",
        "max_q",
        "cu_seqlen_k",
        "total",
        "cu_seqlen_q",
        "cu_seqlen_k",
        "max_q",
        "max_k",
        "input_lengths"
    ],
    "comments": [
        "cuda graphs don't like this and this is necessary to clamp within mistral",
        "Although FA2 might not want the clamping",
        "cu_seqlen_k[0] = 0"
    ],
    "docstrings": [],
    "functions": [
        "clamp"
    ],
    "classes": [
        "Seqlen"
    ]
}