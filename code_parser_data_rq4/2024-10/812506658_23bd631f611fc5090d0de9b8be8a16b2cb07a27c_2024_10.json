{
    "identifiers": [
        "torch",
        "dataclasses",
        "dataclass",
        "typing",
        "TYPE_CHECKING",
        "Any",
        "Dict",
        "Optional",
        "Tuple",
        "Type",
        "torch",
        "vllm",
        "_custom_ops",
        "ops",
        "vllm",
        "attention",
        "backends",
        "AttentionType",
        "query",
        "torch",
        "Tensor",
        "key",
        "torch",
        "Tensor",
        "value",
        "torch",
        "Tensor",
        "kv_cache",
        "torch",
        "Tensor",
        "attn_metadata",
        "k_scale",
        "v_scale",
        "attn_type",
        "AttentionType",
        "AttentionType",
        "DECODER",
        "torch",
        "Tensor",
        "attn_type",
        "AttentionType",
        "DECODER",
        "NotImplementedError",
        "k_scale",
        "v_scale",
        "query",
        "shape",
        "query",
        "view",
        "num_heads",
        "head_size",
        "key",
        "view",
        "num_kv_heads",
        "head_size",
        "value",
        "view",
        "num_kv_heads",
        "head_size",
        "kv_cache",
        "kv_cache",
        "kv_cache",
        "torch",
        "ops",
        "vllm",
        "reshape_and_cache_flash",
        "key",
        "value",
        "kv_cache",
        "attn_metadata",
        "slot_mapping",
        "flatten",
        "kv_cache_dtype",
        "k_scale",
        "v_scale",
        "attn_metadata",
        "num_prefill_tokens",
        "attn_metadata",
        "num_decode_tokens",
        "key",
        "shape",
        "query",
        "shape",
        "key",
        "shape",
        "value",
        "shape",
        "num_kv_tokens",
        "query",
        "shape",
        "num_prefill_tokens",
        "attn_metadata",
        "prefill_meta",
        "kv_cache",
        "prefill_meta",
        "block_tables",
        "prefill_meta",
        "block_tables",
        "numel",
        "torch",
        "ops",
        "vllm",
        "flash_attn_varlen_func",
        "q",
        "query",
        "k",
        "key",
        "v",
        "value",
        "cu_seqlens_q",
        "prefill_meta",
        "query_start_loc",
        "cu_seqlens_k",
        "prefill_meta",
        "seq_start_loc",
        "max_seqlen_q",
        "prefill_meta",
        "max_prefill_seq_len",
        "max_seqlen_k",
        "prefill_meta",
        "max_prefill_seq_len",
        "softmax_scale",
        "scale",
        "causal",
        "window_size",
        "sliding_window",
        "alibi_slopes",
        "alibi_slopes",
        "softcap",
        "logits_soft_cap",
        "prefill_meta",
        "seq_lens",
        "max",
        "prefill_meta",
        "seq_lens",
        "torch",
        "ops",
        "vllm",
        "flash_attn_varlen_func",
        "q",
        "query",
        "k",
        "key_cache",
        "v",
        "value_cache",
        "cu_seqlens_q",
        "prefill_meta",
        "query_start_loc",
        "max_seqlen_q",
        "prefill_meta",
        "max_query_len",
        "cu_seqlens_k",
        "prefill_meta",
        "seq_start_loc",
        "max_seqlen_k",
        "max_seq_len",
        "softmax_scale",
        "scale",
        "causal",
        "alibi_slopes",
        "alibi_slopes",
        "block_table",
        "prefill_meta",
        "block_tables",
        "softcap",
        "logits_soft_cap",
        "prefill_output",
        "prefill_output",
        "view",
        "num_prefill_tokens",
        "hidden_size",
        "key",
        "shape",
        "num_prefill_tokens",
        "num_decode_tokens",
        "value",
        "shape",
        "num_prefill_tokens",
        "num_decode_tokens",
        "query",
        "num_prefill_tokens",
        "query",
        "num_prefill_tokens",
        "key",
        "num_prefill_tokens",
        "value",
        "num_prefill_tokens",
        "query",
        "shape",
        "num_prefill_tokens",
        "decode_query",
        "shape",
        "num_decode_tokens",
        "Optional",
        "torch",
        "Tensor",
        "Optional",
        "torch",
        "Tensor",
        "prefill_meta",
        "attn_metadata",
        "prefill_metadata",
        "kv_cache",
        "prefill_meta",
        "block_tables",
        "prefill_meta",
        "block_tables",
        "numel",
        "torch",
        "ops",
        "vllm",
        "flash_attn_varlen_func",
        "q",
        "query",
        "k",
        "key",
        "v",
        "value",
        "cu_seqlens_q",
        "prefill_meta",
        "seq_start_loc",
        "cu_seqlens_k",
        "prefill_meta",
        "seq_start_loc",
        "max_seqlen_q",
        "prefill_meta",
        "max_prefill_seq_len",
        "max_seqlen_k",
        "prefill_meta",
        "max_prefill_seq_len",
        "softmax_scale",
        "scale",
        "causal",
        "window_size",
        "sliding_window",
        "alibi_slopes",
        "alibi_slopes",
        "softcap",
        "logits_soft_cap",
        "prefill_meta",
        "seq_lens",
        "max",
        "prefill_meta",
        "seq_lens",
        "torch",
        "ops",
        "vllm",
        "flash_attn_varlen_func",
        "q",
        "query",
        "k",
        "key_cache",
        "v",
        "value_cache",
        "cu_seqlens_q",
        "prefill_meta",
        "query_start_loc",
        "max_seqlen_q",
        "prefill_meta",
        "max_query_len",
        "cu_seqlens_k",
        "prefill_meta",
        "seq_start_loc",
        "max_seqlen_k",
        "max_seq_len",
        "softmax_scale",
        "scale",
        "causal",
        "alibi_slopes",
        "alibi_slopes",
        "block_table",
        "prefill_meta",
        "block_tables",
        "softcap",
        "logits_soft_cap",
        "decode_meta",
        "attn_metadata",
        "decode_metadata",
        "torch",
        "ops",
        "vllm",
        "flash_attn_with_kvcache",
        "decode_query",
        "unsqueeze",
        "key_cache",
        "value_cache",
        "block_table",
        "decode_meta",
        "block_tables",
        "cache_seqlens",
        "decode_meta",
        "seq_lens_tensor",
        "softmax_scale",
        "scale",
        "causal",
        "alibi_slopes",
        "alibi_slopes",
        "softcap",
        "logits_soft_cap",
        "squeeze",
        "prefill_output",
        "decode_output",
        "decode_output",
        "view",
        "num_decode_tokens",
        "hidden_size",
        "decode_output",
        "prefill_output",
        "prefill_output",
        "view",
        "num_prefill_tokens",
        "hidden_size",
        "torch",
        "cat",
        "prefill_output",
        "decode_output",
        "dim",
        "output",
        "view",
        "num_tokens",
        "hidden_size",
        "vllm",
        "attention",
        "backends",
        "flash_attn",
        "vllm",
        "attention",
        "backends",
        "flash_attn",
        "FlashAttentionImpl",
        "flash_attn_forward_for_cacheblend"
    ],
    "literals": [
        "\"FlashAttentionMetadata\"",
        "\"Encoder self-attention and \"",
        "\"encoder/decoder cross-attention \"",
        "\"are not implemented for \"",
        "\"FlashAttentionImpl\"",
        "\"key/v_scale is not supported in FlashAttention.\""
    ],
    "variables": [
        "num_tokens",
        "hidden_size",
        "query",
        "key",
        "value",
        "key_cache",
        "value_cache",
        "num_prefill_tokens",
        "num_decode_tokens",
        "num_kv_tokens",
        "prefill_meta",
        "prefill_output",
        "max_seq_len",
        "prefill_output",
        "decode_query",
        "query",
        "key",
        "value",
        "prefill_output",
        "decode_output",
        "prefill_output",
        "max_seq_len",
        "prefill_output",
        "decode_output",
        "output",
        "forward"
    ],
    "comments": [
        "NOTE(woosuk): FlashAttention does not support FP8 KV cache.",
        "Reshape the query, key, and value tensors.",
        "Reshape the input keys and values and store them in the cache.",
        "If kv_cache is not provided, the new key and value tensors are",
        "not cached. This happens during the initial memory profiling run.",
        "Injection for CacheBlend",
        "Cache blend forward",
        "In the cacheblend case, prefill_meta must be not None",
        "normal attention",
        "When block_tables are not filled, it means q and k are the",
        "prompt, and they have the same length.",
        "prefix-enabled attention",
        "noqa",
        "End of injection",
        "Query for decode. KV is not needed because it is already cached.",
        "QKV for prefill.",
        "Prompt run.",
        "normal attention",
        "When block_tables are not filled, it means q and k are the",
        "prompt, and they have the same length.",
        "prefix-enabled attention",
        "noqa",
        "Decoding run."
    ],
    "docstrings": [
        "\"\"\"Forward pass with FlashAttention.\n\n    Args:\n        query: shape = [num_tokens, num_heads * head_size]\n        key: shape = [num_tokens, num_kv_heads * head_size]\n        value: shape = [num_tokens, num_kv_heads * head_size]\n        kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]\n        attn_metadata: Metadata for attention.\n    Returns:\n        shape = [num_tokens, num_heads * head_size]\n    \"\"\""
    ],
    "functions": [
        "flash_attn_forward_for_cacheblend",
        "inject_flash_attn"
    ],
    "classes": []
}