{
    "identifiers": [
        "llmware",
        "models",
        "ModelCatalog",
        "llmware",
        "prompts",
        "Prompt",
        "llmware",
        "configs",
        "LLMWareConfig",
        "ModelCatalog",
        "register_new_model_card",
        "model_card_dict",
        "ModelCatalog",
        "load_model",
        "model",
        "inference",
        "Prompt",
        "load_model",
        "LLMWareConfig",
        "get_model_repo_path",
        "model_path",
        "ModelCatalog",
        "register_new_finetune_wrapper",
        "main_start",
        "main_stop",
        "llm_start",
        "system_start",
        "system_stop"
    ],
    "literals": [
        "\"model_name\"",
        "\"my_model\"",
        "\"insert other params from above...\"",
        "\"my_model\"",
        "\"What is ...\"",
        "\"my_model\"",
        "\"local model path: \"",
        "\"my_new_template\"",
        "\"<user starts here>\"",
        "\"<user ends here>\"",
        "\"<model starts here>\"",
        "\"<you are useful assistant...\"",
        "\"<end system stuff>\""
    ],
    "variables": [
        "model_card_dict",
        "model",
        "response",
        "prompter",
        "model_path"
    ],
    "comments": [
        "Create model card and register in the ModelCatalog",
        "create the model card dictionary manually using the templates above as guides, e.g.,",
        "this is the key step - registering the model card - add as a first line in any script/example",
        "once the model is registered in the catalog, it can then be accessed anytime by name, e.g.,",
        "or if using in conjunction with building a RAG prompt",
        "lookup model repo path",
        "You can manually put the model components in a folder called \"model_name\" at the model repo path, and",
        "'lookups' will all work.",
        "if none of these templates work, then you can also register a new prompt template",
        "once registered, this new prompt wrapper can also be invoked directly by \"my_new_template\", and it will be",
        "picked up in the lookup at the time of instantiating the model"
    ],
    "docstrings": [
        "\"\"\" This example shows how to add a custom or private OpenVino or ONNX model to the llmware model catalog.\n\n    Over the next few releases, we will be expanding the default ModelCatalog considerably, but for the time\n    being, please feel free to follow the steps below to build your own custom catalog.\n\n    We show below templates for the model card dictionaries - most of which is fairly easy to build for a given\n    model.\n\n    We highlight both the main step - which is a simple one-liner to register the model, and then provide\n    more details on three potential troubleshooting items:\n\n        1 - using a model from a custom/private path - and 'inserting' directly into the model_repo lookup\n        2 - identifying the prompt wrapper template\n        3 - customizing a new prompt wrapper\n\n\"\"\"",
        "\"\"\"             Sample OpenVino Model Card template\n\n model_card_dict = {\"model_name\": \"phi-3-ov\", \"model_family\": \"OVGenerativeModel\",\n                    \"model_category\": \"generative_local\", \"display_name\": \"phi-3-ov\",\n                    \"model_location\": \"llmware_repo\",\n                    \"context_window\": 4096, \"instruction_following\": False, \"prompt_wrapper\": \"phi_3\",\n                    \"temperature\": 0.0, \"sample_default\": False, \"trailing_space\": \"\",\n                    \"tokenizer_local\": \"tokenizer_phi3.json\",\n                    \"hf_repo\": \"llmware/phi-3-ov\",\n                    \"custom_model_files\": [], \"custom_model_repo\": \"\",\n                    \"fetch\": {\"snapshot\": True, \"module\": \"llmware.models\", \"method\": \"pull_snapshot_from_hf\"},\n                    \"validation_files\": [\"openvino_model.xml\"],\n                    \"link\": \"https://huggingface.co/llmware/phi-3-ov\"},\n\"\"\"",
        "\"\"\"              Sample ONNX Model Card template \n\nmodel_card_dict =  {\"model_name\": \"phi-3-onnx\", \"model_family\": \"ONNXGenerativeModel\",\n                    \"model_category\": \"generative_local\", \"display_name\": \"phi-3-onnx\",\n                    \"model_location\": \"llmware_repo\",\n                    \"context_window\": 4096, \"instruction_following\": False, \"prompt_wrapper\": \"phi_3\",\n                    \"temperature\": 0.0, \"sample_default\": False, \"trailing_space\": \"\",\n                    \"tokenizer_local\": \"tokenizer_phi3.json\",\n                    \"hf_repo\": \"llmware/phi-3-onnx\",\n                    \"custom_model_files\": [], \"custom_model_repo\": \"\",\n                    \"fetch\": {\"snapshot\": True, \"module\": \"llmware.models\", \"method\": \"pull_snapshot_from_hf\"},\n                    \"validation_files\": [\"model.onnx\", \"model.onnx.data\"],\n                    \"link\": \"https://huggingface.co/llmware/phi-3-onnx\"},\n\"\"\"",
        "\"\"\" Issue # 1 - Models in local/custom path\n\n   If you have the model in a local/custom path, then the easiest thing to do is to copy/move manually to \n    /llmware_data/model_repo/{{my_model_name}}/ and place the model components in this path.\n\"\"\"",
        "\"\"\" Issue # 2 - How do I figure out the prompt template?\n        \n    Below is a list of the prompt wrapper lookups that covers most of the common models:\n        \n        # standard used in most llmware models - bling, dragon and slim\n        \"human_bot\": {\"main_start\": \"<human>: \", \"main_stop\": \"\\n\", \"start_llm_response\": \"<bot>:\"},\n        \n        # commonly used by llama2 and mistral\n        \"<INST>\": {\"main_start\": \"<INST>\", \"main_stop\": \"</INST>\", \"start_llm_response\": \"\"},\n        \n        \"hf_chat\": {\"system_start\": \"<|im_start|>system\\n\", \"system_stop\": \"<|im_end|>\\n\",\n                    \"main_start\": \"<|im_start|>user\", \"main_stop\": \"<|im_end|>\\n\",\n                    \"start_llm_response\": \"<|im_start|>assistant\"},\n        \n        \"open_chat\": {\"main_start\": \"GPT4 User: \", \"main_stop\": \"<|endofturn|>\",\n                      \"start_llm_response\": \"GPT4 Assistant:\"},\n        \n        \"alpaca\": {\"main_start\": \"### Instruction: \", \"main_stop\": \"\\n\",\n                   \"start_llm_response\": \"### Response: \"},\n        \n        \"chat_ml\": {\"system_start\": \"<|im_start|>system\", \"system_stop\": \"<|im_end|>\\n\",\n                    \"main_start\": \"<|im_start|>user\", \"main_stop\": \"<|im_end|>\\n\",\n                    \"start_llm_response\": \"<|im_start|>assistant\"},\n        \n        \"phi_3\": {\"system_start\": \"<|system|>\\n\", \"system_stop\": \"<|end|>\\n\",\n                  \"main_start\": \"<|user|>\\n\", \"main_stop\": \"<|end|>\\n\", \"start_llm_response\": \"<|assistant|>\"},\n        \n        \"llama_3_chat\": {\"system_start\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\",\n                         \"system_stop\": \"<|eot_id|>\",\n                         \"main_start\": \"<|start_header_id|>user>|end_header_id|>\\n\",\n                         \"main_stop\": \"<|eot_id|>\",\n                         \"start_llm_response\": \"<|start_header_id|>assistant<|end_header_id|>\\n\"},\n        \n        \"tiny_llama_chat\": {\"system_start\": \"<|system|>\", \"system_stop\": \"</s>\",\n                            \"main_start\": \"<|user|>\", \"main_stop\": \"</s>\",\n                            \"start_llm_response\": \"<|assistant|>\"},\n        \n        \"stablelm_zephyr_chat\": {\"system_start\": \"\", \"system_stop\": \"\",\n                                 \"main_start\": \"<|user|>\", \"main_stop\": \"<|endoftext|>\\n\",\n                                 \"start_llm_response\": \"<|assistant|>\"},\n        \n        \"google_gemma_chat\": {\"system_start\": \"\", \"system_stop\": \"\",\n                              \"main_start\": \"<bos><start_of_turn>user\\n\",\n                              \"main_stop\": \"<end_of_turn>\\n\",\n                              \"start_llm_response\": \"<start_of_turn>model\"},\n        \n        \"vicuna_chat\": {\"system_start\": \"\", \"system_stop\": \"\",\n                        \"main_start\": \"USER: \", \"main_stop\": \"\",\n                        \"start_llm_response\": \" ASSISTANT:\"}\n\n\"\"\""
    ],
    "functions": [],
    "classes": []
}