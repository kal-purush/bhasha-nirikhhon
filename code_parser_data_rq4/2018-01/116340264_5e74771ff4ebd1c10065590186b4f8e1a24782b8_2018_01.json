{
    "identifiers": [
        "os",
        "path",
        "osp",
        "os",
        "time",
        "joblib",
        "numpy",
        "np",
        "tensorflow",
        "tf",
        "cloudpickle",
        "gym",
        "baselines",
        "logger",
        "baselines",
        "common",
        "set_global_seeds",
        "explained_variance",
        "baselines",
        "acktr",
        "utils",
        "discount_with_dones",
        "baselines",
        "acktr",
        "utils",
        "Scheduler",
        "find_trainable_variables",
        "baselines",
        "acktr",
        "utils",
        "cat_entropy",
        "mse",
        "baselines",
        "acktr",
        "kfac",
        "policy",
        "ob_space",
        "ac_space",
        "nenvs",
        "total_timesteps",
        "nprocs",
        "nsteps",
        "nstack",
        "ent_coef",
        "vf_coef",
        "vf_fisher_coef",
        "lr",
        "max_grad_norm",
        "kfac_clip",
        "lrschedule",
        "tf",
        "ConfigProto",
        "allow_soft_placement",
        "intra_op_parallelism_threads",
        "nprocs",
        "inter_op_parallelism_threads",
        "nprocs",
        "config",
        "gpu_options",
        "tf",
        "Session",
        "config",
        "config",
        "ac_space",
        "n",
        "nenvs",
        "nsteps",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "nbatch",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "nbatch",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "nbatch",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "policy",
        "sess",
        "ob_space",
        "ac_space",
        "nenvs",
        "nstack",
        "reuse",
        "policy",
        "sess",
        "ob_space",
        "ac_space",
        "nenvs",
        "nsteps",
        "nstack",
        "reuse",
        "tf",
        "nn",
        "sparse_softmax_cross_entropy_with_logits",
        "logits",
        "train_model",
        "pi",
        "labels",
        "A",
        "train_model",
        "pi",
        "tf",
        "reduce_mean",
        "ADV",
        "logpac",
        "tf",
        "reduce_mean",
        "cat_entropy",
        "train_model",
        "pi",
        "pg_loss",
        "ent_coef",
        "entropy",
        "tf",
        "reduce_mean",
        "mse",
        "tf",
        "squeeze",
        "train_model",
        "vf",
        "R",
        "pg_loss",
        "vf_coef",
        "vf_loss",
        "tf",
        "reduce_mean",
        "logpac",
        "train_model",
        "vf",
        "tf",
        "random_normal",
        "tf",
        "shape",
        "train_model",
        "vf",
        "vf_fisher_coef",
        "tf",
        "reduce_mean",
        "tf",
        "pow",
        "train_model",
        "vf",
        "tf",
        "stop_gradient",
        "sample_net",
        "pg_fisher_loss",
        "vf_fisher_loss",
        "find_trainable_variables",
        "tf",
        "gradients",
        "train_loss",
        "tf",
        "device",
        "kfac",
        "KfacOptimizer",
        "learning_rate",
        "PG_LR",
        "clip_kl",
        "kfac_clip",
        "momentum",
        "kfac_update",
        "epsilon",
        "stats_decay",
        "async",
        "cold_iter",
        "max_grad_norm",
        "max_grad_norm",
        "optim",
        "compute_and_apply_stats",
        "joint_fisher_loss",
        "var_list",
        "optim",
        "apply_gradients",
        "grads",
        "q_runner",
        "Scheduler",
        "v",
        "lr",
        "nvalues",
        "total_timesteps",
        "schedule",
        "lrschedule",
        "obs",
        "states",
        "rewards",
        "masks",
        "actions",
        "values",
        "rewards",
        "values",
        "step",
        "len",
        "obs",
        "lr",
        "value",
        "train_model",
        "X",
        "obs",
        "A",
        "actions",
        "ADV",
        "advs",
        "R",
        "rewards",
        "PG_LR",
        "cur_lr",
        "states",
        "train_model",
        "S",
        "states",
        "train_model",
        "M",
        "masks",
        "sess",
        "run",
        "pg_loss",
        "vf_loss",
        "entropy",
        "train_op",
        "td_map",
        "policy_loss",
        "value_loss",
        "policy_entropy",
        "save_path",
        "sess",
        "run",
        "joblib",
        "dump",
        "ps",
        "save_path",
        "load_path",
        "joblib",
        "load",
        "load_path",
        "p",
        "loaded_p",
        "loaded_params",
        "restores",
        "append",
        "p",
        "assign",
        "loaded_p",
        "sess",
        "run",
        "restores",
        "train",
        "save",
        "load",
        "train_model",
        "step_model",
        "step_model",
        "step",
        "step_model",
        "value",
        "step_model",
        "initial_state",
        "tf",
        "global_variables_initializer",
        "run",
        "session",
        "sess",
        "env",
        "model",
        "nsteps",
        "nstack",
        "gamma",
        "env",
        "model",
        "env",
        "observation_space",
        "shape",
        "env",
        "num_envs",
        "nenv",
        "nsteps",
        "feature_count",
        "nstack",
        "np",
        "zeros",
        "nenv",
        "feature_count",
        "nstack",
        "dtype",
        "np",
        "float32",
        "feature_count",
        "env",
        "reset",
        "update_obs",
        "obs",
        "gamma",
        "nsteps",
        "model",
        "initial_state",
        "_",
        "nenv",
        "obs",
        "np",
        "roll",
        "obs",
        "shift",
        "feature_count",
        "axis",
        "obs",
        "ndim",
        "obs",
        "feature_count",
        "obs",
        "states",
        "n",
        "nsteps",
        "model",
        "step",
        "obs",
        "states",
        "dones",
        "mb_obs",
        "append",
        "np",
        "copy",
        "obs",
        "mb_actions",
        "append",
        "actions",
        "mb_values",
        "append",
        "values",
        "mb_dones",
        "append",
        "dones",
        "env",
        "step",
        "actions",
        "states",
        "dones",
        "n",
        "done",
        "dones",
        "done",
        "obs",
        "obs",
        "n",
        "update_obs",
        "obs",
        "mb_rewards",
        "append",
        "rewards",
        "mb_dones",
        "append",
        "dones",
        "np",
        "asarray",
        "mb_obs",
        "dtype",
        "np",
        "float32",
        "swapaxes",
        "reshape",
        "batch_ob_shape",
        "np",
        "asarray",
        "mb_rewards",
        "dtype",
        "np",
        "float32",
        "swapaxes",
        "np",
        "asarray",
        "mb_actions",
        "dtype",
        "np",
        "int32",
        "swapaxes",
        "np",
        "asarray",
        "mb_values",
        "dtype",
        "np",
        "float32",
        "swapaxes",
        "np",
        "asarray",
        "mb_dones",
        "dtype",
        "np",
        "swapaxes",
        "mb_dones",
        "mb_dones",
        "model",
        "value",
        "obs",
        "states",
        "dones",
        "tolist",
        "n",
        "rewards",
        "dones",
        "value",
        "mb_rewards",
        "mb_dones",
        "last_values",
        "rewards",
        "tolist",
        "dones",
        "tolist",
        "dones",
        "discount_with_dones",
        "rewards",
        "value",
        "dones",
        "gamma",
        "discount_with_dones",
        "rewards",
        "dones",
        "gamma",
        "rewards",
        "mb_rewards",
        "flatten",
        "mb_actions",
        "flatten",
        "mb_values",
        "flatten",
        "mb_masks",
        "flatten",
        "mb_obs",
        "mb_states",
        "mb_rewards",
        "mb_masks",
        "mb_actions",
        "mb_values",
        "save_dir",
        "model",
        "save_dir",
        "osp",
        "join",
        "save_dir",
        "osp",
        "exists",
        "checkpoint_model_path",
        "model",
        "load",
        "checkpoint_model_path",
        "policy",
        "env",
        "seed",
        "total_timesteps",
        "gamma",
        "log_interval",
        "nprocs",
        "nsteps",
        "nstack",
        "ent_coef",
        "vf_coef",
        "vf_fisher_coef",
        "lr",
        "max_grad_norm",
        "kfac_clip",
        "save_interval",
        "lrschedule",
        "animate_interval",
        "env_id",
        "tf",
        "reset_default_graph",
        "set_global_seeds",
        "seed",
        "env",
        "num_envs",
        "env",
        "observation_space",
        "env",
        "action_space",
        "Model",
        "policy",
        "ob_space",
        "ac_space",
        "nenvs",
        "total_timesteps",
        "nprocs",
        "nprocs",
        "nsteps",
        "nsteps",
        "nstack",
        "nstack",
        "ent_coef",
        "ent_coef",
        "vf_coef",
        "vf_coef",
        "vf_fisher_coef",
        "vf_fisher_coef",
        "lr",
        "lr",
        "max_grad_norm",
        "max_grad_norm",
        "kfac_clip",
        "kfac_clip",
        "lrschedule",
        "lrschedule",
        "save_interval",
        "logger",
        "get_dir",
        "open",
        "osp",
        "join",
        "logger",
        "get_dir",
        "fh",
        "fh",
        "write",
        "cloudpickle",
        "dumps",
        "make_model",
        "make_model",
        "logger",
        "get_dir",
        "osp",
        "abspath",
        "osp",
        "join",
        "logger",
        "get_dir",
        "os",
        "pardir",
        "maybe_load_model",
        "logger_parent_dir",
        "model",
        "Runner",
        "env",
        "model",
        "nsteps",
        "nsteps",
        "nstack",
        "nstack",
        "gamma",
        "gamma",
        "nenvs",
        "nsteps",
        "time",
        "time",
        "tf",
        "train",
        "Coordinator",
        "model",
        "q_runner",
        "create_threads",
        "model",
        "sess",
        "coord",
        "coord",
        "start",
        "update",
        "total_timesteps",
        "nbatch",
        "runner",
        "run",
        "model",
        "train",
        "obs",
        "states",
        "rewards",
        "masks",
        "actions",
        "values",
        "obs",
        "time",
        "time",
        "tstart",
        "update",
        "nbatch",
        "nseconds",
        "update",
        "log_interval",
        "update",
        "explained_variance",
        "values",
        "rewards",
        "logger",
        "record_tabular",
        "update",
        "logger",
        "record_tabular",
        "update",
        "nbatch",
        "logger",
        "record_tabular",
        "fps",
        "logger",
        "record_tabular",
        "policy_entropy",
        "logger",
        "record_tabular",
        "policy_loss",
        "logger",
        "record_tabular",
        "value_loss",
        "logger",
        "record_tabular",
        "ev",
        "logger",
        "dump_tabular",
        "save_interval",
        "update",
        "save_interval",
        "update",
        "logger",
        "get_dir",
        "osp",
        "join",
        "logger",
        "get_dir",
        "update",
        "savepath",
        "model",
        "save",
        "savepath",
        "animate_interval",
        "update",
        "animate_interval",
        "update",
        "env_id",
        "gym",
        "make",
        "env_id",
        "test_env",
        "seed",
        "update",
        "ob_space",
        "shape",
        "np",
        "zeros",
        "nenvs",
        "feature_count",
        "nstack",
        "dtype",
        "np",
        "float32",
        "test_env",
        "reset",
        "done",
        "test_env",
        "render",
        "np",
        "roll",
        "obs_history",
        "shift",
        "feature_count",
        "axis",
        "obs_history",
        "ndim",
        "obs_history",
        "feature_count",
        "obs",
        "model",
        "step",
        "obs_history",
        "test_env",
        "step",
        "action",
        "episode_rew",
        "rew",
        "episode_rew",
        "test_env",
        "close",
        "logger",
        "get_dir",
        "osp",
        "join",
        "logger",
        "get_dir",
        "savepath",
        "model",
        "save",
        "savepath",
        "coord",
        "request_stop",
        "coord",
        "join",
        "enqueue_threads",
        "env",
        "close"
    ],
    "literals": [
        "'linear'",
        "\"model\"",
        "'/gpu:0'",
        "'checkpoint_model'",
        "'no previous model to load'",
        "'loading previous model to start from'",
        "'linear'",
        "'make_model.pkl'",
        "'wb'",
        "\"nupdates\"",
        "\"total_timesteps\"",
        "\"fps\"",
        "\"policy_entropy\"",
        "\"policy_loss\"",
        "\"value_loss\"",
        "\"explained_variance\"",
        "'checkpoint%.5i'",
        "'Saving to'",
        "\"Episode reward\"",
        "'final_model'",
        "'Saving to'"
    ],
    "variables": [
        "config",
        "allow_growth",
        "sess",
        "sess",
        "nact",
        "nbatch",
        "A",
        "ADV",
        "R",
        "PG_LR",
        "VF_LR",
        "model",
        "step_model",
        "model2",
        "train_model",
        "logpac",
        "logits",
        "logits",
        "pg_loss",
        "entropy",
        "pg_loss",
        "vf_loss",
        "train_loss",
        "pg_fisher",
        "pg_fisher_loss",
        "sample_net",
        "vf_fisher",
        "vf_fisher_loss",
        "joint_fisher",
        "joint_fisher_loss",
        "grads_check",
        "grads",
        "optim",
        "optim",
        "update_stats_op",
        "train_op",
        "q_runner",
        "q_runner",
        "lr",
        "advs",
        "cur_lr",
        "td_map",
        "td_map",
        "td_map",
        "policy_loss",
        "value_loss",
        "policy_entropy",
        "_",
        "ps",
        "loaded_params",
        "restores",
        "train",
        "save",
        "load",
        "train_model",
        "step_model",
        "step",
        "value",
        "initial_state",
        "env",
        "model",
        "feature_count",
        "nenv",
        "batch_ob_shape",
        "obs",
        "feature_count",
        "obs",
        "gamma",
        "nsteps",
        "states",
        "dones",
        "obs",
        "mb_obs",
        "mb_rewards",
        "mb_actions",
        "mb_values",
        "mb_dones",
        "mb_states",
        "actions",
        "values",
        "states",
        "obs",
        "rewards",
        "dones",
        "_",
        "states",
        "dones",
        "n",
        "mb_obs",
        "mb_rewards",
        "mb_actions",
        "mb_values",
        "mb_dones",
        "mb_masks",
        "mb_dones",
        "last_values",
        "rewards",
        "dones",
        "rewards",
        "rewards",
        "mb_rewards",
        "n",
        "mb_rewards",
        "mb_actions",
        "mb_values",
        "mb_masks",
        "checkpoint_model_path",
        "nenvs",
        "ob_space",
        "ac_space",
        "make_model",
        "model",
        "logger_parent_dir",
        "runner",
        "nbatch",
        "tstart",
        "coord",
        "enqueue_threads",
        "obs",
        "states",
        "rewards",
        "masks",
        "actions",
        "values",
        "policy_loss",
        "value_loss",
        "policy_entropy",
        "model",
        "old_obs",
        "nseconds",
        "fps",
        "ev",
        "savepath",
        "test_env",
        "feature_count",
        "obs_history",
        "obs",
        "done",
        "episode_rew",
        "obs_history",
        "action",
        "v",
        "_",
        "obs",
        "rew",
        "done",
        "info",
        "savepath"
    ],
    "comments": [
        "training loss",
        "Fisher loss construction",
        "batch of steps to batch of rollouts",
        "discount/bootstrap off value fn",
        "try to load the model from a previous save",
        "This requires the operator to copy a model to the parent",
        "directory of the logging dir (typically /tmp) as \"checkpoint_model\"",
        "are we supposed to save?",
        "animate every so often if requested",
        "setup history for same number of environments, for the given feature count and history stack size",
        "this is because we need to feed the same shape tensors into the step model, but we'll only use the first env",
        "add the current observation onto our history list",
        "get the suggested action for the current observation history",
        "the model returns an action for each env, but we're only using the first one to animate",
        "always save the model when we stop training, if we have a place to save to"
    ],
    "docstrings": [],
    "functions": [
        "train",
        "save",
        "load",
        "update_obs",
        "run",
        "maybe_load_model",
        "learn"
    ],
    "classes": [
        "Model",
        "Runner"
    ]
}