{
    "identifiers": [
        "print_function",
        "argparse",
        "gzip",
        "json",
        "os",
        "pickle",
        "numpy",
        "np",
        "tensorflow",
        "tf",
        "matplotlib",
        "matplotlib",
        "pyplot",
        "plt",
        "matplotlib",
        "use",
        "plt",
        "switch_backend",
        "labels",
        "np",
        "unique",
        "labels",
        "classes",
        "size",
        "np",
        "zeros",
        "labels",
        "shape",
        "n_classes",
        "c",
        "classes",
        "labels",
        "c",
        "one_hot_labels",
        "datasets_dir",
        "os",
        "path",
        "exists",
        "datasets_dir",
        "os",
        "mkdir",
        "datasets_dir",
        "os",
        "path",
        "join",
        "datasets_dir",
        "os",
        "path",
        "exists",
        "data_file",
        "urllib",
        "urllib",
        "urlretrieve",
        "AttributeError",
        "urllib",
        "request",
        "urllib",
        "urllib",
        "urlretrieve",
        "url",
        "data_file",
        "gzip",
        "open",
        "data_file",
        "pickle",
        "load",
        "f",
        "encoding",
        "pickle",
        "load",
        "f",
        "f",
        "close",
        "test_set",
        "test_x",
        "astype",
        "test_x",
        "astype",
        "reshape",
        "test_x",
        "shape",
        "test_y",
        "astype",
        "valid_set",
        "valid_x",
        "astype",
        "valid_x",
        "astype",
        "reshape",
        "valid_x",
        "shape",
        "valid_y",
        "astype",
        "train_set",
        "train_x",
        "astype",
        "reshape",
        "train_x",
        "shape",
        "train_y",
        "astype",
        "train_x",
        "one_hot",
        "train_y",
        "valid_x",
        "one_hot",
        "valid_y",
        "test_x",
        "one_hot",
        "test_y",
        "data",
        "nf",
        "fs",
        "tf",
        "reshape",
        "data",
        "tf",
        "layers",
        "conv2d",
        "inputs",
        "input_layer",
        "filters",
        "nf",
        "kernel_size",
        "fs",
        "fs",
        "padding",
        "activation",
        "tf",
        "nn",
        "relu",
        "tf",
        "layers",
        "max_pooling2d",
        "inputs",
        "conv1",
        "pool_size",
        "strides",
        "tf",
        "layers",
        "conv2d",
        "inputs",
        "pool1",
        "filters",
        "nf",
        "kernel_size",
        "fs",
        "fs",
        "padding",
        "activation",
        "tf",
        "nn",
        "relu",
        "tf",
        "layers",
        "max_pooling2d",
        "inputs",
        "conv2",
        "pool_size",
        "strides",
        "tf",
        "reshape",
        "pool2",
        "nf",
        "tf",
        "layers",
        "dense",
        "inputs",
        "pool2_flat",
        "units",
        "activation",
        "tf",
        "nn",
        "relu",
        "tf",
        "layers",
        "dense",
        "inputs",
        "dense",
        "units",
        "output",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "num_epochs",
        "lr",
        "num_filters",
        "batch_size",
        "filter_size",
        "tf",
        "placeholder",
        "tf",
        "placeholder",
        "neural_network_model",
        "x",
        "nf",
        "num_filters",
        "fs",
        "filter_size",
        "tf",
        "reduce_mean",
        "tf",
        "nn",
        "softmax_cross_entropy_with_logits",
        "logits",
        "prediction",
        "labels",
        "y",
        "tf",
        "train",
        "GradientDescentOptimizer",
        "lr",
        "minimize",
        "cost",
        "tf",
        "equal",
        "tf",
        "argmax",
        "prediction",
        "tf",
        "argmax",
        "y",
        "tf",
        "reduce_mean",
        "tf",
        "cast",
        "correct",
        "num_epochs",
        "np",
        "zeros",
        "num_epochs",
        "tf",
        "Session",
        "sess",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "epoch",
        "num_epochs",
        "_",
        "len",
        "x_train",
        "batch_size",
        "np",
        "random",
        "choice",
        "len",
        "x_train",
        "batch_size",
        "x_train",
        "rand_index",
        "reshape",
        "y_train",
        "rand_index",
        "sess",
        "run",
        "optimizer",
        "feed_dict",
        "x",
        "batch_x",
        "y",
        "batch_y",
        "sess",
        "run",
        "accuracy",
        "cost",
        "feed_dict",
        "x",
        "batch_x",
        "y",
        "batch_y",
        "epoch",
        "num_epochs",
        "format",
        "loss",
        "format",
        "acc",
        "val_step",
        "len",
        "x_valid",
        "batch_size",
        "x_valid",
        "val_step",
        "batch_size",
        "val_step",
        "batch_size",
        "reshape",
        "y_valid",
        "val_step",
        "batch_size",
        "val_step",
        "batch_size",
        "sess",
        "run",
        "accuracy",
        "cost",
        "feed_dict",
        "x",
        "val_batch_x",
        "y",
        "val_batch_y",
        "val_acc",
        "step_acc",
        "val_loss",
        "step_loss",
        "val_acc",
        "val_step",
        "val_loss",
        "val_step",
        "val_acc",
        "val_loss",
        "loss",
        "sess",
        "run",
        "accuracy",
        "feed_dict",
        "x",
        "x_test",
        "reshape",
        "y",
        "y_test",
        "test_acc",
        "test_acc",
        "learning_curve",
        "learning_curve",
        "test_error",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "num_filters",
        "batch_size",
        "filter_size",
        "np",
        "arange",
        "epochs",
        "lr",
        "train_validate_and_test",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "lr",
        "num_filters",
        "batch_size",
        "filter_size",
        "plt",
        "plot",
        "x_axis",
        "learning_curve",
        "plt",
        "legend",
        "loc",
        "plt",
        "savefig",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "lr",
        "num_filters",
        "batch_size",
        "np",
        "arange",
        "epochs",
        "filter_size",
        "train_validate_and_test",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "lr",
        "num_filters",
        "batch_size",
        "filter_size",
        "plt",
        "plot",
        "x_axis",
        "learning_curve",
        "plt",
        "legend",
        "loc",
        "plt",
        "savefig",
        "argparse",
        "ArgumentParser",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "add_argument",
        "nargs",
        "help",
        "parser",
        "parse_args",
        "args",
        "learning_rate",
        "args",
        "num_filters",
        "args",
        "batch_size",
        "args",
        "epochs",
        "args",
        "filter_size",
        "mnist",
        "args",
        "input_path",
        "train_validate_and_test",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "lr",
        "num_filters",
        "batch_size",
        "filter_size",
        "different_learning_rates",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "num_filters",
        "batch_size",
        "filter_size",
        "different_filter_sizes",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "epochs",
        "lr",
        "num_filters",
        "batch_size",
        "lr",
        "num_filters",
        "batch_size",
        "filter_size",
        "learning_curve",
        "test_error",
        "os",
        "path",
        "join",
        "args",
        "output_path",
        "os",
        "makedirs",
        "path",
        "exist_ok",
        "os",
        "path",
        "join",
        "path",
        "args",
        "run_id",
        "open",
        "fname",
        "json",
        "dump",
        "results",
        "fh",
        "fh",
        "close"
    ],
    "literals": [
        "'Agg'",
        "'agg'",
        "'./data'",
        "'mnist.pkl.gz'",
        "'... downloading MNIST from the web'",
        "'http://google.com'",
        "'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'",
        "'... loading data'",
        "'rb'",
        "\"latin1\"",
        "'float32'",
        "'float32'",
        "'int32'",
        "'float32'",
        "'float32'",
        "'int32'",
        "'float32'",
        "'int32'",
        "'... done loading data'",
        "\"same\"",
        "\"same\"",
        "'float'",
        "'float'",
        "'float'",
        "\"Epoch \"",
        "\" / \"",
        "\": Epoch Training Loss= \"",
        "\"{:.6f}\"",
        "\", Training Accuracy= \"",
        "\"{:.5f}\"",
        "\"Validation set accuracy: %s\"",
        "\"Training Finished!\"",
        "\"Testing Accuracy:\"",
        "'Val Curve'",
        "'0.1'",
        "'0.01'",
        "'0.001'",
        "'0.0001'",
        "'upper right'",
        "'different_learning_rates.png'",
        "'1x1'",
        "'3x3'",
        "'5x5'",
        "'7x7'",
        "'upper right'",
        "'different_filter_sizes.png'",
        "\"__main__\"",
        "\"--output_path\"",
        "\"./\"",
        "\"?\"",
        "\"Path where the results will be stored\"",
        "\"--input_path\"",
        "\"./\"",
        "\"?\"",
        "\"Path where the data is located. If the data is not available it will be downloaded first\"",
        "\"--learning_rate\"",
        "\"?\"",
        "\"Learning rate for SGD\"",
        "\"--num_filters\"",
        "\"?\"",
        "\"The number of filters for each convolution layer\"",
        "\"--batch_size\"",
        "\"?\"",
        "\"Batch size for SGD\"",
        "\"--epochs\"",
        "\"?\"",
        "\"Determines how many epochs the network will be trained\"",
        "\"--run_id\"",
        "\"?\"",
        "\"Helps to identify different runs of an experiments\"",
        "\"--filter_size\"",
        "\"?\"",
        "\"Filter width and height\"",
        "\"lr\"",
        "\"num_filters\"",
        "\"batch_size\"",
        "\"filter_size\"",
        "\"learning_curve\"",
        "\"test_error\"",
        "\"results\"",
        "\"results_run_%d.json\"",
        "\"w\""
    ],
    "variables": [
        "classes",
        "n_classes",
        "one_hot_labels",
        "one_hot_labels",
        "c",
        "data_file",
        "url",
        "f",
        "train_set",
        "valid_set",
        "test_set",
        "train_set",
        "valid_set",
        "test_set",
        "test_x",
        "test_y",
        "test_x",
        "test_x",
        "test_y",
        "valid_x",
        "valid_y",
        "valid_x",
        "valid_x",
        "valid_y",
        "train_x",
        "train_y",
        "train_x",
        "train_y",
        "input_layer",
        "conv1",
        "pool1",
        "conv2",
        "pool2",
        "pool2_flat",
        "dense",
        "output",
        "x",
        "y",
        "prediction",
        "cost",
        "optimizer",
        "correct",
        "accuracy",
        "learning_curve",
        "train_loss_history",
        "rand_index",
        "batch_x",
        "batch_y",
        "acc",
        "loss",
        "val_acc",
        "val_loss",
        "val_batch_x",
        "val_batch_y",
        "step_acc",
        "step_loss",
        "val_acc",
        "val_loss",
        "learning_curve",
        "epoch",
        "train_loss_history",
        "epoch",
        "test_acc",
        "test_error",
        "x_axis",
        "learning_curve",
        "test_error",
        "x_axis",
        "learning_curve",
        "test_error",
        "parser",
        "args",
        "lr",
        "num_filters",
        "batch_size",
        "epochs",
        "filter_size",
        "x_train",
        "y_train",
        "x_valid",
        "y_valid",
        "x_test",
        "y_test",
        "learning_curve",
        "test_error",
        "results",
        "results",
        "results",
        "results",
        "results",
        "results",
        "results",
        "path",
        "fname",
        "fh"
    ],
    "comments": [
        "Force matplotlib to not use any Xwindows backend.",
        "Load the dataset",
        "Input Layer",
        "Convolutional Layer #1",
        "Pooling Layer #1",
        "Convolutional Layer #2 and Pooling Layer #2",
        "Dense Layer",
        "Logits Layer",
        "Calculate training accuracy and loss per epoch",
        "Calculate validation accuracy and loss per epoch",
        "Implementing Part2 of the exercise",
        "Implementing Part3 of the exercise",
        "hyperparameters",
        "train and test convolutional neural network",
        "save results in a dictionary and write them into a .json file"
    ],
    "docstrings": [
        "\"\"\"this creates a one hot encoding from a flat vector:\n    i.e. given y = [0,2,1]\n     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n    \"\"\"",
        "\"\"\"Model function for CNN.\"\"\""
    ],
    "functions": [
        "one_hot",
        "mnist",
        "neural_network_model",
        "train_validate_and_test",
        "different_learning_rates",
        "different_filter_sizes"
    ],
    "classes": []
}