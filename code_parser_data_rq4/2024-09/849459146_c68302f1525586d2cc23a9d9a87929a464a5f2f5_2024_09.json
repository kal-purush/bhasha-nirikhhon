{
    "identifiers": [
        "typing",
        "Dict",
        "Optional",
        "Tuple",
        "gymnasium",
        "gym",
        "numpy",
        "np",
        "numpy",
        "typing",
        "npt",
        "gym",
        "Env",
        "num_targets",
        "max_area",
        "shuffle_time",
        "seed",
        "np",
        "random",
        "seed",
        "seed",
        "shuffle_time",
        "num_targets",
        "num_targets",
        "max_area",
        "npt",
        "NDArray",
        "np",
        "float32",
        "_generate_points",
        "num_targets",
        "npt",
        "NDArray",
        "np",
        "float32",
        "_calculate_distances",
        "locations",
        "npt",
        "NDArray",
        "np",
        "float32",
        "np",
        "arange",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "npt",
        "NDArray",
        "np",
        "float32",
        "initial_profits",
        "copy",
        "np",
        "concatenate",
        "np",
        "array",
        "dtype",
        "np",
        "float32",
        "np",
        "zeros",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "np",
        "zeros",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "np",
        "zeros",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "np",
        "concatenate",
        "np",
        "array",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "np",
        "ones",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "max_area",
        "np",
        "ones",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "max_area",
        "np",
        "ones",
        "num_targets",
        "dtype",
        "np",
        "float32",
        "gym",
        "spaces",
        "Box",
        "low",
        "obs_low",
        "high",
        "obs_high",
        "gym",
        "spaces",
        "Discrete",
        "num_targets",
        "seed",
        "Optional",
        "options",
        "Optional",
        "Tuple",
        "np",
        "ndarray",
        "Dict",
        "episodes",
        "distances",
        "loc",
        "shuffle_time",
        "episodes",
        "np",
        "random",
        "shuffle",
        "initial_profits",
        "np",
        "concatenate",
        "np",
        "array",
        "loc",
        "np",
        "array",
        "dist",
        "np",
        "array",
        "locations",
        "reshape",
        "dtype",
        "np",
        "float32",
        "state",
        "action",
        "Tuple",
        "np",
        "ndarray",
        "Dict",
        "steps",
        "loc",
        "action",
        "current_profits",
        "distances",
        "past_loc",
        "next_loc",
        "_get_rewards",
        "next_loc",
        "visited_targets",
        "append",
        "next_loc",
        "distances",
        "next_loc",
        "steps",
        "max_steps",
        "np",
        "concatenate",
        "np",
        "array",
        "next_loc",
        "next_dist",
        "np",
        "array",
        "locations",
        "reshape",
        "dtype",
        "np",
        "float32",
        "loc",
        "dist",
        "next_loc",
        "next_dist",
        "next_state",
        "reward",
        "terminated",
        "truncated",
        "num_points",
        "npt",
        "NDArray",
        "np",
        "float32",
        "np",
        "random",
        "uniform",
        "low",
        "high",
        "max_area",
        "size",
        "num_points",
        "astype",
        "np",
        "float32",
        "locations",
        "npt",
        "NDArray",
        "np",
        "float32",
        "npt",
        "NDArray",
        "np",
        "float32",
        "len",
        "locations",
        "np",
        "zeros",
        "n",
        "n",
        "dtype",
        "np",
        "float32",
        "i",
        "n",
        "j",
        "n",
        "np",
        "linalg",
        "norm",
        "locations",
        "i",
        "locations",
        "j",
        "distances",
        "next_loc",
        "current_profits",
        "next_loc",
        "next_loc",
        "visited_targets",
        "reward",
        "ModTSP",
        "num_targets",
        "env",
        "reset",
        "ep",
        "env",
        "reset",
        "_",
        "env",
        "action_space",
        "sample",
        "env",
        "step",
        "action",
        "terminated",
        "truncated",
        "ret",
        "reward",
        "done",
        "ep_rets",
        "append",
        "ret",
        "ep",
        "ret",
        "np",
        "mean",
        "ep_rets",
        "main"
    ],
    "literals": [
        "f\"Episode {ep} : {ret}\"",
        "\"__main__\""
    ],
    "variables": [
        "steps",
        "episodes",
        "shuffle_time",
        "num_targets",
        "max_steps",
        "max_area",
        "locations",
        "distances",
        "initial_profits",
        "current_profits",
        "obs_low",
        "obs_high",
        "observation_space",
        "action_space",
        "steps",
        "loc",
        "visited_targets",
        "dist",
        "state",
        "past_loc",
        "next_loc",
        "reward",
        "next_dist",
        "terminated",
        "truncated",
        "next_state",
        "n",
        "distances",
        "distances",
        "i",
        "j",
        "reward",
        "num_targets",
        "env",
        "obs",
        "ep_rets",
        "ret",
        "obs",
        "action",
        "obs_",
        "reward",
        "terminated",
        "truncated",
        "info",
        "done"
    ],
    "comments": [
        "Initialize profits for each target",
        "Observation Space : {current loc (loc), current profits, dist_array (distances), coordintates (locations)}",
        "Current location",
        "Array of all current profits values",
        "Distance to each target from current location",
        "Cooridinates of all targets",
        "Current location",
        "Array of all current profits values",
        "Distance to each target from current location",
        "Cooridinates of all targets",
        "Action Space : {next_target}",
        "You need to replace this with your algorithm that predicts the action."
    ],
    "docstrings": [
        "\"\"\"Environment for Travelling Salesman Problem.\"\"\"",
        "\"\"\"Travelling Salesman Problem (TSP) RL environment for persistent monitoring.\n\n    The agent navigates a set of targets based on precomputed distances. It aims to visit\n    all targets in the least number of steps, with rewards determined by the distance traveled.\n    \"\"\"",
        "\"\"\"Initialize the TSP environment.\n\n        Args:\n            num_targets (int): No. of targets the agent needs to visit.\n            max_area (int): Max. Square area where the targets are defined.\n            shuffle_time (int): No. of episodes after which the profits ar to be shuffled.\n            seed (int): Random seed for reproducibility.\n        \"\"\"",
        "\"\"\"Reset the environment to the initial state.\n\n        Args:\n            seed (Optional[int], optional): Seed to reset the environment. Defaults to None.\n            options (Optional[dict], optional): Additional reset options. Defaults to None.\n\n        Returns:\n            Tuple[np.ndarray, Dict[str, None]]: The initial state of the environment and an empty info dictionary.\n        \"\"\"",
        "\"\"\"Take an action (move to the next target).\n\n        Args:\n            action (int): The index of the next target to move to.\n\n        Returns:\n            Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:\n                - The new state of the environment.\n                - The reward for the action.\n                - A boolean indicating whether the episode has terminated.\n                - A boolean indicating if the episode is truncated.\n                - An empty info dictionary.\n        \"\"\"",
        "\"\"\"Generate random 2D points representing target locations.\n\n        Args:\n            num_points (int): Number of points to generate.\n\n        Returns:\n            np.ndarray: Array of 2D coordinates for each target.\n        \"\"\"",
        "\"\"\"Calculate the distance matrix between all target locations.\n\n        Args:\n            locations: List of 2D target locations.\n\n        Returns:\n            np.ndarray: Matrix of pairwise distances between targets.\n        \"\"\"",
        "\"\"\"Calculate the reward based on the distance traveled, however if a target gets visited again then it incurs a high penalty.\n\n        Args:\n            next_loc (int): Next location of the agent.\n\n        Returns:\n            float: Reward based on the travel distance between past and next locations, or negative reward if repeats visit.\n        \"\"\"",
        "\"\"\"Main function.\"\"\""
    ],
    "functions": [
        "reset",
        "step",
        "_generate_points",
        "_calculate_distances",
        "_get_rewards",
        "main"
    ],
    "classes": [
        "ModTSP"
    ]
}