{
    "identifiers": [
        "numpy",
        "np",
        "pickle",
        "torch",
        "torch",
        "utils",
        "data",
        "Dataset",
        "Dataset",
        "x_array",
        "y_labels",
        "saved_encoder",
        "cls_token_id",
        "max_size",
        "open",
        "x_array",
        "handle",
        "pickle",
        "load",
        "handle",
        "open",
        "y_labels",
        "handle",
        "pickle",
        "load",
        "handle",
        "open",
        "saved_encoder",
        "handle",
        "pickle",
        "load",
        "handle",
        "cls_token_id",
        "max_size",
        "x_array",
        "max_size",
        "y_labels",
        "max_size",
        "len",
        "x_array",
        "idx",
        "torch",
        "is_tensor",
        "idx",
        "idx",
        "tolist",
        "x_array",
        "idx",
        "tolist",
        "saved_encoder",
        "encode",
        "saved_encoder",
        "encode",
        "l",
        "insert",
        "cls_token_id",
        "l",
        "index",
        "start_token",
        "l",
        "index",
        "end_token",
        "np",
        "zeros",
        "len",
        "l",
        "dtype",
        "index",
        "len",
        "l",
        "l",
        "index",
        "np",
        "zeros",
        "len",
        "l",
        "dtype",
        "np",
        "zeros",
        "len",
        "l",
        "dtype",
        "index",
        "start_index",
        "count",
        "token_type",
        "count",
        "index",
        "start_index",
        "end_index",
        "count",
        "token_type",
        "count",
        "index",
        "end_index",
        "len",
        "position",
        "count",
        "token_type",
        "count",
        "torch",
        "tensor",
        "l",
        "torch",
        "tensor",
        "position",
        "torch",
        "tensor",
        "token_type_list",
        "l",
        "position",
        "token_type_list",
        "mask",
        "y_labels",
        "idx"
    ],
    "literals": [
        "'rb'",
        "'rb'",
        "'rb'",
        "'<START>'",
        "'<END>'"
    ],
    "variables": [
        "x_array",
        "y_labels",
        "saved_encoder",
        "cls_token_id",
        "x_array",
        "y_labels",
        "idx",
        "l",
        "start_token",
        "end_token",
        "start_index",
        "end_index",
        "mask",
        "mask",
        "index",
        "position",
        "token_type_list",
        "count",
        "token_type",
        "position",
        "index",
        "token_type_list",
        "index",
        "count",
        "token_type",
        "position",
        "index",
        "token_type_list",
        "index",
        "count",
        "token_type",
        "position",
        "index",
        "token_type_list",
        "index",
        "l",
        "position",
        "token_type_list"
    ],
    "comments": [
        "self.x_array[idx] is the idx-th example in dataset",
        "cls_token_id = 218799",
        "Prepending <CLS> token_id to each datapoint",
        "Attention mask calculation -- CLS token also gets 1",
        "Position id calculation -- CLS till <START> = 0 to m; <START>+1 till <END> = 0 to n; <END>+1 till 1000 = 0 to p",
        "Token type calculation -- CLS-<START> = 0; <START>-<END> = 1; <END>-1000 = 2"
    ],
    "docstrings": [
        "\"\"\"\n\t\tArgs: ---------CHANGE PATHS PLEASE!!!!---------\n\t\tarray (string): Path to the concatenated tokenized array (array of token-ids)\n\t\ty_labels (string): Path to the y_labels\n\t\tsaved_encoder (string): Path to the saved encoder (saved by preprocess.py). We need this so that strings can be converted to tokens & vice versa\n\t\t\"\"\""
    ],
    "functions": [
        "__len__",
        "__getitem__"
    ],
    "classes": [
        "LineDefect"
    ]
}