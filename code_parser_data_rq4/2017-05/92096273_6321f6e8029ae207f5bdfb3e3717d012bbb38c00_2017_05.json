{
    "identifiers": [
        "shape",
        "tf",
        "truncated_normal",
        "shape",
        "stddev",
        "tf",
        "Variable",
        "initial",
        "shape",
        "tf",
        "constant",
        "shape",
        "shape",
        "tf",
        "Variable",
        "initial",
        "x",
        "W",
        "tf",
        "nn",
        "conv2d",
        "x",
        "W",
        "strides",
        "padding",
        "x",
        "tf",
        "nn",
        "max_pool",
        "x",
        "ksize",
        "strides",
        "padding",
        "tensorflow",
        "examples",
        "tutorials",
        "mnist",
        "input_data",
        "input_data",
        "read_data_sets",
        "one_hot",
        "tensorflow",
        "tf",
        "tf",
        "InteractiveSession",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "shape",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "shape",
        "weight_variable",
        "bias_variable",
        "tf",
        "reshape",
        "x",
        "tf",
        "nn",
        "relu",
        "conv2d",
        "x_image",
        "W_conv1",
        "b_conv1",
        "max_pool_2x2",
        "h_conv1",
        "weight_variable",
        "bias_variable",
        "tf",
        "nn",
        "relu",
        "conv2d",
        "h_pool1",
        "W_conv2",
        "b_conv2",
        "max_pool_2x2",
        "h_conv2",
        "weight_variable",
        "bias_variable",
        "tf",
        "reshape",
        "h_pool2",
        "tf",
        "nn",
        "relu",
        "tf",
        "matmul",
        "h_pool2_flat",
        "W_fc1",
        "b_fc1",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "tf",
        "nn",
        "dropout",
        "h_fc1",
        "keep_prob",
        "weight_variable",
        "bias_variable",
        "tf",
        "matmul",
        "h_fc1_drop",
        "W_fc2",
        "b_fc2",
        "tf",
        "reduce_mean",
        "tf",
        "nn",
        "softmax_cross_entropy_with_logits",
        "labels",
        "y_",
        "logits",
        "y_conv",
        "tf",
        "train",
        "AdamOptimizer",
        "minimize",
        "cross_entropy",
        "tf",
        "equal",
        "tf",
        "argmax",
        "y_conv",
        "tf",
        "argmax",
        "y_",
        "tf",
        "reduce_mean",
        "tf",
        "cast",
        "correct_prediction",
        "tf",
        "float32",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "i",
        "mnist",
        "train",
        "next_batch",
        "i",
        "accuracy",
        "eval",
        "feed_dict",
        "x",
        "batch",
        "y_",
        "batch",
        "keep_prob",
        "i",
        "train_accuracy",
        "train_step",
        "run",
        "feed_dict",
        "x",
        "batch",
        "y_",
        "batch",
        "keep_prob",
        "accuracy",
        "eval",
        "feed_dict",
        "x",
        "mnist",
        "test",
        "images",
        "y_",
        "mnist",
        "test",
        "labels",
        "keep_prob"
    ],
    "literals": [
        "'SAME'",
        "'SAME'",
        "'MNIST_data'",
        "\"step %d, training accuracy %g\"",
        "\"test accuracy %g\""
    ],
    "variables": [
        "initial",
        "initial",
        "mnist",
        "sess",
        "x",
        "y_",
        "W_conv1",
        "b_conv1",
        "x_image",
        "h_conv1",
        "h_pool1",
        "W_conv2",
        "b_conv2",
        "h_conv2",
        "h_pool2",
        "W_fc1",
        "b_fc1",
        "h_pool2_flat",
        "h_fc1",
        "keep_prob",
        "h_fc1_drop",
        "W_fc2",
        "b_fc2",
        "y_conv",
        "cross_entropy",
        "train_step",
        "correct_prediction",
        "accuracy",
        "batch",
        "train_accuracy"
    ],
    "comments": [
        "tf-mnist-nlcnn.py #####",
        "This file implements a convolutional neural network",
        "using Tensor Flow to solve the MNIST handwriting",
        "recognition problem.",
        "",
        "Seth Haney, 5/22/2017",
        "",
        "The following are useful shortcuts for",
        "standard methods in TF",
        "Initialize weights as normal",
        "Initialize Bias as constant",
        "Do 2D convulution with unit strides and padding to equalize input and output size",
        "Define grid of max pooling as 2x2 with padding to equailize input and output size",
        "",
        "IMPORT THE DATA ###",
        "IMPORT TF and start session",
        "Input and output placeholders - NOTE y_ is true output y is predicted",
        "DEFINE Weight and bias variables and sizes",
        "Weights are for a 5x5 rec. field, 1 color, and 32 features",
        "a bias is given for each feature.",
        "Note image sizes are 28 x 28 pixels",
        "Second Convolutional Layer - NOTE: The input size is now 32 - NFeatures from Conv. Layer 1.",
        "Densely Connected Layer for classification. NOTE: Input size is smaller due to max pooling.",
        "Dropout. This cuts neurons at the densely connected layer based on prob. stored here",
        "Classifying layer",
        "define loss function - cross_entropy",
        "define trianing step with Adam Optimizer",
        "determine accuracy",
        "Initialize variables in backend",
        "Train"
    ],
    "docstrings": [],
    "functions": [
        "weight_variable",
        "bias_variable",
        "conv2d",
        "max_pool_2x2"
    ],
    "classes": []
}