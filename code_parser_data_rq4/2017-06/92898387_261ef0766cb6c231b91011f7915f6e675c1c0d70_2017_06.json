{
    "identifiers": [
        "tensorflow",
        "tf",
        "pandas",
        "pd",
        "numpy",
        "np",
        "os",
        "json",
        "keras",
        "preprocessing",
        "sequence",
        "dim_image",
        "n_words",
        "dim_hidden",
        "batch_size",
        "n_lstm_steps",
        "bias_init_vector",
        "dim_image",
        "n_words",
        "dim_hidden",
        "batch_size",
        "n_lstm_steps",
        "tf",
        "contrib",
        "rnn",
        "tf",
        "Variable",
        "tf",
        "random_uniform",
        "n_words",
        "dim_hidden",
        "name",
        "rnn_cell",
        "BasicLSTMCell",
        "dim_hidden",
        "state_is_tuple",
        "rnn_cell",
        "BasicLSTMCell",
        "dim_hidden",
        "state_is_tuple",
        "tf",
        "Variable",
        "tf",
        "random_uniform",
        "dim_image",
        "dim_hidden",
        "name",
        "tf",
        "Variable",
        "tf",
        "zeros",
        "dim_hidden",
        "name",
        "tf",
        "Variable",
        "tf",
        "random_uniform",
        "dim_hidden",
        "n_words",
        "name",
        "bias_init_vector",
        "tf",
        "Variable",
        "bias_init_vector",
        "astype",
        "np",
        "float32",
        "name",
        "tf",
        "Variable",
        "tf",
        "zeros",
        "n_words",
        "name",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "batch_size",
        "n_lstm_steps",
        "dim_image",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "batch_size",
        "n_lstm_steps",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "batch_size",
        "n_lstm_steps",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "batch_size",
        "n_lstm_steps",
        "tf",
        "reshape",
        "video",
        "dim_image",
        "tf",
        "nn",
        "xw_plus_b",
        "video_flat",
        "encode_image_W",
        "encode_image_b",
        "tf",
        "reshape",
        "image_emb",
        "batch_size",
        "n_lstm_steps",
        "dim_hidden",
        "lstm1",
        "state_size",
        "tf",
        "zeros",
        "batch_size",
        "lstm1",
        "state_size",
        "tf",
        "zeros",
        "batch_size",
        "lstm2",
        "state_size",
        "tf",
        "zeros",
        "batch_size",
        "dim_hidden",
        "i",
        "n_lstm_steps",
        "i",
        "tf",
        "get_variable_scope",
        "reuse_variables",
        "tf",
        "variable_scope",
        "lstm1",
        "image_emb",
        "i",
        "state1",
        "tf",
        "variable_scope",
        "lstm2",
        "tf",
        "concat",
        "values",
        "padding",
        "output1",
        "axis",
        "state2",
        "i",
        "n_lstm_steps",
        "i",
        "tf",
        "zeros",
        "batch_size",
        "dim_hidden",
        "tf",
        "device",
        "tf",
        "nn",
        "embedding_lookup",
        "Wemb",
        "caption",
        "i",
        "tf",
        "get_variable_scope",
        "reuse_variables",
        "tf",
        "variable_scope",
        "lstm1",
        "padding",
        "state1",
        "tf",
        "variable_scope",
        "lstm2",
        "tf",
        "concat",
        "values",
        "current_embed",
        "output1",
        "axis",
        "state2",
        "tf",
        "expand_dims",
        "caption",
        "i",
        "tf",
        "expand_dims",
        "tf",
        "batch_size",
        "tf",
        "concat",
        "values",
        "indices",
        "labels",
        "axis",
        "tf",
        "sparse_to_dense",
        "concated",
        "tf",
        "stack",
        "batch_size",
        "n_words",
        "tf",
        "nn",
        "xw_plus_b",
        "output2",
        "embed_word_W",
        "embed_word_b",
        "tf",
        "nn",
        "softmax_cross_entropy_with_logits",
        "logits",
        "logit_words",
        "labels",
        "onehot_labels",
        "cross_entropy",
        "caption_mask",
        "i",
        "probs",
        "append",
        "logit_words",
        "tf",
        "reduce_sum",
        "cross_entropy",
        "loss",
        "current_loss",
        "loss",
        "tf",
        "reduce_sum",
        "caption_mask",
        "loss",
        "video",
        "video_mask",
        "caption",
        "caption_mask",
        "probs",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "n_lstm_steps",
        "dim_image",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "n_lstm_steps",
        "tf",
        "reshape",
        "video",
        "dim_image",
        "tf",
        "nn",
        "xw_plus_b",
        "video_flat",
        "encode_image_W",
        "encode_image_b",
        "tf",
        "reshape",
        "image_emb",
        "n_lstm_steps",
        "dim_hidden",
        "tf",
        "zeros",
        "lstm1",
        "state_size",
        "tf",
        "zeros",
        "lstm2",
        "state_size",
        "tf",
        "zeros",
        "dim_hidden",
        "i",
        "n_lstm_steps",
        "i",
        "tf",
        "get_variable_scope",
        "reuse_variables",
        "tf",
        "variable_scope",
        "lstm1",
        "image_emb",
        "i",
        "state1",
        "tf",
        "variable_scope",
        "lstm2",
        "tf",
        "concat",
        "padding",
        "output1",
        "axis",
        "state2",
        "i",
        "n_lstm_steps",
        "tf",
        "get_variable_scope",
        "reuse_variables",
        "i",
        "tf",
        "zeros",
        "dim_hidden",
        "tf",
        "variable_scope",
        "lstm1",
        "padding",
        "state1",
        "tf",
        "variable_scope",
        "lstm2",
        "tf",
        "concat",
        "current_embed",
        "output1",
        "axis",
        "state2",
        "tf",
        "nn",
        "xw_plus_b",
        "output2",
        "embed_word_W",
        "embed_word_b",
        "tf",
        "argmax",
        "logit_words",
        "generated_words",
        "append",
        "max_prob_index",
        "probs",
        "append",
        "logit_words",
        "tf",
        "device",
        "tf",
        "nn",
        "embedding_lookup",
        "Wemb",
        "max_prob_index",
        "tf",
        "expand_dims",
        "current_embed",
        "embeds",
        "append",
        "current_embed",
        "video",
        "video_mask",
        "generated_words",
        "probs",
        "embeds",
        "video_feat_path",
        "train_label_file",
        "train_ratio",
        "pd",
        "read_json",
        "train_label_file",
        "video_data",
        "x",
        "x",
        "video_data",
        "x",
        "os",
        "path",
        "join",
        "video_feat_path",
        "x",
        "video_data",
        "unique",
        "len",
        "unique_filenames",
        "train_ratio",
        "unique_filenames",
        "train_len",
        "unique_filenames",
        "train_len",
        "video_data",
        "video_data",
        "x",
        "x",
        "train_vids",
        "video_data",
        "video_data",
        "x",
        "x",
        "test_vids",
        "train_data",
        "test_data",
        "sentence_iterator",
        "word_count_threshold",
        "word_count_threshold",
        "sent",
        "sentence_iterator",
        "nsents",
        "w",
        "sent",
        "lower",
        "split",
        "word_counts",
        "get",
        "w",
        "w",
        "w",
        "word_counts",
        "word_counts",
        "w",
        "word_count_threshold",
        "len",
        "word_counts",
        "len",
        "vocab",
        "w",
        "vocab",
        "ix",
        "w",
        "ix",
        "nsents",
        "np",
        "array",
        "word_counts",
        "ixtoword",
        "i",
        "i",
        "ixtoword",
        "bias_init_vector",
        "np",
        "sum",
        "bias_init_vector",
        "np",
        "log",
        "bias_init_vector",
        "bias_init_vector",
        "np",
        "max",
        "bias_init_vector",
        "wordtoix",
        "ixtoword",
        "bias_init_vector",
        "get_video_data",
        "video_feat_path",
        "train_label_file",
        "train_ratio",
        "train_data",
        "captions",
        "x",
        "join",
        "x",
        "replace",
        "captions",
        "x",
        "join",
        "x",
        "replace",
        "preProBuildWordVocab",
        "captions",
        "word_count_threshold",
        "np",
        "save",
        "ixtoword",
        "Video_Caption_Generator",
        "dim_image",
        "dim_image",
        "n_words",
        "len",
        "wordtoix",
        "dim_hidden",
        "dim_hidden",
        "batch_size",
        "batch_size",
        "n_lstm_steps",
        "n_frame_step",
        "bias_init_vector",
        "bias_init_vector",
        "model",
        "build_model",
        "tf",
        "InteractiveSession",
        "tf",
        "train",
        "Saver",
        "max_to_keep",
        "tf",
        "variable_scope",
        "tf",
        "get_variable_scope",
        "reuse",
        "tf",
        "train",
        "AdamOptimizer",
        "learning_rate",
        "minimize",
        "tf_loss",
        "tf",
        "global_variables_initializer",
        "sess",
        "run",
        "init",
        "epoch",
        "n_epochs",
        "train_data",
        "index",
        "np",
        "random",
        "shuffle",
        "index",
        "train_data",
        "ix",
        "index",
        "train_data",
        "groupby",
        "apply",
        "x",
        "x",
        "irow",
        "np",
        "random",
        "choice",
        "len",
        "x",
        "current_train_data",
        "reset_index",
        "drop",
        "start",
        "end",
        "len",
        "current_train_data",
        "batch_size",
        "batch_size",
        "len",
        "current_train_data",
        "batch_size",
        "current_train_data",
        "start",
        "end",
        "current_batch",
        "values",
        "np",
        "zeros",
        "batch_size",
        "n_frame_step",
        "dim_image",
        "vid",
        "np",
        "load",
        "vid",
        "current_videos",
        "np",
        "zeros",
        "batch_size",
        "n_frame_step",
        "ind",
        "feat",
        "current_feats_vals",
        "current_feats",
        "ind",
        "len",
        "current_feats_vals",
        "ind",
        "feat",
        "current_video_masks",
        "ind",
        "len",
        "current_feats_vals",
        "ind",
        "current_batch",
        "values",
        "cap",
        "wordtoix",
        "word",
        "word",
        "join",
        "cap",
        "lower",
        "split",
        "word",
        "wordtoix",
        "current_captions",
        "sequence",
        "pad_sequences",
        "current_caption_ind",
        "padding",
        "maxlen",
        "n_frame_step",
        "np",
        "hstack",
        "current_caption_matrix",
        "np",
        "zeros",
        "len",
        "current_caption_matrix",
        "astype",
        "np",
        "zeros",
        "current_caption_matrix",
        "shape",
        "current_caption_matrix",
        "shape",
        "np",
        "array",
        "x",
        "x",
        "sum",
        "current_caption_matrix",
        "ind",
        "row",
        "current_caption_masks",
        "nonzeros",
        "ind",
        "sess",
        "run",
        "tf_probs",
        "feed_dict",
        "tf_video",
        "current_feats",
        "tf_caption",
        "current_caption_matrix",
        "sess",
        "run",
        "train_op",
        "tf_loss",
        "feed_dict",
        "tf_video",
        "current_feats",
        "tf_video_mask",
        "current_video_masks",
        "tf_caption",
        "current_caption_matrix",
        "tf_caption_mask",
        "current_caption_masks",
        "loss_val",
        "np",
        "mod",
        "epoch",
        "epoch",
        "saver",
        "save",
        "sess",
        "os",
        "path",
        "join",
        "model_path",
        "global_step",
        "epoch",
        "model_path",
        "video_feat_path",
        "video_feat_path",
        "get_video_data",
        "video_data_path",
        "video_feat_path",
        "train_ratio",
        "test_data",
        "unique",
        "pd",
        "Series",
        "np",
        "load",
        "tolist",
        "Video_Caption_Generator",
        "dim_image",
        "dim_image",
        "n_words",
        "len",
        "ixtoword",
        "dim_hidden",
        "dim_hidden",
        "batch_size",
        "batch_size",
        "n_lstm_steps",
        "n_frame_step",
        "bias_init_vector",
        "model",
        "build_generator",
        "tf",
        "InteractiveSession",
        "tf",
        "train",
        "Saver",
        "saver",
        "restore",
        "sess",
        "model_path",
        "video_feat_path",
        "test_videos",
        "video_feat_path",
        "np",
        "load",
        "video_feat_path",
        "np",
        "ones",
        "video_feat",
        "shape",
        "video_feat",
        "shape",
        "sess",
        "run",
        "caption_tf",
        "feed_dict",
        "video_tf",
        "video_feat",
        "video_mask_tf",
        "video_mask",
        "sess",
        "run",
        "probs_tf",
        "feed_dict",
        "video_tf",
        "video_feat",
        "sess",
        "run",
        "last_embed_tf",
        "feed_dict",
        "video_tf",
        "video_feat",
        "ixtoword",
        "generated_word_index",
        "np",
        "argmax",
        "np",
        "array",
        "generated_words",
        "generated_words",
        "punctuation",
        "join",
        "generated_words",
        "generated_sentence",
        "train"
    ],
    "literals": [
        "'Wemb'",
        "'encode_image_W'",
        "'encode_image_b'",
        "'embed_word_W'",
        "'embed_word_b'",
        "'embed_word_b'",
        "'self.lstm1.state_size'",
        "\"LSTM1\"",
        "\"LSTM2\"",
        "\"/cpu:0\"",
        "\"LSTM1\"",
        "\"LSTM2\"",
        "\"LSTM1\"",
        "\"LSTM2\"",
        "\"LSTM1\"",
        "\"LSTM2\"",
        "\"/cpu:0\"",
        "'/media/storage3/Study/data/youtube_videos'",
        "'MLDS_hw2_data/training_data/feat/'",
        "'MLDS_hw2_data/training_label.json'",
        "'./models/'",
        "'video_path'",
        "'id'",
        "'.npy'",
        "'video_path'",
        "'video_path'",
        "'video_path'",
        "'video_path'",
        "'video_path'",
        "'preprocessing word counts and creating vocab based on word count threshold %d'",
        "' '",
        "'filtered words from %d to %d'",
        "'.'",
        "'#START#'",
        "'.'",
        "'caption'",
        "''",
        "'.'",
        "''",
        "''",
        "','",
        "''",
        "'./data/ixtoword'",
        "'video_path'",
        "'video_path'",
        "'caption'",
        "\"\"",
        "' '",
        "'post'",
        "\"Epoch \"",
        "\" is done. Saving the model ...\"",
        "'model'",
        "'models/model-900'",
        "'video_path'",
        "'./data/ixtoword.npy'",
        "'.'",
        "' '"
    ],
    "variables": [
        "dim_image",
        "n_words",
        "dim_hidden",
        "batch_size",
        "n_lstm_steps",
        "rnn_cell",
        "Wemb",
        "lstm1",
        "lstm2",
        "encode_image_W",
        "encode_image_b",
        "embed_word_W",
        "embed_word_b",
        "embed_word_b",
        "video",
        "video_mask",
        "caption",
        "caption_mask",
        "video_flat",
        "image_emb",
        "image_emb",
        "state1",
        "state2",
        "padding",
        "probs",
        "loss",
        "output1",
        "state1",
        "output2",
        "state2",
        "current_embed",
        "current_embed",
        "output1",
        "state1",
        "output2",
        "state2",
        "labels",
        "indices",
        "concated",
        "onehot_labels",
        "logit_words",
        "cross_entropy",
        "cross_entropy",
        "current_loss",
        "loss",
        "video",
        "video_mask",
        "video_flat",
        "image_emb",
        "image_emb",
        "state1",
        "state2",
        "padding",
        "generated_words",
        "probs",
        "embeds",
        "output1",
        "state1",
        "output2",
        "state2",
        "current_embed",
        "output1",
        "state1",
        "output2",
        "state2",
        "logit_words",
        "max_prob_index",
        "current_embed",
        "current_embed",
        "video_path",
        "video_feat_path",
        "train_label_file",
        "model_path",
        "dim_image",
        "dim_hidden",
        "n_frame_step",
        "n_epochs",
        "batch_size",
        "learning_rate",
        "video_data",
        "video_data",
        "video_data",
        "unique_filenames",
        "train_len",
        "train_vids",
        "test_vids",
        "train_data",
        "test_data",
        "word_counts",
        "nsents",
        "word_counts",
        "w",
        "vocab",
        "ixtoword",
        "ixtoword",
        "wordtoix",
        "wordtoix",
        "ix",
        "wordtoix",
        "w",
        "ixtoword",
        "ix",
        "word_counts",
        "bias_init_vector",
        "bias_init_vector",
        "train_data",
        "_",
        "captions",
        "captions",
        "captions",
        "wordtoix",
        "ixtoword",
        "bias_init_vector",
        "model",
        "tf_loss",
        "tf_video",
        "tf_video_mask",
        "tf_caption",
        "tf_caption_mask",
        "tf_probs",
        "sess",
        "saver",
        "train_op",
        "init",
        "index",
        "train_data",
        "current_train_data",
        "current_train_data",
        "current_batch",
        "current_videos",
        "current_feats",
        "current_feats_vals",
        "current_video_masks",
        "current_captions",
        "current_caption_ind",
        "current_caption_matrix",
        "current_caption_matrix",
        "current_caption_masks",
        "nonzeros",
        "row",
        "probs_val",
        "_",
        "loss_val",
        "train_data",
        "test_data",
        "test_videos",
        "ixtoword",
        "model",
        "video_tf",
        "video_mask_tf",
        "caption_tf",
        "probs_tf",
        "last_embed_tf",
        "sess",
        "saver",
        "video_feat",
        "video_mask",
        "generated_word_index",
        "probs_val",
        "embed_val",
        "generated_words",
        "punctuation",
        "generated_words",
        "generated_sentence"
    ],
    "comments": [
        "import ipdb",
        "import cv2",
        "from tensorflow.models.rnn import rnn_cell",
        "import tf.contrib.rnn as rnn_cell",
        "with tf.device(\"/cpu:0\"):",
        "(batch_size*n_lstm_steps, dim_hidden)",
        "LSTMStateTuple(c=256, h=256)",
        "Phase 1 => only read frames",
        "Each video might have different length. Need to mask those.",
        "But how? Padding with 0 would be enough?",
        "Therefore... TODO: for those short videos, keep the last LSTM hidden and output til the end.",
        "Phase 2 => only generate captions",
        "tf.pack has been renamed as tf.stack.",
        "Global Parameters ###############",
        "Train Parameters #################",
        "",
        "video_data = video_data[video_data['video_path'].map(lambda x: os.path.exists(x))]",
        "video_data = video_data[video_data['caption'].map(lambda x: isinstance(x, str))]",
        "borrowed this function from NeuralTalk",
        "period at the end of the sentence. make first dimension be end token",
        "make first vector be the start token",
        "normalize to frequencies",
        "shift to nice numeric range",
        "tf.initialize_all_variables().run()",
        "print current_videos[0]",
        "print np.load(current_videos[0])",
        "ipdb.set_trace()",
        "ipdb.set_trace()"
    ],
    "docstrings": [],
    "functions": [
        "build_model",
        "build_generator",
        "get_video_data",
        "preProBuildWordVocab",
        "train",
        "test"
    ],
    "classes": [
        "Video_Caption_Generator"
    ]
}