{
    "identifiers": [
        "requests",
        "random",
        "redis_slave_connect",
        "conn",
        "aiohttp",
        "page_url",
        "page_url",
        "url",
        "r",
        "url",
        "decode",
        "aiohttp",
        "ClientSession",
        "session",
        "session",
        "get",
        "url",
        "resp",
        "resp",
        "text",
        "encoding",
        "data",
        "data"
    ],
    "literals": [
        "'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19'",
        "'Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30'",
        "'Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'",
        "'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0'",
        "'Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0'",
        "'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'",
        "'Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19'",
        "'Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3'",
        "'Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3'",
        "\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0\"",
        "'user-agent'",
        "'utf-8'",
        "'gbk'"
    ],
    "variables": [
        "User_Agent",
        "url",
        "r",
        "headers",
        "url",
        "data"
    ],
    "comments": [
        "-*- coding: utf-8 -*-",
        "from urllib import request",
        "import urllib.request",
        "from urllib import parse",
        "url = conn.brpop('task_url')",
        "useragent = random.choice(User_Agent)",
        "proxyip = conn.srandmember('proxyip_pool')",
        "proxies = {'https': proxyip}",
        "assert resp.status == 200"
    ],
    "docstrings": [
        "'''\nclass HtmlDownloader(object):\n\n    def download(self, url):\n        if url is None:\n            return \n\n        req = request.Request(url)  # the html is downloaded in the local\n        #req = Request(url)\n        #携带浏览器的头部，防止网站反爬\n        req.add_header(\"User-Agent\",\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:49.0) Gecko/20100101 Firefox/49.0\")\n        response = request.urlopen(req)\n        if response.getcode() != 200:\n            return None\n        #print (response.read())\n        return response.read()\n'''",
        "'''\n        async with aiohttp.request('GET', url) as resp:\n            assert resp.status == 200\n            await resp.text(encoding = 'gbk')\n            return resp.text\n\n        '''",
        "'''使用requests同步请求\n        try:\n            #response = requests.get(url, proxies = proxies, timeout = 5)\n            response = requests.get(url, headers = headers, timeout = 5)\n            response.encoding = 'utf-8'\n            print(response.status_code)\n            print(response.text)\n            return response.text\n        except:\n            print('request error')\n        '''",
        "'''\nif __name__ == '__main__':\n    test = HtmlDownloader()\n    test.download('http://www.baidu.com')\n'''"
    ],
    "functions": [
        "download"
    ],
    "classes": [
        "HtmlDownloader"
    ]
}