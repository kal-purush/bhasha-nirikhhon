{
    "identifiers": [
        "argparse",
        "genrl",
        "A2C",
        "PPO1",
        "genrl",
        "deep",
        "common",
        "OnPolicyTrainer",
        "genrl",
        "deep",
        "common",
        "actor_critic",
        "MlpActorCritic",
        "genrl",
        "deep",
        "common",
        "utils",
        "get_env_properties",
        "genrl",
        "environments",
        "VectorEnv",
        "genrl",
        "evolutionary",
        "GeneticHyperparamTuner",
        "log",
        "log",
        "log",
        "log",
        "split",
        "log",
        "log",
        "i",
        "i",
        "log",
        "i",
        "i",
        "i",
        "log",
        "i",
        "log",
        "GeneticHyperparamTuner",
        "agent",
        "agent",
        "get_logging_params",
        "agents",
        "envirnment",
        "args",
        "get_logger",
        "args",
        "log",
        "agent",
        "agents",
        "OnPolicyTrainer",
        "agent",
        "envirnment",
        "logger",
        "epochs",
        "args",
        "epochs",
        "render",
        "args",
        "render",
        "log_interval",
        "args",
        "log_interval",
        "trainer",
        "train",
        "trainer",
        "generations",
        "no_of_parents",
        "agent_parameter_choices",
        "envirnment",
        "generic_agent",
        "args",
        "GATuner",
        "agent_parameter_choices",
        "optimizer",
        "initialize_population",
        "no_of_parents",
        "generic_agent",
        "i",
        "generations",
        "i",
        "generations",
        "train_population",
        "agents",
        "envirnment",
        "args",
        "optimizer",
        "grade",
        "agents",
        "avg_reward",
        "i",
        "generations",
        "optimizer",
        "evolve",
        "agents",
        "sorted",
        "agents",
        "key",
        "x",
        "optimizer",
        "fitness",
        "x",
        "reverse",
        "i",
        "i",
        "optimizer",
        "fitness",
        "agents",
        "i",
        "args",
        "VectorEnv",
        "args",
        "env",
        "n_envs",
        "args",
        "n_envs",
        "parallel",
        "args",
        "serial",
        "env_type",
        "args",
        "env_type",
        "get_env_properties",
        "env",
        "MlpActorCritic",
        "input_dim",
        "action_dim",
        "discrete",
        "action_lim",
        "action_lim",
        "activation",
        "A2C",
        "network",
        "env",
        "rollout_size",
        "args",
        "rollout_size",
        "generate",
        "args",
        "generations",
        "args",
        "population",
        "agent_parameter_choices",
        "env",
        "generic_agent",
        "args",
        "argparse",
        "ArgumentParser",
        "description",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "action",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument_group",
        "offpolicyargs",
        "add_argument",
        "help",
        "offpolicyargs",
        "add_argument",
        "help",
        "parser",
        "add_argument_group",
        "onpolicyargs",
        "add_argument",
        "help",
        "parser",
        "parse_args",
        "main",
        "args"
    ],
    "literals": [
        "\",\"",
        "\",\"",
        "\"\"",
        "\" \"",
        "\"\"",
        "\" \"",
        "\"mean_reward\"",
        "\"-\"",
        "f\"Doing generation {i}/{generations}\"",
        "f\"Generation avg reward:{avg_reward}\"",
        "\"-\"",
        "f\"Top {i+1} agent reward: {optimizer.fitness(agents[i])}\"",
        "\"mlp\"",
        "\"V\"",
        "\"relu\"",
        "\"gamma\"",
        "\"__main__\"",
        "\"Train Deep RL algorithms\"",
        "\"-e\"",
        "\"--env\"",
        "\"Which env to train on\"",
        "\"CartPole-v0\"",
        "\"--env-type\"",
        "\"What kind of env is it\"",
        "\"gym\"",
        "\"-n\"",
        "\"--n-envs\"",
        "\"Number of vectorized envs to train on\"",
        "\"--serial\"",
        "\"Vectorized envs should be serial or parallel\"",
        "\"--epochs\"",
        "\"How many epochs to train on\"",
        "\"--render\"",
        "\"Should the env be rendered\"",
        "\"store_true\"",
        "\"--log\"",
        "\"Comma separated string of logs\"",
        "\"stdout\"",
        "\"--arch\"",
        "\"Which architecture mlp/cnn for now\"",
        "\"mlp\"",
        "\"--log-interval\"",
        "\"Set Log interval\"",
        "\"--batch-size\"",
        "\"Batch Size\"",
        "\"--population\"",
        "\"No. of agents in a generation\"",
        "\"--generations\"",
        "\"No. of generations\"",
        "\"Off Policy Args\"",
        "\"-ws\"",
        "\"--warmup-steps\"",
        "\"Warmup steps\"",
        "\"--replay-size\"",
        "\"Replay Buffer Size\"",
        "\"On Policy Args\"",
        "\"--rollout-size\"",
        "\"Rollout Buffer Size\""
    ],
    "variables": [
        "log",
        "log",
        "log",
        "logger",
        "trainer",
        "optimizer",
        "agents",
        "avg_reward",
        "agents",
        "agents",
        "env",
        "input_dim",
        "action_dim",
        "discrete",
        "action_lim",
        "network",
        "generic_agent",
        "agent_parameter_choices",
        "parser",
        "offpolicyargs",
        "onpolicyargs",
        "args"
    ],
    "comments": [
        "\"\"\"",
        "Okay so parameters to tune:-",
        "- layers",
        "- lr_policy",
        "- lr_value",
        "- clip param",
        "- entropy coeff",
        "- value coeff",
        "- gamma",
        "\"\"\"",
        "Code inspired from https://github.com/harvitronix/neural-network-genetic-algorithm",
        "evolve the generation",
        "Train the agents",
        "get average fitness of the generation",
        "Evolve the generation",
        "sort our final population",
        "print rewards of top 5",
        "layers",
        "type of value function",
        "'clip_param': [0.2, 0.3],",
        "'lr_policy': [0.001, 0.002],",
        "'lr_value': [0.001, 0.002]",
        "parser.add_argument(\"-a\", \"--algo\", help=\"Which Algo to train\", default=\"ppo\", type=str)"
    ],
    "docstrings": [
        "\"\"\"\n        Return the mean rewards, which is our fitness function\n        \"\"\"",
        "\"\"\"\n    Train all the agents in the population\n\n    Args:\n        agents (List) : List of agent\n        envirnment: Gym envirnment\n\n    \"\"\"",
        "\"\"\"\n    Genetic Algorithm for RL\n\n    Args:\n        generations (int): No of generations\n        no_of_parents(int): No of agents in a generation\n        agent_parameter_choices(Dict): Parameter choices for the agent\n        envirnment: Gym Envirnment\n        generic_agent : RL Agent to be tuned\n\n\n    \"\"\""
    ],
    "functions": [
        "get_logger",
        "fitness",
        "train_population",
        "generate",
        "main"
    ],
    "classes": [
        "GATuner"
    ]
}