{
    "identifiers": [
        "pickle",
        "gram",
        "pickle",
        "load",
        "open",
        "gram",
        "word",
        "known",
        "edits0",
        "word",
        "known",
        "edits1",
        "word",
        "known",
        "edits2",
        "word",
        "word",
        "max",
        "candidates",
        "key",
        "x",
        "counter",
        "get",
        "x",
        "text",
        "join",
        "correct",
        "text",
        "split",
        "words",
        "w",
        "w",
        "words",
        "w",
        "counter",
        "word",
        "word",
        "word",
        "allSplits",
        "word",
        "a",
        "b",
        "a",
        "b",
        "pairs",
        "b",
        "a",
        "c",
        "b",
        "a",
        "b",
        "pairs",
        "c",
        "alphabet",
        "b",
        "a",
        "c",
        "b",
        "a",
        "b",
        "pairs",
        "c",
        "alphabet",
        "a",
        "b",
        "a",
        "b",
        "a",
        "b",
        "pairs",
        "a",
        "b",
        "deletes",
        "replaces",
        "inserts",
        "switches",
        "word",
        "e2",
        "e1",
        "edits1",
        "word",
        "e2",
        "edits1",
        "e1",
        "word",
        "e3",
        "e1",
        "edits1",
        "word",
        "e2",
        "edits1",
        "e1",
        "e3",
        "edits1",
        "e2",
        "word",
        "word",
        "i",
        "word",
        "i",
        "i",
        "len",
        "word",
        "word",
        "allSplits",
        "word",
        "a",
        "b",
        "a",
        "b",
        "pairs",
        "b",
        "deletes",
        "word",
        "allSplits",
        "word",
        "a",
        "c",
        "b",
        "a",
        "b",
        "pairs",
        "c",
        "alphabet",
        "b",
        "replaces",
        "word",
        "allSplits",
        "word",
        "a",
        "c",
        "b",
        "a",
        "b",
        "pairs",
        "c",
        "alphabet",
        "inserts",
        "word",
        "allSplits",
        "word",
        "a",
        "b",
        "b",
        "b",
        "a",
        "b",
        "pairs",
        "len",
        "b",
        "transposes",
        "Corrector",
        "crt",
        "correct",
        "crt",
        "correct",
        "crt",
        "correct"
    ],
    "literals": [
        "\"Loading corpus...\"",
        "\"gram.counter\"",
        "'rb'",
        "\"Corpus loading complete!\"",
        "'abcdefghijklmnopqrstuvwxyz'",
        "' '",
        "\"__main__\"",
        "\"zhognguo\"",
        "\"qighuadaxeu\"",
        "\"beijindaxue\""
    ],
    "variables": [
        "counter",
        "alphabet",
        "candidates",
        "pairs",
        "deletes",
        "replaces",
        "inserts",
        "switches",
        "pairs",
        "deletes",
        "pairs",
        "replaces",
        "pairs",
        "inserts",
        "pairs",
        "transposes",
        "crt"
    ],
    "comments": [
        "!/usr/bin/env python",
        "coding: utf-8",
        "author: https://github.com/SimZhou",
        "## 思考总结：",
        "",
        "",
        "",
        "",
        ""
    ],
    "docstrings": [
        "'''\n        the grams parameter specifies the N-gram counter object to read from.\n        Currently we only have 1gram, 4gram and 7gram\n        '''",
        "'''\n        This function corrects a sequence of pinyin splited by spaces.\n        E.g. \n        \"wo shi shiu\" -> \"wo shi shui\"\n        '''",
        "'''\n        判断某个词是否在Counter中\n        '''",
        "'''\n        编辑距离为1的词，应该是这些词，在原词的每个<切分点>，可以有这么些操作：\n        1. 漏打：删除<切分点>右边的字符（当切分点右边有词时）\n        2. 错打：替换<切分点>右边的字符（当切分点右边有词时）\n        3. 多打：在<切分点>插入一个字符\n        4. 换位：两个字符的位置打反了，交换切分点左右的两个词（当切分点左右都有词时）\n        '''",
        "'''编辑距离为2的词，就是所有那些与目标词编辑距离为1的词 编辑距离为1的词'''",
        "'''Return a list of all possible (first, rest) pairs that comprise pinyin'''",
        "**1. 成功通过n-gram实现了连打拼音纠错功能，初步判断效果还不错**",
        "**2. 我最后的实现方式和高老师上课所提到的方法不同：高老师提到的先切分再改进的方法，我尝试了一下，由于错误的输入序列很难通过语言模型进行正确的分割，所以没能成功；因此我改用了另一种方法，也就是直接进行纠错，然后再切分的方法，成功实现了**",
        "**3. 这种实现方式需要的计算量并不算大，不过需要提前建立n-gram的语言模型并保存到硬盘中，需要的储存量可能较大，这可能也是各大输入法的拼音纠错功能仅在联网时开启的原因？（手机端）**",
        "**改进方向1：这次用的语料库有限（仅为article_9k），可以用过增加、改进语料库来进一步提升效果**",
        "**改进方向2：这次没有考虑中文分词，可以考虑在预处理token的时候，先用jieba进行分词，然后再建n-gram，而不是现在这样简单地用单个中文字当作token**"
    ],
    "functions": [
        "correct",
        "correct_sequence",
        "known",
        "edits0",
        "edits1",
        "edits2",
        "edits3",
        "allSplits",
        "get_deletes",
        "get_replaces",
        "get_inserts",
        "get_transposes"
    ],
    "classes": [
        "Corrector"
    ]
}