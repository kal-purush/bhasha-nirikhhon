{
    "identifiers": [
        "pickle",
        "pandas",
        "pd",
        "sklearn",
        "feature_extraction",
        "text",
        "CountVectorizer",
        "nltk",
        "tokenize",
        "RegexpTokenizer",
        "sklearn",
        "model_selection",
        "train_test_split",
        "nltk",
        "stem",
        "porter",
        "PorterStemmer",
        "nltk",
        "tokenize",
        "word_tokenize",
        "nltk",
        "corpus",
        "stopwords",
        "datetime",
        "datetime",
        "re",
        "sklearn",
        "pipeline",
        "Pipeline",
        "sklearn",
        "feature_extraction",
        "text",
        "TfidfTransformer",
        "sklearn",
        "metrics",
        "classification_report",
        "sklearn",
        "preprocessing",
        "StandardScaler",
        "tqdm",
        "tqdm",
        "itertools",
        "csv",
        "tqdm",
        "pandas",
        "desc",
        "sklearn",
        "naive_bayes",
        "BernoulliNB",
        "ComplementNB",
        "MultinomialNB",
        "sklearn",
        "neighbors",
        "KNeighborsClassifier",
        "sklearn",
        "tree",
        "DecisionTreeClassifier",
        "sklearn",
        "ensemble",
        "RandomForestClassifier",
        "AdaBoostClassifier",
        "sklearn",
        "linear_model",
        "LogisticRegression",
        "SGDClassifier",
        "sklearn",
        "neural_network",
        "MLPClassifier",
        "sklearn",
        "metrics",
        "sklearn",
        "feature_extraction",
        "text",
        "TfidfVectorizer",
        "ComplementNB",
        "DecisionTreeClassifier",
        "RandomForestClassifier",
        "LogisticRegression",
        "max_iter",
        "AdaBoostClassifier",
        "clean_json",
        "pd",
        "read_json",
        "data",
        "info",
        "pipeline",
        "data",
        "name",
        "path",
        "vectorizer",
        "vec_name",
        "pd",
        "read_json",
        "path",
        "vectorizer",
        "transform",
        "article",
        "vec_name",
        "name",
        "pickle",
        "load",
        "open",
        "filename",
        "loaded_clf",
        "predict_proba",
        "article_counts",
        "i",
        "len",
        "predicted",
        "predicted",
        "i",
        "predicted",
        "i",
        "predicted",
        "i",
        "article",
        "i",
        "proba_str",
        "re",
        "compile",
        "re",
        "compile",
        "stopwords",
        "words",
        "text",
        "text",
        "lower",
        "REPLACE_BY_SPACE_RE",
        "sub",
        "text",
        "BAD_SYMBOLS_RE",
        "sub",
        "text",
        "join",
        "word",
        "word",
        "text",
        "split",
        "word",
        "STOPWORDS",
        "text",
        "data",
        "pd",
        "DataFrame",
        "data",
        "apply",
        "clean_text",
        "RegexpTokenizer",
        "CountVectorizer",
        "tokenizer",
        "tokens",
        "tokenize",
        "stop_words",
        "cv",
        "fit_transform",
        "data",
        "cv",
        "get_feature_names",
        "text_counts",
        "shape",
        "feature_list",
        "i",
        "i",
        "i",
        "len",
        "feature_list",
        "open",
        "f",
        "csv",
        "DictReader",
        "f",
        "tqdm",
        "total",
        "row",
        "reader",
        "row",
        "row",
        "feature_dict",
        "feature_dict",
        "row",
        "doc_i",
        "text_counts",
        "doc_i",
        "i",
        "text_counts",
        "doc_i",
        "i",
        "row",
        "pbar",
        "update",
        "pbar",
        "close",
        "open",
        "f",
        "csv",
        "DictReader",
        "f",
        "tqdm",
        "total",
        "row",
        "reader",
        "row",
        "row",
        "feature_dict",
        "feature_dict",
        "row",
        "doc_i",
        "data",
        "doc_i",
        "text_counts",
        "doc_i",
        "i",
        "text_counts",
        "doc_i",
        "i",
        "row",
        "pbar",
        "update",
        "pbar",
        "close",
        "open",
        "f",
        "csv",
        "DictReader",
        "f",
        "tqdm",
        "total",
        "row",
        "reader",
        "row",
        "row",
        "feature_dict",
        "feature_dict",
        "row",
        "doc_i",
        "data",
        "doc_i",
        "text_counts",
        "doc_i",
        "i",
        "text_counts",
        "doc_i",
        "i",
        "row",
        "pbar",
        "update",
        "pbar",
        "close",
        "TfidfTransformer",
        "fit_transform",
        "text_counts",
        "train_test_split",
        "tfidf_counts",
        "data",
        "test_size",
        "random_state",
        "AdaBoostClassifier",
        "fit",
        "X_train",
        "y_train",
        "clf",
        "predict",
        "X_test",
        "log_new",
        "metrics",
        "accuracy_score",
        "y_test",
        "y_pred",
        "accuracy",
        "name",
        "log_results",
        "accuracy",
        "name",
        "classification_report",
        "y_test",
        "y_pred",
        "target_names",
        "my_tags",
        "name",
        "pickle",
        "dump",
        "clf",
        "open",
        "filename",
        "result",
        "open",
        "f",
        "datetime",
        "now",
        "now",
        "strftime",
        "f",
        "write",
        "dt_string",
        "result",
        "open",
        "f",
        "datetime",
        "now",
        "now",
        "strftime",
        "f",
        "write",
        "dt_string",
        "path_r",
        "path_w",
        "open",
        "path_r",
        "f_r",
        "open",
        "path_w",
        "f_w",
        "line",
        "f_r",
        "readlines",
        "count",
        "f_w",
        "write",
        "count",
        "count",
        "f_w",
        "write",
        "line",
        "count",
        "f_w",
        "write",
        "line",
        "f_w",
        "write",
        "main"
    ],
    "literals": [
        "\"progress-bar\"",
        "\"ComplementNB\"",
        "\"DecisionTreeClassifier\"",
        "\"RandomForestClassifier\"",
        "\"LogisticRegression\"",
        "\"AdaBoostClassifier\"",
        "\"\\nCleaning json...\"",
        "'../released_data.json'",
        "'../Dataset/cleaned_data.json'",
        "\"\\nReading json...\"",
        "'../Dataset/cleaned_data.json'",
        "\"nothing\"",
        "\"\\nPipeline training...\"",
        "'content'",
        "\"Models/\"",
        "\"_\"",
        "\".sav\"",
        "'rb'",
        "F\"L: {predicted[i][1]:.2%} | R: {predicted[i][2]:.2%} | C: {predicted[i][0]:.2%}\"",
        "F\"\\t\\\"{article['title'][i][:10]}...\\\" | \"",
        "'title'",
        "'[/(){}\\[\\]\\|@,;]'",
        "'[^0-9a-z #+_]'",
        "'english'",
        "' '",
        "''",
        "' '",
        "'content'",
        "r'[a-zA-Z]+'",
        "'english'",
        "'content'",
        "\"./../Dataset/ibc_data/feature_lists/neu_list.csv\"",
        "'r'",
        "'gram'",
        "'1'",
        "'1st'",
        "'1st'",
        "'freq'",
        "\"./../Dataset/ibc_data/feature_lists/lib_list.csv\"",
        "'r'",
        "'gram'",
        "'1'",
        "'1st'",
        "'1st'",
        "'allsides_bias'",
        "\"From the Left\"",
        "'freq'",
        "\"./../Dataset/ibc_data/feature_lists/con_list.csv\"",
        "'r'",
        "'gram'",
        "'1'",
        "'1st'",
        "'1st'",
        "'allsides_bias'",
        "\"From the Right\"",
        "'freq'",
        "'allsides_bias'",
        "\"AdaBoostClassifier\"",
        "F\"{accuracy:.2%} - pl{name}\"",
        "F\"{accuracy:.2%} - pl{name}\"",
        "'From the Right'",
        "'From the Left'",
        "'From the Center'",
        "'Models/pl_'",
        "'.sav'",
        "'wb'",
        "\"results.txt\"",
        "'a'",
        "\"%H:%M:%S %d/%m/%Y\"",
        "\" - \"",
        "\"\\n\"",
        "\"results.txt\"",
        "'a'",
        "\"%H:%M:%S %d/%m/%Y\"",
        "\"\\n\"",
        "\" - New training session:\\n\"",
        "'r'",
        "'w'",
        "\"[\\n\"",
        "\",\"",
        "\"\\n]\"",
        "'__main__'"
    ],
    "variables": [
        "classifiers",
        "data",
        "clf_name",
        "article",
        "article_counts",
        "filename",
        "loaded_clf",
        "predicted",
        "proba_str",
        "REPLACE_BY_SPACE_RE",
        "BAD_SYMBOLS_RE",
        "STOPWORDS",
        "text",
        "text",
        "text",
        "text",
        "tokens",
        "cv",
        "text_counts",
        "feature_list",
        "feature_dict",
        "reader",
        "pbar",
        "i",
        "reader",
        "pbar",
        "i",
        "reader",
        "pbar",
        "i",
        "tfidf_counts",
        "X_train",
        "X_test",
        "y_train",
        "y_test",
        "clf",
        "y_pred",
        "accuracy",
        "name",
        "my_tags",
        "filename",
        "now",
        "dt_string",
        "now",
        "dt_string",
        "count"
    ],
    "comments": [
        "comment/uncomment to choose classifiers to be trained",
        "\"BernoulliNB\": BernoulliNB(),",
        "\"MultinomialNB\": MultinomialNB(),",
        "\"KNeighborsClassifier\": KNeighborsClassifier(),",
        "\"MLPClassifier\": MLPClassifier(max_iter=1000),",
        "uncomment to ues full dataset",
        "clean_json('../Dataset/test_data.json', '../Dataset/cleaned_data.json') # comment to use smaller dataset",
        "reads json data into a pandas dataframe object",
        "uncomment to train all classifiers",
        "pipeline(data, clf_name)  # uncomment to train specific classifier",
        "transform new article into doc-term matrix",
        "run trained model on new article",
        "predicted = loaded_clf.predict(article_counts) # predict class",
        "predict probabilities",
        "print(\"\\t\\\"\" + article['title'][i][:20] + \"...\\\" : \" + predicted[i]) # print class",
        "print probabilities",
        "lowercase text",
        "replace REPLACE_BY_SPACE_RE symbols by space in text",
        "delete symbols which are in BAD_SYMBOLS_RE from text",
        "delete stopwors from text",
        "use pipeline to normalize document term matrix",
        "row: document number, col: feature frequency, ordered by get_features_names()",
        "turn feature_list into a dict with the index as value -> random access",
        "TODO try bigrams/trigrams + try to pickle the matrices",
        "tfidf_counts = text_counts",
        "clf = SGDClassifier().fit(X_train, y_train)"
    ],
    "docstrings": [],
    "functions": [
        "main",
        "classify_out_of_sample",
        "clean_text",
        "pipeline",
        "log_results",
        "log_new",
        "clean_json"
    ],
    "classes": []
}