{
    "identifiers": [
        "time",
        "monitor",
        "Monitor",
        "tb_dcgm_types",
        "gpu_free_memory",
        "GPUFreeMemory",
        "tb_dcgm_types",
        "gpu_tensoractive",
        "GPUTensorActive",
        "tb_dcgm_types",
        "gpu_peak_memory",
        "GPUPeakMemory",
        "tb_dcgm_types",
        "gpu_utilization",
        "GPUUtilization",
        "tb_dcgm_types",
        "gpu_power_usage",
        "GPUPowerUsage",
        "tb_dcgm_types",
        "gpu_fp32active",
        "GPUFP32Active",
        "tb_dcgm_types",
        "gpu_dram_active",
        "GPUDRAMActive",
        "tb_dcgm_types",
        "gpu_pcie_rx",
        "GPUPCIERX",
        "tb_dcgm_types",
        "gpu_pcie_tx",
        "GPUPCIETX",
        "tb_dcgm_types",
        "da_exceptions",
        "TorchBenchAnalyzerException",
        "dcgm_agent",
        "dcgm_fields",
        "dcgm_field_helpers",
        "dcgm_structs",
        "structs",
        "Monitor",
        "GPUPeakMemory",
        "GPUFreeMemory",
        "GPUUtilization",
        "GPUPowerUsage",
        "gpus",
        "frequency",
        "metrics",
        "frequency",
        "metrics",
        "pynvml",
        "pynvml",
        "_nvml",
        "nvmlInit",
        "metrics",
        "gpus",
        "gpu",
        "_gpus",
        "_gpu_handles",
        "_nvml",
        "nvmlDeviceGetHandleByUUID",
        "gpu",
        "device_uuid",
        "_records",
        "metric",
        "_metrics",
        "_records",
        "gpu",
        "_get_gpu_metrics",
        "gpu",
        "_gpus",
        "_nvml",
        "nvmlDeviceGetHandleByUUID",
        "gpu",
        "device_uuid",
        "metric",
        "_metrics",
        "model_analyzer_to_nvml_field",
        "metric",
        "time",
        "time_ns",
        "metric",
        "GPUPeakMemory",
        "_nvml",
        "nvmlDeviceGetMemoryInfo",
        "handle",
        "_records",
        "gpu",
        "metric",
        "append",
        "atimestamp",
        "getattr",
        "info",
        "nvml_field",
        "metric",
        "GPUFreeMemory",
        "_nvml",
        "nvmlDeviceGetMemoryInfo",
        "handle",
        "_records",
        "gpu",
        "metric",
        "append",
        "atimestamp",
        "getattr",
        "info",
        "nvml_field",
        "metric",
        "GPUUtilization",
        "_nvml",
        "nvmlDeviceGetUtilizationRates",
        "handle",
        "_records",
        "gpu",
        "metric",
        "append",
        "atimestamp",
        "getattr",
        "info",
        "nvml_field",
        "metric",
        "GPUPowerUsage",
        "_nvml",
        "nvmlDeviceGetPowerUsage",
        "handle",
        "_records",
        "gpu",
        "metric",
        "append",
        "atimestamp",
        "info",
        "gpu",
        "_gpus",
        "metric_type",
        "_metrics",
        "measurement",
        "_records",
        "gpu",
        "metric_type",
        "records",
        "append",
        "metric_type",
        "value",
        "measurement",
        "timestamp",
        "measurement",
        "device_uuid",
        "gpu",
        "device_uuid",
        "records",
        "_nvml",
        "nvmlShutdown",
        "destroy"
    ],
    "literals": [
        "\"used\"",
        "\"free\"",
        "\"utilization.gpu\"",
        "\"power.draw\""
    ],
    "variables": [
        "model_analyzer_to_nvml_field",
        "_nvml",
        "_metrics",
        "_records",
        "_gpus",
        "_gpu_handles",
        "gpu",
        "gpu",
        "metric",
        "handle",
        "nvml_field",
        "atimestamp",
        "info",
        "info",
        "info",
        "info",
        "records"
    ],
    "comments": [
        "Mapping between the NVML Fields and Model Analyzer Records",
        "For more explainations, please refer to https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html",
        "raw records: {gpu: {field: [(timestamp, value), ...]}}",
        "gpu handles: {gpu: handle}",
        "@Yueming TODO: need to update with the nvml API version 2. Because the nvml API version 1 returns the used memory including the memory allocated by the GPU driver.",
        "used_mem = info.used",
        "reserved_mem = info.reserved",
        "self._records[gpu][metric].append((atimestamp, used_mem - reserved_mem))"
    ],
    "docstrings": [
        "\"\"\"\n    Use NVML to monitor GPU metrics\n    \"\"\"",
        "\"\"\"\n        Parameters\n        ----------\n        gpus : list of GPUDevice\n            The gpus to be monitored\n        frequency : int\n            Sampling frequency for the metric\n        metrics : list\n            List of Record types to monitor\n        \"\"\"",
        "\"\"\"\n        Get the metrics of all the GPUs\n        \"\"\""
    ],
    "functions": [
        "_monitoring_iteration",
        "_get_gpu_metrics",
        "_collect_records",
        "destroy"
    ],
    "classes": [
        "NVMLMonitor"
    ]
}