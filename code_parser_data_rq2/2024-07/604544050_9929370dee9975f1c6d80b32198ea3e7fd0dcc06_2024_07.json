{
    "identifiers": [
        "tempfile",
        "unittest",
        "functools",
        "partial",
        "torch",
        "accelerate",
        "Accelerator",
        "datasets",
        "Dataset",
        "parameterized",
        "parameterized",
        "pytest",
        "mark",
        "transformers",
        "AutoModel",
        "AutoModelForCausalLM",
        "AutoModelForSeq2SeqLM",
        "AutoTokenizer",
        "trl",
        "BCOConfig",
        "BCOTrainer",
        "trl",
        "trainer",
        "bco_trainer",
        "_process_tokens",
        "_tokenize",
        "testing_utils",
        "require_no_wandb",
        "require_peft",
        "unittest",
        "TestCase",
        "classmethod",
        "cls",
        "AutoModelForCausalLM",
        "from_pretrained",
        "cls",
        "model_id",
        "AutoModelForCausalLM",
        "from_pretrained",
        "cls",
        "model_id",
        "AutoTokenizer",
        "from_pretrained",
        "cls",
        "model_id",
        "cls",
        "tokenizer",
        "cls",
        "tokenizer",
        "eos_token",
        "AutoModelForSeq2SeqLM",
        "from_pretrained",
        "model_id",
        "AutoModelForSeq2SeqLM",
        "from_pretrained",
        "model_id",
        "AutoTokenizer",
        "from_pretrained",
        "model_id",
        "AutoModel",
        "from_pretrained",
        "model_id",
        "AutoTokenizer",
        "from_pretrained",
        "model_id",
        "Dataset",
        "from_dict",
        "dummy_dataset_dict",
        "parameterized",
        "expand",
        "name",
        "pre_compute",
        "eval_dataset",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "precompute_ref_log_probs",
        "pre_compute",
        "report_to",
        "_init_dummy_dataset",
        "name",
        "model",
        "ref_model",
        "tokenizer",
        "name",
        "t5_model",
        "t5_ref_model",
        "t5_tokenizer",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "eval_dataset",
        "n",
        "param",
        "clone",
        "n",
        "param",
        "trainer",
        "model",
        "named_parameters",
        "trainer",
        "train",
        "assertIsNotNone",
        "trainer",
        "state",
        "log_history",
        "n",
        "param",
        "previous_trainable_params",
        "items",
        "trainer",
        "model",
        "get_parameter",
        "n",
        "param",
        "sum",
        "assertFalse",
        "torch",
        "equal",
        "param",
        "cpu",
        "new_param",
        "cpu",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "report_to",
        "_init_dummy_dataset",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "dummy_dataset",
        "_tokenize",
        "fn_kwargs",
        "trainer",
        "tokenizer",
        "batched",
        "batch_size",
        "assertListEqual",
        "tokenized_dataset",
        "dummy_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "dummy_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "dummy_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "assertListEqual",
        "tokenized_dataset",
        "trainer",
        "is_encoder_decoder",
        "trainer",
        "tokenizer",
        "trainer",
        "max_length",
        "trainer",
        "truncation_mode",
        "trainer",
        "label_pad_token_id",
        "trainer",
        "max_prompt_length",
        "tokenized_dataset",
        "_process_tokens",
        "fn_kwargs",
        "fn_kwargs",
        "num_proc",
        "assertListEqual",
        "processed_dataset",
        "dummy_dataset",
        "assertListEqual",
        "processed_dataset",
        "dummy_dataset",
        "assertListEqual",
        "processed_dataset",
        "dummy_dataset",
        "assertListEqual",
        "processed_dataset",
        "assertListEqual",
        "processed_dataset",
        "assertListEqual",
        "processed_dataset",
        "assertListEqual",
        "processed_dataset",
        "assertListEqual",
        "processed_dataset",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "report_to",
        "_init_dummy_dataset",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "n",
        "param",
        "clone",
        "n",
        "param",
        "trainer",
        "model",
        "named_parameters",
        "trainer",
        "train",
        "assertIsNotNone",
        "trainer",
        "state",
        "log_history",
        "n",
        "param",
        "previous_trainable_params",
        "items",
        "trainer",
        "model",
        "get_parameter",
        "n",
        "param",
        "sum",
        "assertFalse",
        "torch",
        "equal",
        "param",
        "cpu",
        "new_param",
        "cpu",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "report_to",
        "_init_dummy_dataset",
        "input_ids",
        "attention_mask",
        "model",
        "model",
        "input_ids",
        "input_ids",
        "attention_mask",
        "attention_mask",
        "outputs",
        "last_hidden_state",
        "mean",
        "dim",
        "Accelerator",
        "prepare_model",
        "embedding_model",
        "partial",
        "embed_prompt",
        "model",
        "embedding_model",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "embedding_func",
        "embedding_func",
        "embedding_tokenizer",
        "embedding_tokenizer",
        "n",
        "param",
        "clone",
        "n",
        "param",
        "trainer",
        "model",
        "named_parameters",
        "trainer",
        "train",
        "assertIsNotNone",
        "trainer",
        "state",
        "log_history",
        "n",
        "param",
        "previous_trainable_params",
        "items",
        "trainer",
        "model",
        "get_parameter",
        "n",
        "param",
        "sum",
        "assertFalse",
        "torch",
        "equal",
        "param",
        "cpu",
        "new_param",
        "cpu",
        "require_peft",
        "mark",
        "peft_test",
        "peft",
        "LoraConfig",
        "LoraConfig",
        "r",
        "lora_alpha",
        "lora_dropout",
        "bias",
        "task_type",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "report_to",
        "_init_dummy_dataset",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "peft_config",
        "lora_config",
        "n",
        "param",
        "clone",
        "n",
        "param",
        "trainer",
        "model",
        "named_parameters",
        "trainer",
        "train",
        "assertIsNotNone",
        "trainer",
        "state",
        "log_history",
        "n",
        "param",
        "previous_trainable_params",
        "items",
        "n",
        "trainer",
        "model",
        "get_parameter",
        "n",
        "param",
        "sum",
        "assertFalse",
        "torch",
        "equal",
        "param",
        "cpu",
        "new_param",
        "cpu",
        "require_no_wandb",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "generate_during_eval",
        "report_to",
        "_init_dummy_dataset",
        "assertRaisesRegex",
        "ValueError",
        "expected_regex",
        "BCOTrainer",
        "model",
        "model",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "require_peft",
        "mark",
        "peft_test",
        "peft",
        "LoraConfig",
        "get_peft_model",
        "LoraConfig",
        "r",
        "lora_alpha",
        "lora_dropout",
        "bias",
        "task_type",
        "AutoModelForCausalLM",
        "from_pretrained",
        "model_id",
        "get_peft_model",
        "model",
        "lora_config",
        "tempfile",
        "TemporaryDirectory",
        "tmp_dir",
        "BCOConfig",
        "output_dir",
        "tmp_dir",
        "per_device_train_batch_size",
        "max_steps",
        "remove_unused_columns",
        "gradient_accumulation_steps",
        "learning_rate",
        "eval_strategy",
        "beta",
        "report_to",
        "_init_dummy_dataset",
        "BCOTrainer",
        "model",
        "model_peft",
        "ref_model",
        "args",
        "training_args",
        "tokenizer",
        "tokenizer",
        "train_dataset",
        "dummy_dataset",
        "eval_dataset",
        "dummy_dataset",
        "peft_config",
        "lora_config",
        "trainer",
        "train",
        "trainer",
        "save_model",
        "AutoModelForCausalLM",
        "from_pretrained",
        "tmp_dir",
        "OSError",
        "fail"
    ],
    "literals": [
        "\"trl-internal-testing/dummy-GPT2-correct-vocab\"",
        "\"trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab\"",
        "\"facebook/bart-base\"",
        "\"prompt\"",
        "\"Hey, hello\"",
        "\"How are you\"",
        "\"What is your name?\"",
        "\"What is your name?\"",
        "\"Which is the best programming language?\"",
        "\"Which is the best programming language?\"",
        "\"Which is the best programming language?\"",
        "\"completion\"",
        "\"hi nice to meet you\"",
        "\"leave me alone\"",
        "\"I don't have a name\"",
        "\"My name is Mary\"",
        "\"Python\"",
        "\"C++\"",
        "\"Java\"",
        "\"label\"",
        "\"gpt2\"",
        "\"gpt2\"",
        "\"gpt2\"",
        "\"gpt2\"",
        "\"steps\"",
        "\"gpt2\"",
        "\"t5\"",
        "\"train_loss\"",
        "\"steps\"",
        "\"tokenizer\"",
        "\"prompt\"",
        "\"prompt\"",
        "\"completion\"",
        "\"completion\"",
        "\"label\"",
        "\"label\"",
        "\"prompt_input_ids\"",
        "\"prompt_attention_mask\"",
        "\"answer_input_ids\"",
        "\"answer_attention_mask\"",
        "\"prefix\"",
        "\"\"",
        "\"is_encoder_decoder\"",
        "\"tokenizer\"",
        "\"max_length\"",
        "\"truncation_mode\"",
        "\"label_pad_token_id\"",
        "\"max_prompt_length\"",
        "\"prompt\"",
        "\"prompt\"",
        "\"completion\"",
        "\"completion\"",
        "\"label\"",
        "\"label\"",
        "\"prompt_input_ids\"",
        "\"prompt_attention_mask\"",
        "\"completion_input_ids\"",
        "\"completion_attention_mask\"",
        "\"completion_labels\"",
        "\"steps\"",
        "\"train_loss\"",
        "\"steps\"",
        "\"train_loss\"",
        "\"none\"",
        "\"CAUSAL_LM\"",
        "\"steps\"",
        "\"train_loss\"",
        "\"lora\"",
        "\"steps\"",
        "\"`generate_during_eval=True` requires Weights and Biases to be installed.\"",
        "\" Please install with `pip install wandb` to resolve.\"",
        "\"none\"",
        "\"CAUSAL_LM\"",
        "\"steps\"",
        "\"Loading the saved peft adapter failed\""
    ],
    "variables": [
        "cls",
        "model_id",
        "cls",
        "model",
        "cls",
        "ref_model",
        "cls",
        "tokenizer",
        "pad_token",
        "model_id",
        "cls",
        "t5_model",
        "cls",
        "t5_ref_model",
        "cls",
        "t5_tokenizer",
        "model_id",
        "cls",
        "embedding_model",
        "cls",
        "embedding_tokenizer",
        "dummy_dataset_dict",
        "training_args",
        "dummy_dataset",
        "model",
        "ref_model",
        "tokenizer",
        "model",
        "ref_model",
        "tokenizer",
        "trainer",
        "previous_trainable_params",
        "new_param",
        "training_args",
        "dummy_dataset",
        "trainer",
        "tokenized_dataset",
        "fn_kwargs",
        "processed_dataset",
        "training_args",
        "dummy_dataset",
        "trainer",
        "previous_trainable_params",
        "new_param",
        "training_args",
        "dummy_dataset",
        "outputs",
        "embedding_model",
        "embedding_func",
        "trainer",
        "previous_trainable_params",
        "new_param",
        "lora_config",
        "training_args",
        "dummy_dataset",
        "trainer",
        "previous_trainable_params",
        "new_param",
        "training_args",
        "dummy_dataset",
        "lora_config",
        "model",
        "model_peft",
        "training_args",
        "dummy_dataset",
        "trainer"
    ],
    "comments": [
        "Copyright 2024 The HuggingFace Team. All rights reserved.",
        "",
        "Licensed under the Apache License, Version 2.0 (the \"License\");",
        "you may not use this file except in compliance with the License.",
        "You may obtain a copy of the License at",
        "",
        "http://www.apache.org/licenses/LICENSE-2.0",
        "",
        "Unless required by applicable law or agreed to in writing, software",
        "distributed under the License is distributed on an \"AS IS\" BASIS,",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
        "See the License for the specific language governing permissions and",
        "limitations under the License.",
        "get t5 as seq2seq example:",
        "get embedding model",
        "fmt: off",
        "fmt: on",
        "check the params have changed",
        "check the params have changed - ignore 0 biases",
        "check the params have changed",
        "check the params have changed - ignore 0 biases",
        "check the params have changed",
        "check the params have changed - ignore 0 biases",
        "check the params have changed",
        "check the params have changed - ignore 0 biases",
        "lora model",
        "bco train lora model with a lora config",
        "train the model",
        "save peft adapter",
        "assert that the model is loaded without giving OSError"
    ],
    "docstrings": [],
    "functions": [
        "setUpClass",
        "_init_dummy_dataset",
        "test_bco_trainer",
        "test_tokenize_and_process_tokens",
        "test_bco_trainer_without_providing_ref_model",
        "test_bco_trainer_udm",
        "embed_prompt",
        "test_bco_trainer_without_providing_ref_model_with_lora",
        "test_bco_trainer_generate_during_eval_no_wandb",
        "test_bco_lora_save"
    ],
    "classes": [
        "BCOTrainerTester"
    ]
}