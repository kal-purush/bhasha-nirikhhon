{
    "identifiers": [
        "transformers",
        "configuration_utils",
        "PretrainedConfig",
        "transformers",
        "utils",
        "logging",
        "logging",
        "get_logger",
        "PretrainedConfig",
        "vocab_size",
        "hidden_size",
        "num_layers",
        "num_attention_heads",
        "layernorm_epsilon",
        "use_cache",
        "bos_token_id",
        "eos_token_id",
        "pad_token_id",
        "max_sequence_length",
        "inner_hidden_size",
        "position_encoding_2d",
        "kwargs",
        "num_layers",
        "vocab_size",
        "hidden_size",
        "num_attention_heads",
        "max_sequence_length",
        "layernorm_epsilon",
        "inner_hidden_size",
        "use_cache",
        "bos_token_id",
        "eos_token_id",
        "pad_token_id",
        "position_encoding_2d",
        "pad_token_id",
        "pad_token_id",
        "bos_token_id",
        "bos_token_id",
        "eos_token_id",
        "eos_token_id",
        "kwargs"
    ],
    "literals": [
        "r\"\"\"\n    This is the configuration class to store the configuration of a [`~ChatGLMModel`].\n    It is used to instantiate an ChatGLM model according to the specified arguments, defining the model\n    architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n    the ChatGLM-6B [THUDM/ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b) architecture.\n\n    Configuration objects inherit from  [`PretrainedConfig`] and can be used\n    to control the model outputs. Read the documentation from  [`PretrainedConfig`]\n    for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 150528):\n            Vocabulary size of the ChatGLM-6B model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`~ChatGLMModel`] or\n            [`~TFChatGLMModel`].\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 28):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        inner_hidden_size (`int`, *optional*, defaults to 16384):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n        max_sequence_length (`int`, *optional*, defaults to 512):\n            The maximum sequence length that this model might ever be used with.\n            Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n        layernorm_epsilon (`float`, *optional*, defaults to 1e-5):\n            The epsilon used by the layer normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether the model should return the last key/values attentions (not used by all models).\n        Example:\n\n    ```python\n    >>> from configuration_chatglm import ChatGLMConfig\n    >>> from modeling_chatglm import ChatGLMModel\n\n    >>> # Initializing a ChatGLM-6B THUDM/ChatGLM-6B style configuration\n    >>> configuration = ChatGLMConfig()\n\n    >>> # Initializing a model from the THUDM/ChatGLM-6B style configuration\n    >>> model = ChatGLMModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\n\"\"\"",
        "\"chatglm\""
    ],
    "variables": [
        "logger",
        "model_type",
        "num_layers",
        "vocab_size",
        "hidden_size",
        "num_attention_heads",
        "max_sequence_length",
        "layernorm_epsilon",
        "inner_hidden_size",
        "use_cache",
        "bos_token_id",
        "eos_token_id",
        "pad_token_id",
        "position_encoding_2d"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\" ChatGLM model configuration \"\"\""
    ],
    "functions": [],
    "classes": [
        "ChatGLMConfig"
    ]
}