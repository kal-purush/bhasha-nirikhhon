{
    "identifiers": [
        "voynich",
        "VoynichManuscript",
        "torch",
        "torch",
        "nn",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "TensorDataset",
        "RandomSampler",
        "SequentialSampler",
        "pytorch_pretrained_bert",
        "BertTokenizer",
        "BertForSequenceClassification",
        "BertAdam",
        "keras",
        "preprocessing",
        "sequence",
        "pad_sequences",
        "random",
        "numpy",
        "np",
        "numbers",
        "random",
        "sample",
        "numbers",
        "sorted",
        "pair",
        "pair",
        "seen",
        "seen",
        "add",
        "pair",
        "pair",
        "inputs_filename",
        "masks_filename",
        "labels_filename",
        "VoynichManuscript",
        "inline_comments",
        "page",
        "vm",
        "pages",
        "vm",
        "pages",
        "page",
        "section",
        "labelNums",
        "labelNums",
        "vm",
        "pages",
        "page",
        "section",
        "labelCount",
        "vm",
        "pages",
        "page",
        "section",
        "labelCount",
        "labelCount",
        "line",
        "vm",
        "pages",
        "page",
        "lines",
        "append",
        "line",
        "text",
        "replace",
        "labels",
        "append",
        "section_label",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "BertTokenizer",
        "from_pretrained",
        "do_lower_case",
        "tokenizer",
        "tokenize",
        "line",
        "line",
        "lines",
        "tokenizer",
        "convert_tokens_to_ids",
        "x",
        "x",
        "tokenized_texts",
        "pad_sequences",
        "input_ids",
        "maxlen",
        "MAX_LEN",
        "dtype",
        "truncating",
        "padding",
        "seq",
        "input_ids",
        "i",
        "i",
        "seq",
        "attention_masks",
        "append",
        "seq_mask",
        "len",
        "input_ids",
        "random_pair_generator",
        "input_indices",
        "n",
        "NUM_TEST_PAIRS",
        "next",
        "pair_gen",
        "pair",
        "pair",
        "test_inputs",
        "append",
        "input_ids",
        "i",
        "input_ids",
        "j",
        "test_masks",
        "append",
        "attention_masks",
        "i",
        "attention_masks",
        "j",
        "labels",
        "i",
        "labels",
        "j",
        "test_labels",
        "append",
        "test_label",
        "torch",
        "tensor",
        "test_inputs",
        "torch",
        "tensor",
        "test_masks",
        "torch",
        "tensor",
        "test_labels",
        "torch",
        "save",
        "test_inputs",
        "inputs_filename",
        "torch",
        "save",
        "test_masks",
        "masks_filename",
        "torch",
        "save",
        "test_labels",
        "labels_filename",
        "inputs_filename",
        "masks_filename",
        "labels_filename",
        "torch",
        "load",
        "inputs_filename",
        "torch",
        "load",
        "masks_filename",
        "torch",
        "load",
        "labels_filename",
        "TensorDataset",
        "test_inputs",
        "test_masks",
        "test_labels",
        "test_data",
        "generate_test_data"
    ],
    "literals": [
        "\"voynich-text.txt\"",
        "\"[CLS]\"",
        "\".\"",
        "\" \"",
        "\"[SEP]\"",
        "\"cuda\"",
        "\"cpu\"",
        "'bert-base-uncased'",
        "\"long\"",
        "\"post\"",
        "\"post\"",
        "\"__main__\"",
        "\"test_inputs.pt\"",
        "\"test_masks.pt\"",
        "\"test_labels.pt\""
    ],
    "variables": [
        "seen",
        "pair",
        "pair",
        "lines",
        "labelNums",
        "labelCount",
        "labels",
        "vm",
        "section_label",
        "section_label",
        "labelNums",
        "device",
        "tokenizer",
        "tokenized_texts",
        "MAX_LEN",
        "input_ids",
        "input_ids",
        "attention_masks",
        "seq_mask",
        "NUM_TEST_PAIRS",
        "input_indices",
        "pair_gen",
        "test_inputs",
        "test_masks",
        "test_labels",
        "pair",
        "i",
        "j",
        "test_label",
        "test_inputs",
        "test_masks",
        "test_labels",
        "test_inputs",
        "test_masks",
        "test_labels",
        "test_data"
    ],
    "comments": [
        "generate pairs of inputs for test data",
        "three files are generated, each containing a torch tensor",
        "inputs file: contains pair of input tokens",
        "masks file: contains pair of attention masks",
        "labels file: contains labels for each pair",
        "",
        "for the inputs and mask files:",
        "the pair index is along axis 0",
        "axis 1 is of length 2, with each element representing a different input",
        "",
        "for the labels file:",
        "the pair index is along axis 0",
        "axis 1 is of length 1, the one element represents the label for the pair",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],",
        "maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")",
        "generate text comparison pairs"
    ],
    "docstrings": [],
    "functions": [
        "random_pair_generator",
        "generate_test_data",
        "load_test_data"
    ],
    "classes": []
}