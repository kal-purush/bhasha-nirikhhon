{
    "identifiers": [
        "org",
        "teiid",
        "sizing",
        "source_count",
        "queries_concurrent",
        "queries_per_sec",
        "row_count_each",
        "row_size_each",
        "avg_time_each",
        "row_count_federated",
        "row_size_federated",
        "avg_time_sample",
        "isAggregation",
        "sources",
        "getSource_count",
        "concurrent",
        "getQueries_concurrent",
        "total_in_mb",
        "concurrent",
        "sources",
        "heap",
        "total_in_mb",
        "heap",
        "sources",
        "getSource_count",
        "row_count_each",
        "getRow_count_each",
        "row_size_each",
        "getRow_size_each",
        "source_latency",
        "getAvg_time_each",
        "row_count_federdated",
        "getRow_count_federated",
        "row_size_federdated",
        "getRow_size_federated",
        "walltime",
        "getAvg_time_sample",
        "isAggregation",
        "isAggregation",
        "queries_per_sec",
        "getQueries_per_sec",
        "source_processing",
        "getSourceProcessingTime",
        "row_count_each",
        "row_size_each",
        "sources",
        "initial_latency",
        "getInitialLatency",
        "row_count_each",
        "row_size_each",
        "sources",
        "source_latency",
        "additional_latency",
        "getAdditionalLatency",
        "sources",
        "source_latency",
        "client_processing",
        "getClientProcessing",
        "row_count_federdated",
        "row_size_federdated",
        "engine_time",
        "getEngineTime",
        "isAggregation",
        "sources",
        "row_count_each",
        "row_size_each",
        "row_size_federdated",
        "row_count_federdated",
        "walltime",
        "source_latency",
        "cores",
        "getcorenumbers",
        "source_latency",
        "sources",
        "source_processing",
        "initial_latency",
        "additional_latency",
        "engine_time",
        "client_processing",
        "queries_per_sec",
        "cores",
        "source_latency",
        "sources",
        "source_processing",
        "initial_latency",
        "additional_latency",
        "engine_time",
        "client_processing",
        "queries_per_sec",
        "cpu_time",
        "source_processing",
        "engine_time",
        "client_processing",
        "wall_time",
        "cpu_time",
        "initial_latency",
        "additional_latency",
        "cpu_utilization_per_query",
        "cpu_time",
        "wall_time",
        "threads_used_per_query",
        "sources",
        "cores",
        "cpu_time",
        "queries_per_sec",
        "cpu_utilization_per_query",
        "threads_used_per_query",
        "round",
        "cores",
        "isAggregation",
        "sources",
        "row_count_each",
        "row_size_each",
        "row_size_federdated",
        "row_count_federdated",
        "walltime",
        "source_latency",
        "serializing_time",
        "getSourceProcessingTime",
        "row_count_each",
        "row_size_each",
        "sources",
        "deserializing_time",
        "getClientProcessing",
        "row_count_federdated",
        "row_size_federdated",
        "initial_latency",
        "getInitialLatency",
        "row_count_each",
        "row_size_each",
        "sources",
        "source_latency",
        "additional_latency",
        "getAdditionalLatency",
        "sources",
        "source_latency",
        "engine_time",
        "engine_time_rough",
        "walltime",
        "serializing_time",
        "deserializing_time",
        "initial_latency",
        "additional_latency",
        "total_fer_size",
        "row_size_federdated",
        "row_count_federdated",
        "engine_time_rough",
        "engine_time",
        "isAggregation",
        "total_fer_size",
        "engine_time",
        "engine_time_rough",
        "isAggregation",
        "engine_time",
        "engine_time_rough",
        "engine_time",
        "engine_time_rough",
        "round",
        "engine_time",
        "row_count_federdated",
        "row_size_federdated",
        "total_byte",
        "row_count_federdated",
        "row_size_federdated",
        "client_procesing",
        "total_byte",
        "size_in_mb",
        "total_byte",
        "client_procesing",
        "size_in_mb",
        "total_byte",
        "total_byte",
        "percentage",
        "client_procesing",
        "percentage",
        "total_byte",
        "total_byte",
        "total_byte",
        "percentage",
        "client_procesing",
        "percentage",
        "total_byte",
        "total_byte",
        "total_byte",
        "percentage",
        "client_procesing",
        "percentage",
        "total_byte",
        "total_byte",
        "total_byte",
        "percentage",
        "client_procesing",
        "percentage",
        "total_byte",
        "total_byte",
        "total_byte",
        "percentage",
        "client_procesing",
        "percentage",
        "total_byte",
        "total_byte",
        "client_procesing",
        "round",
        "client_procesing",
        "sources",
        "source_latency",
        "additional_latency",
        "sources",
        "additional_latency",
        "source_latency",
        "additional_latency",
        "source_latency",
        "round",
        "additional_latency",
        "row_count_each",
        "row_size_each",
        "sources",
        "source_latency",
        "total_byte",
        "row_count_each",
        "row_size_each",
        "initial_latency",
        "sources",
        "initial_latency",
        "source_latency",
        "total_byte",
        "initial_latency",
        "source_latency",
        "initial_latency",
        "source_latency",
        "round",
        "initial_latency",
        "row_count_each",
        "row_size_each",
        "sources",
        "total_byte",
        "row_count_each",
        "row_size_each",
        "sources",
        "source_processing",
        "total_byte",
        "size_in_mb",
        "total_byte",
        "source_processing",
        "size_in_mb",
        "total_byte",
        "total_byte",
        "percentage",
        "source_processing",
        "percentage",
        "total_byte",
        "total_byte",
        "total_byte",
        "percentage",
        "source_processing",
        "percentage",
        "total_byte",
        "source_processing",
        "round",
        "source_processing",
        "source_count",
        "queries_concurrent",
        "source_count",
        "source_count",
        "queries_concurrent",
        "queries_concurrent",
        "source_count",
        "queries_concurrent",
        "queries_per_sec",
        "row_count_each",
        "row_size_each",
        "avg_time_each",
        "row_count_federated",
        "row_size_federated",
        "avg_time_sample",
        "isAggregation",
        "source_count",
        "source_count",
        "queries_concurrent",
        "queries_concurrent",
        "queries_per_sec",
        "queries_per_sec",
        "row_count_each",
        "row_count_each",
        "row_size_each",
        "row_size_each",
        "avg_time_each",
        "avg_time_each",
        "row_count_federated",
        "row_count_federated",
        "row_size_federated",
        "row_size_federated",
        "avg_time_sample",
        "avg_time_sample",
        "isAggregation",
        "isAggregation",
        "source_count",
        "source_count",
        "source_count",
        "source_count",
        "queries_concurrent",
        "queries_concurrent",
        "queries_concurrent",
        "queries_concurrent",
        "queries_per_sec",
        "queries_per_sec",
        "queries_per_sec",
        "queries_per_sec",
        "row_count_each",
        "row_count_each",
        "row_count_each",
        "row_count_each",
        "row_size_each",
        "row_size_each",
        "row_size_each",
        "row_size_each",
        "avg_time_each",
        "avg_time_each",
        "avg_time_each",
        "avg_time_each",
        "row_count_federated",
        "row_count_federated",
        "row_count_federated",
        "row_count_federated",
        "row_size_federated",
        "row_size_federated",
        "row_size_federated",
        "row_size_federated",
        "avg_time_sample",
        "avg_time_sample",
        "avg_time_sample",
        "avg_time_sample",
        "isAggregation",
        "isAggregation",
        "isAggregation",
        "isAggregation"
    ],
    "literals": [],
    "variables": [
        "source_count",
        "queries_concurrent",
        "queries_per_sec",
        "row_count_each",
        "row_size_each",
        "avg_time_each",
        "row_count_federated",
        "row_size_federated",
        "avg_time_sample",
        "isAggregation"
    ],
    "comments": [
        "1. How many data sources do you want to integrate?",
        "2. How many concurrent queries (at peak load time) do you want to support?",
        "1. How many client queries per second do you want to support?",
        "2. What is the average row count from each physical source?",
        "3. What is the average row size (in bytes) that returns from each source?",
        "4. What is the average time (in milliseconds) required by each source to return a result?",
        "5. What is the expected average row size (in bytes) received as the result of a federated query?",
        "6. What is the average expected row count received as the result of a federated query?",
        "7. What is the average time (in milliseconds) required to execute a client query in sample runs?",
        "8. Does the client query perform aggregations, sorts, or view transformations that perform sorts and aggregations?",
        "if(heap < 16) {",
        "heap = 16 ;",
        "}",
        "if (cores < 16) {",
        "cores = 16;",
        "}",
        "",
        "if(cores > 128) {",
        "cores = 128 ;",
        "}",
        "sampleruntime should large than latencies + serializing_time + deserializing_time"
    ],
    "docstrings": [
        "* JVM Size Caculation Formula: Size = #concurrency * (5mb)  * #source queries + 300mb\n     * \n     * @return heap size in GB",
        "* \n     * @return",
        "* CPU calculation logic & Formula:\n     *   cpu_time = sum(source_processing) + engine_time + client_processing\n     *   wall_time = low(source_latency) + cpu_time + additional_latency\n     *   cpu_utilization_per_query = cpu_time/wall_time\n     *   total_cpu_time_available = cpu_core_count * 2 * 1000ms\n     *   queries/sec = total_cpu_time_available / (threads_used_per_query * cpu_utilization_per_query * cpu_time)\n     *",
        "* If there are lot of sorting, aggregations this can be high, if not can be very low as in pass through scenarios.\n     * \n     * \"how much time they took in their sample runs\" will get a time for running a sample query, then remove all source and deserialization/Serialization latencies then we roughly have the engine time\n     * \n     * based on that time, and sorting and aggregation, we can say low, medium or high processing (< 25%, 60%, > 90%) of times",
        "* How much time took for serializing the results and put on the socket.\n     *\n     *  The Source Serialize Processing Time(time) and Total Serialize Processing Size(size) are in a Linear Regression trend,\n     *      time = K * time + V\n    *  In previous test, \n     *    if size > 1 MB, the K is 4.6, the V is 210, the formula like: time = 4.6 * size + 210\n     *    if size < 1 MB, it's more complex, not get a precise value of K and V, 125, 75, 5 and 2 in below are come from test\n     *    \n     *  TODO: need more trial, collect more tuples (time, size), use algorithm to get the K and V\n     *        TEIID-3398",
        "* Even after the first row of results came(lowest of source_latency), how much more *additional* time spent on waiting for results. Consider a guess of half (0.5) when we parallelize,\n     * \n     * in serialized situations (XA) this will be 1. So, typically this should be 0.5(high(source_latency) - low(source_latency)) or in XA it should be 1 * sum(source_latency).",
        "* this also variation \"source latency\", as to first source to return results, where processing starts. We can say this is \"low(source_latency)\".\n     * \n     * 'source_latency' is average source latency for each data source, so the formula used to estimate source latency like below method",
        "* How much time took to deserialize rows coming back from source?\n     * \n     *  The Source Deserialize Processing Time(time) and Total Deserialize Processing Size(size) are in a Linear Regression trend,\n     *      time = K * time + V\n     *  \n     *  In previous test, \n     *    if size > 1 MB, the K is 4.6, the V is 100, the formula like: time = 4.6 * size + 100\n     *    if size < 1 MB, it's more complex, not get a precise value of K and V, 80 and 18 in below are come from test\n     *    \n     *  TODO: need more trial, collect more tuples (time, size), use algorithm to get the K and V\n     *        TEIID-3398\n     *"
    ],
    "functions": [
        "heapCaculation",
        "coreCaculation",
        "getcorenumbers",
        "getEngineTime",
        "getClientProcessing",
        "getAdditionalLatency",
        "getInitialLatency",
        "getSourceProcessingTime",
        "Caculation",
        "Caculation",
        "getSource_count",
        "setSource_count",
        "getQueries_concurrent",
        "setQueries_concurrent",
        "getQueries_per_sec",
        "setQueries_per_sec",
        "getRow_count_each",
        "setRow_count_each",
        "getRow_size_each",
        "setRow_size_each",
        "getAvg_time_each",
        "setAvg_time_each",
        "getRow_count_federated",
        "setRow_count_federated",
        "getRow_size_federated",
        "setRow_size_federated",
        "getAvg_time_sample",
        "setAvg_time_sample",
        "isAggregation",
        "setAggregation"
    ],
    "classes": [
        "Caculation"
    ]
}