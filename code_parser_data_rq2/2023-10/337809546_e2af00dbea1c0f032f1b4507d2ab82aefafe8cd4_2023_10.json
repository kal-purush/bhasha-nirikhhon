{
    "identifiers": [
        "absl",
        "app",
        "absl",
        "flags",
        "os",
        "shutil",
        "pandas",
        "pd",
        "pyspark",
        "sql",
        "SparkSession",
        "pyspark",
        "apache_beam",
        "beam",
        "apache_beam",
        "runners",
        "portability",
        "fn_api_runner",
        "apache_beam",
        "dataframe",
        "io",
        "read_csv",
        "pipeline_dp",
        "pipeline_dp",
        "dataframes",
        "typing",
        "Union",
        "flags",
        "FLAGS",
        "flags",
        "DEFINE_string",
        "flags",
        "DEFINE_string",
        "flags",
        "DEFINE_enum",
        "beam",
        "dataframe",
        "frames",
        "DeferredDataFrame",
        "pyspark",
        "sql",
        "dataframe",
        "DataFrame",
        "filename",
        "os",
        "path",
        "exists",
        "filename",
        "os",
        "path",
        "isdir",
        "filename",
        "shutil",
        "rmtree",
        "filename",
        "os",
        "remove",
        "filename",
        "spark",
        "SparkSession",
        "pyspark",
        "sql",
        "dataframe",
        "DataFrame",
        "spark",
        "read",
        "csv",
        "FLAGS",
        "input_file",
        "header",
        "inferSchema",
        "df",
        "withColumnRenamed",
        "withColumnRenamed",
        "withColumnRenamed",
        "withColumnRenamed",
        "withColumnRenamed",
        "df",
        "Union",
        "pd",
        "DataFrame",
        "BeamDataFrame",
        "SparkDataFrame",
        "Union",
        "pd",
        "DataFrame",
        "BeamDataFrame",
        "SparkDataFrame",
        "dataframes",
        "QueryBuilder",
        "df",
        "dp_query_builder",
        "groupby",
        "max_partitions_contributed",
        "max_contributions_per_partition",
        "count",
        "sum",
        "min_value",
        "max_value",
        "build_query",
        "query",
        "run_query",
        "dataframes",
        "Budget",
        "epsilon",
        "delta",
        "noise_kind",
        "pipeline_dp",
        "NoiseKind",
        "GAUSSIAN",
        "result_df",
        "SparkSession",
        "builder",
        "master",
        "appName",
        "getOrCreate",
        "load_data_in_spark_dataframe",
        "spark",
        "df",
        "printSchema",
        "compute_private_result",
        "df",
        "result_df",
        "printSchema",
        "delete_if_exists",
        "FLAGS",
        "output_file",
        "result_df",
        "write",
        "format",
        "option",
        "save",
        "FLAGS",
        "output_file",
        "unused_argv",
        "FLAGS",
        "dataframes",
        "compute_on_spark_dataframes",
        "ValueError",
        "FLAGS",
        "dataframes",
        "flags",
        "mark_flag_as_required",
        "app",
        "run",
        "main"
    ],
    "literals": [
        "'input_file'",
        "'restaurants_week_data.csv'",
        "'The file with the restaraunt visits data'",
        "'output_file'",
        "'Output file. It will be overwritten '",
        "'if exists'",
        "'dataframes'",
        "'pandas'",
        "'pandas'",
        "'spark'",
        "'beam'",
        "'Which dataframes to use.'",
        "'VisitorId'",
        "'visitor_id'",
        "'Time entered'",
        "'enter_time'",
        "'Time spent (minutes)'",
        "'spent_minutes'",
        "'Money spent (euros)'",
        "'spent_money'",
        "'Day'",
        "'day'",
        "'visitor_id'",
        "'day'",
        "'spent_money'",
        "\"local[1]\"",
        "\"Restaurant\"",
        "\"csv\"",
        "\"header\"",
        "'spark'",
        "f\"{FLAGS.dataframes} dataframes are not supported.\"",
        "'__main__'",
        "\"output_file\""
    ],
    "variables": [
        "FLAGS",
        "BeamDataFrame",
        "SparkDataFrame",
        "df",
        "dp_query_builder",
        "query",
        "result_df",
        "spark",
        "df",
        "result_df"
    ],
    "comments": [
        "Copyright 2023 OpenMined.",
        "",
        "Licensed under the Apache License, Version 2.0 (the \"License\");",
        "you may not use this file except in compliance with the License.",
        "You may obtain a copy of the License at",
        "",
        "http://www.apache.org/licenses/LICENSE-2.0",
        "",
        "Unless required by applicable law or agreed to in writing, software",
        "distributed under the License is distributed on an \"AS IS\" BASIS,",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
        "See the License for the specific language governing permissions and",
        "limitations under the License.",
        "TODO: Extend this example to support Beam and Pandas DataFrames."
    ],
    "docstrings": [
        "\"\"\" Demo of running PipelineDP on (Pandas, Spark, Beam) DataFrames.\n\nIn order to run:\n1. Install Python and run on the command line `pip install pipeline-dp absl-py`\n2. Install Pandas/Spark/Beam depending on which DataFrame you would like to use\nby `pip install pandas` or `pip install pyspark` or `pip install apache_beam`\n3. Run python run_on_dataframes.py --input_file=<path to restaurants_week_data.csv> --output_file=<...> --dataframes=pandas<spark, beam>\n\"\"\""
    ],
    "functions": [
        "delete_if_exists",
        "load_data_in_spark_dataframe",
        "compute_private_result",
        "compute_on_spark_dataframes",
        "main"
    ],
    "classes": []
}