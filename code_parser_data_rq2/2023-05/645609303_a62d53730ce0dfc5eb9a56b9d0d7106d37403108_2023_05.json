{
    "identifiers": [],
    "literals": [
        "'sophia'",
        "'gpt2-medium-adam-100k'",
        "'adamw'",
        "'out_medium_adam_100k'"
    ],
    "variables": [
        "wandb_log",
        "wandb_project",
        "wandb_run_name",
        "batch_size",
        "block_size",
        "gradient_accumulation_steps",
        "n_layer",
        "n_head",
        "n_embd",
        "dropout",
        "bias",
        "scale_attn_by_inverse_layer_idx",
        "max_iters",
        "lr_decay_iters",
        "eval_interval",
        "eval_iters",
        "log_interval",
        "optimizer_name",
        "learning_rate",
        "weight_decay",
        "beta1",
        "beta2",
        "grad_clip",
        "decay_lr",
        "warmup_iters",
        "min_lr",
        "compile",
        "out_dir"
    ],
    "comments": [
        "these make the total batch size be ~0.5M",
        "6 batch size * 1024 block size * 10 gradaccum * 8 GPUs = 491,520",
        "for pretraining 0 is good, for finetuning try 0.1+",
        "this makes total number of tokens be 300B",
        "eval stuff",
        "optimizer",
        "max learning rate",
        "clip gradients at this value, or disable if == 0.0",
        "learning rate decay settings",
        "whether to decay the learning rate",
        "how many steps to warm up for"
    ],
    "docstrings": [],
    "functions": [],
    "classes": []
}