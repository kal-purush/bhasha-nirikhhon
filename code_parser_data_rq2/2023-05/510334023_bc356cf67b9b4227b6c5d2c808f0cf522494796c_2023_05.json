{
    "identifiers": [
        "pip",
        "install",
        "pandas",
        "pathlib",
        "azure",
        "storage",
        "file",
        "datalake",
        "numpy",
        "pyarrow",
        "great_expectations",
        "openpyxl",
        "os",
        "io",
        "tempfile",
        "datetime",
        "datetime",
        "json",
        "pandas",
        "pd",
        "numpy",
        "np",
        "pathlib",
        "Path",
        "azure",
        "storage",
        "filedatalake",
        "DataLakeServiceClient",
        "great_expectations",
        "ge",
        "dbutils",
        "secrets",
        "get",
        "scope",
        "key",
        "dbutils",
        "secrets",
        "get",
        "scope",
        "key",
        "datalake_download",
        "CONNECTION_STRING",
        "file_system_config",
        "file_path_config",
        "file_name_config",
        "json",
        "loads",
        "io",
        "BytesIO",
        "config_JSON",
        "read",
        "dbutils",
        "secrets",
        "get",
        "scope",
        "key",
        "config_JSON",
        "datalake_latestFolder",
        "CONNECTION_STRING",
        "file_system",
        "new_source_path",
        "datalake_listContents",
        "CONNECTION_STRING",
        "file_system",
        "new_source_path",
        "latestFolder",
        "file",
        "file",
        "file_name_list",
        "file",
        "new_source_file",
        "file_name_list",
        "datalake_download",
        "CONNECTION_STRING",
        "file_system",
        "new_source_path",
        "latestFolder",
        "new_source_file",
        "pd",
        "read_excel",
        "io",
        "BytesIO",
        "new_dataset",
        "sheet_name",
        "header",
        "engine",
        "new_dataframe",
        "mask",
        "new_dataframe",
        "ge",
        "from_pandas",
        "val_df",
        "df1",
        "expect_column_values_to_not_be_null",
        "column",
        "test_result",
        "expect",
        "info",
        "expect",
        "success",
        "new_source_path",
        "latestFolder",
        "new_source_file",
        "len",
        "new_dataframe",
        "pd",
        "to_datetime",
        "strftime",
        "datetime",
        "strptime",
        "today",
        "row_count",
        "date",
        "full_path",
        "pd",
        "DataFrame",
        "in_row",
        "write_to_sql",
        "df",
        "log_table"
    ],
    "literals": [
        "'AzureDataLake'",
        "\"DATALAKE_CONNECTION_STRING\"",
        "\"config/pipelines/nhsx-au-analytics\"",
        "\"config_ers_api.json\"",
        "\"dbo.pre_load_log\"",
        "\"AzureDataLake\"",
        "\"DATALAKE_CONTAINER_NAME\"",
        "\"AzureDataLake\"",
        "\"DATALAKE_CONTAINER_NAME\"",
        "\"pipeline\"",
        "'raw'",
        "\"source_path\"",
        "'.xlsx'",
        "'e_rs_api'",
        "'openpyxl'",
        "\" \"",
        "\"Checking that the Date column do not contain any null values\\n\"",
        "'Report_End _Date'",
        "'now'",
        "\"%Y-%m-%d %H:%M:%S\"",
        "'%Y-%m-%d %H:%M:%S'",
        "'row_count'",
        "'load_date'",
        "'file_to_load'",
        "\"append\""
    ],
    "variables": [
        "CONNECTION_STRING",
        "file_path_config",
        "file_name_config",
        "log_table",
        "file_system_config",
        "config_JSON",
        "config_JSON",
        "file_system",
        "new_source_path",
        "latestFolder",
        "file_name_list",
        "file_name_list",
        "new_dataset",
        "new_dataframe",
        "val_df",
        "df1",
        "info",
        "expect",
        "full_path",
        "row_count",
        "today",
        "date",
        "in_row",
        "df"
    ],
    "comments": [
        "Databricks notebook source",
        "!/usr/bin python3",
        "-------------------------------------------------------------------------",
        "Copyright (c) 2021 NHS England and NHS Improvement. All rights reserved.",
        "Licensed under the MIT License. See license.txt in the project root for",
        "license information.",
        "-------------------------------------------------------------------------",
        "COMMAND ----------",
        "Install libs",
        "-------------------------------------------------------------------------------------",
        "COMMAND ----------",
        "Imports",
        "-------------------------------------------------------------------------",
        "Python:",
        "3rd party:",
        "Connect to Azure datalake",
        "-------------------------------------------------------------------------",
        "!env from databricks secrets",
        "COMMAND ----------",
        "MAGIC %run /Shared/databricks/au-azure-databricks-cicd/functions/dbrks_helper_functions",
        "COMMAND ----------",
        "Load parameters and JSON config from Azure datalake",
        "-------------------------------------------------------------------------",
        "COMMAND ----------",
        "Read parameters from JSON config",
        "-------------------------------------------------------------------------",
        "COMMAND ----------",
        "Pull new dataset",
        "-------------------------",
        "COMMAND ----------",
        "validate data",
        "Greate expectations https://www.architecture-performance.fr/ap_blog/built-in-expectations-in-great-expectations/",
        "----------------------------------",
        "convert all blanks to NaN for validtion",
        "Create great expectations dataframe from pandas datafarme",
        "COMMAND ----------",
        "MAGIC %md",
        "MAGIC ## Tests Begin",
        "COMMAND ----------",
        "Test that the Date column do not contain any null values",
        "COMMAND ----------",
        "ServiceUserCount contains NULL value, need to find a way we can verify this",
        "#Test that the count only contains ints",
        "types = {",
        "\"ServiceUserCount\": \"int\"",
        "}",
        "info = \"Checking that 'Count' column data are all int\\n\"",
        "for column, type_ in types.items():",
        "expect = df1.expect_column_values_to_be_of_type(column=column, type_=type_)",
        "test_result(expect, info)",
        "assert expect.success",
        "COMMAND ----------",
        "MAGIC %md",
        "MAGIC ## Tests End",
        "COMMAND ----------",
        "Count rows in file and write to log table",
        "___________________________________________"
    ],
    "docstrings": [
        "\"\"\"\nFILE:           dbrks_e_rs_api_file_validation.py\nDESCRIPTION:\n                To reduce manual checks and improve quality validations need to added to dbrks_e_rs_api_file_validation pipeline for DCT \nUSAGE:\n                ...\nCONTRIBUTORS:   Everistus Oputa\nCONTACT:        nhsx.data@england.nhs.uk\nCREATED:        18 May 2023\nVERSION:        0.0.1\n\"\"\""
    ],
    "functions": [],
    "classes": []
}