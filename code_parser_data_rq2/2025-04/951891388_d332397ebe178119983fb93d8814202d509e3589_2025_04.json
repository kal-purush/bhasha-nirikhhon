{
    "identifiers": [
        "numpy",
        "np",
        "core",
        "args",
        "time",
        "core",
        "baseModule",
        "PRL",
        "VRL",
        "PRL",
        "env",
        "args",
        "kwargs",
        "env",
        "args",
        "kwargs",
        "time",
        "time",
        "epoch",
        "max_epochs",
        "timestep",
        "max_timesteps",
        "env",
        "reset",
        "act",
        "cur_s",
        "env",
        "step",
        "a",
        "squeeze",
        "terminated",
        "truncated",
        "cur_s",
        "a",
        "reward",
        "next_s",
        "done",
        "buffer",
        "add",
        "trainsition",
        "next_s",
        "rewards",
        "reward",
        "buffer",
        "is_ready",
        "done",
        "done",
        "timestep",
        "timestep",
        "_update",
        "monitor",
        "timestep_report",
        "timestep",
        "timestep",
        "max_timesteps",
        "early_stop",
        "reach_maxTimestep",
        "done",
        "monitor",
        "episode_evaluate",
        "epoch_record",
        "append",
        "rewards",
        "epoch",
        "early_stop",
        "reach_maxTimestep",
        "time",
        "time",
        "training_time",
        "end",
        "start",
        "VRL",
        "env",
        "args",
        "kwargs",
        "env",
        "args",
        "kwargs",
        "time",
        "time",
        "epoch",
        "max_epochs",
        "timestep",
        "max_timesteps",
        "env",
        "reset",
        "act",
        "s",
        "env",
        "step",
        "a",
        "terminated",
        "truncated",
        "memory",
        "add",
        "s",
        "a",
        "reward",
        "next_s",
        "terminated",
        "rewards",
        "reward",
        "next_s",
        "_update",
        "hasattr",
        "timestep",
        "sync_freq",
        "alg_name",
        "target_net",
        "load_state_dict",
        "policy_net",
        "state_dict",
        "monitor",
        "timestep_report",
        "report_dict",
        "timestep",
        "timestep",
        "max_timesteps",
        "early_stop",
        "reach_maxTimestep",
        "done",
        "monitor",
        "episode_evaluate",
        "epoch_record",
        "append",
        "rewards",
        "epoch",
        "alg_name",
        "noise",
        "policy_net",
        "reset_noise",
        "target_net",
        "reset_noise",
        "early_stop",
        "reach_maxTimestep",
        "time",
        "time",
        "training_time",
        "end",
        "start"
    ],
    "literals": [
        "\"sync_freq\"",
        "\"DQN\"",
        "\"DQN\""
    ],
    "variables": [
        "start",
        "reach_maxTimestep",
        "cur_s",
        "rewards",
        "a",
        "next_s",
        "reward",
        "terminated",
        "truncated",
        "info",
        "done",
        "trainsition",
        "cur_s",
        "early_stop",
        "reach_maxTimestep",
        "end",
        "start",
        "reach_maxTimestep",
        "rewards",
        "s",
        "a",
        "next_s",
        "reward",
        "terminated",
        "truncated",
        "info",
        "done",
        "s",
        "report_dict",
        "early_stop",
        "reach_maxTimestep",
        "end"
    ],
    "comments": [],
    "docstrings": [],
    "functions": [
        "train",
        "train"
    ],
    "classes": [
        "OnPolicy",
        "OffPolicy"
    ]
}