{
    "identifiers": [
        "nltk",
        "open",
        "infile",
        "infile",
        "read",
        "nltk",
        "sent_tokenize",
        "content",
        "sentences",
        "open",
        "infile",
        "infile",
        "read",
        "nltk",
        "word_tokenize",
        "content",
        "tokens",
        "open",
        "infile",
        "infile",
        "read",
        "nltk",
        "word_tokenize",
        "content",
        "token",
        "tokens",
        "token",
        "endswith",
        "ed_words",
        "append",
        "token",
        "ed_words",
        "nltk",
        "sent_tokenize",
        "nltk",
        "word_tokenize",
        "sentence",
        "sentence",
        "content",
        "content",
        "open",
        "infile",
        "infile",
        "read",
        "segment_and_tokenize",
        "maketrans",
        "punctuation",
        "segment_and_tokenize",
        "w",
        "translate",
        "translator",
        "w",
        "l",
        "l",
        "tokens",
        "open",
        "infile",
        "infile",
        "read",
        "remove_punctuation",
        "remove_punctuation",
        "w",
        "lower",
        "w",
        "l",
        "l",
        "tokens",
        "open",
        "infile",
        "infile",
        "read",
        "lower_case",
        "nltk",
        "nltk",
        "download",
        "nltk",
        "corpus",
        "stopwords",
        "tokens",
        "lower_case",
        "w",
        "w",
        "l",
        "w",
        "stopwords",
        "words",
        "l",
        "tokens",
        "open",
        "infile",
        "infile",
        "read",
        "remove_stop_words",
        "nltk",
        "help",
        "nltk",
        "pos_tag",
        "text",
        "nltk",
        "download",
        "nltk",
        "word_tokenize",
        "content",
        "nltk",
        "pos_tag",
        "tokens",
        "token",
        "tag",
        "tagged_tokens",
        "tag",
        "verb_tags",
        "token",
        "endswith",
        "verbs",
        "append",
        "token",
        "verbs",
        "open",
        "infile",
        "infile",
        "read",
        "get_ed_verbs",
        "content",
        "ed_verbs",
        "spacy",
        "pandas",
        "pd",
        "spacy",
        "load",
        "nlp",
        "sentence",
        "word",
        "word",
        "tag_",
        "word",
        "pos_",
        "word",
        "nlp_sentence",
        "pd",
        "DataFrame",
        "spacy_pos_tagged",
        "columns",
        "nltk",
        "nltk",
        "download",
        "nltk",
        "word_tokenize",
        "sentence",
        "nltk",
        "pos_tag",
        "tokens",
        "token",
        "tag",
        "tagged_tokens",
        "tag",
        "verb_tags",
        "verbs",
        "append",
        "token",
        "nltk",
        "stem",
        "wordnet",
        "WordNetLemmatizer",
        "word_form",
        "verbs",
        "lemmatizer",
        "lemmatize",
        "word_form",
        "verb_lemmas",
        "append",
        "word_form",
        "lemma",
        "f",
        "l",
        "verb_lemmas",
        "f",
        "l",
        "nltk",
        "corpus",
        "wordnet",
        "wn",
        "penn_tag",
        "penn_tag",
        "wn",
        "NOUN",
        "penn_tag",
        "wn",
        "VERB",
        "penn_tag",
        "wn",
        "ADV",
        "penn_tag",
        "wn",
        "ADJ",
        "wn_tag",
        "nltk",
        "stem",
        "wordnet",
        "WordNetLemmatizer",
        "token",
        "pos",
        "tagged_tokens",
        "penn_to_wn",
        "pos",
        "wn_tag",
        "lmtzr",
        "lemmatize",
        "token",
        "wn_tag",
        "lmtzr",
        "lemmatize",
        "token",
        "lemmas",
        "append",
        "lemma",
        "lemmas",
        "tagged_tokens",
        "nltk",
        "stem",
        "wordnet",
        "WordNetLemmatizer",
        "token",
        "pos",
        "tagged_tokens",
        "penn_to_wn",
        "pos",
        "wn_tag",
        "lmtzr",
        "lemmatize",
        "token",
        "wn_tag",
        "lmtzr",
        "lemmatize",
        "token",
        "token",
        "lemma",
        "pos",
        "lemmas",
        "append",
        "tlp",
        "lemmas",
        "lemmatize_postagged_tokens",
        "tagged_tokens",
        "spacy",
        "spacy",
        "load",
        "disable",
        "nlp",
        "sentence",
        "join",
        "token",
        "lemma_",
        "token",
        "doc",
        "sys",
        "os",
        "os",
        "environ",
        "java_path",
        "nltk",
        "parse",
        "stanford",
        "StanfordParser",
        "StanfordParser",
        "path_to_jar",
        "path_to_models_jar",
        "open",
        "encoding",
        "errors",
        "infile",
        "infile",
        "read",
        "nltk",
        "sent_tokenize",
        "content",
        "sentence",
        "sentences",
        "counter",
        "sentence",
        "counter",
        "scp",
        "raw_parse",
        "sentence",
        "parse_trees",
        "s",
        "tree",
        "subtrees",
        "tree",
        "tree",
        "label",
        "s",
        "leaves",
        "nltk",
        "tag",
        "StanfordNERTagger",
        "os",
        "pandas",
        "pd",
        "os",
        "environ",
        "java_path",
        "StanfordNERTagger",
        "path_to_jar",
        "open",
        "encoding",
        "errors",
        "infile",
        "infile",
        "read",
        "nltk",
        "sent_tokenize",
        "content",
        "sentence",
        "sentences",
        "counter",
        "sner",
        "tag",
        "sentence",
        "split",
        "ne",
        "ne",
        "ner_tagged_sentence",
        "ne",
        "named_entities",
        "sentence_named_entities",
        "named_entities",
        "nltk",
        "tag",
        "StanfordNERTagger",
        "spacy",
        "spacy",
        "displacy",
        "pprint",
        "pprint",
        "spacy",
        "load",
        "entity",
        "nlp",
        "displacy",
        "render",
        "nlp_sentence",
        "style",
        "jupyter"
    ],
    "literals": [
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"ed\"",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "''",
        "''",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "'stopwords'",
        "'english'",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "'averaged_perceptron_tagger'",
        "\"VBD\"",
        "\"VBG\"",
        "\"VBN\"",
        "\"VBP\"",
        "\"VBZ\"",
        "'ed'",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "'en'",
        "\"Amélie is a story about a girl named Amélie whose childhood was suppressed by her Father's mistaken concerns of a heart defect.\"",
        "'Word'",
        "'POS tag'",
        "'Tag type'",
        "'wordnet'",
        "\"Amélie is a story about a girl named Amélie whose childhood was suppressed by her Father's mistaken concerns of a heart defect.\"",
        "\"VBD\"",
        "\"VBG\"",
        "\"VBN\"",
        "\"VBP\"",
        "\"VBZ\"",
        "\"v\"",
        "'NN'",
        "'NNS'",
        "'NNP'",
        "'NNPS'",
        "'VB'",
        "'VBD'",
        "'VBG'",
        "'VBN'",
        "'VBP'",
        "'VBZ'",
        "'RB'",
        "'RBR'",
        "'RBS'",
        "'JJ'",
        "'JJR'",
        "'JJS'",
        "'en'",
        "'parser'",
        "'ner'",
        "\"The striped bats are hanging on their feet for best\"",
        "\" \"",
        "r'D:/jdk-13.0.2/bin/java.exe'",
        "'JAVAHOME'",
        "'/home/lzanella/stanford-parser-full-2018-10-17/stanford-parser.jar'",
        "'/home/lzanella/stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar'",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"r\"",
        "\"utf8\"",
        "'ignore'",
        "\"\\n SENTENCE %i : %s \\n \\n NPs: \\n\"",
        "\"NP\"",
        "'D:/jdk-13.0.2/bin/java.exe'",
        "'JAVA_HOME'",
        "'/home/lzanella/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz'",
        "'/home/lzanella/stanford-ner-2018-10-16/stanford-ner.jar'",
        "\"/home/lzanella/ameliepoulain.txt\"",
        "\"r\"",
        "\"utf8\"",
        "'ignore'",
        "'O'",
        "'en'",
        "\"Amélie is a story about a girl named Amélie whose childhood was suppressed by her Father's mistaken concerns of a heart defect.\"",
        "\"nlp_sentence\"",
        "'ent'"
    ],
    "variables": [
        "content",
        "sentences",
        "content",
        "tokens",
        "content",
        "tokens",
        "ed_words",
        "content",
        "content",
        "translator",
        "tokens",
        "tokens",
        "tokens",
        "tokens",
        "tagged_tokens",
        "verb_tags",
        "verbs",
        "content",
        "ed_verbs",
        "nlp",
        "sentence",
        "nlp_sentence",
        "spacy_pos_tagged",
        "sentence",
        "tokens",
        "tagged_tokens",
        "verb_tags",
        "verbs",
        "lemmatizer",
        "verb_lemmas",
        "lemma",
        "wn_tag",
        "wn_tag",
        "wn_tag",
        "wn_tag",
        "wn_tag",
        "lmtzr",
        "lemmas",
        "wn_tag",
        "lemma",
        "lemma",
        "lmtzr",
        "lemmas",
        "wn_tag",
        "lemma",
        "lemma",
        "tlp",
        "nlp",
        "sentence",
        "doc",
        "java_path",
        "scp",
        "content",
        "sentences",
        "counter",
        "parse_trees",
        "tree",
        "java_path",
        "sner",
        "named_entities",
        "content",
        "sentences",
        "counter",
        "ner_tagged_sentence",
        "sentence_named_entities",
        "named_entities",
        "nlp",
        "nlp_sentence"
    ],
    "comments": [
        "!/usr/bin/env python",
        "coding: utf-8",
        "In[2]:",
        "help(nltk.sent_tokenize)",
        "In[3]:",
        "Open the file \"data/hp.txt\"",
        "Use NLTK to tokenize) the text",
        "Print out the tokens",
        "In[5]:",
        "Open and read in file as a string, assign it to the variable `content`",
        "Split up entire text into tokens using word_tokenize():",
        "create an empty list to collect all words ending in -ed:",
        "Iterate over all tokens",
        "check if a token ends with -ed",
        "if the condition is met, add it to the ed-list",
        "Print the ed-words",
        "In[6]:",
        "Sentence splitting",
        "tokenizing",
        "print(filename)",
        "Split and Tokenize",
        "Print out the result",
        "In[7]:",
        "Define a translation table that maps each punctuation sign to the empty string",
        "remove punctuation",
        "print(filename)",
        "Segment, tokenize and remove punctuation",
        "In[8]:",
        "Segment, tokenize, remove punctuation",
        "Lowercase",
        "print(filename)",
        "Segment, tokenize, remove punctuation and lower case",
        "In[11]:",
        "Segment, tokenize, remove punctuation and lower case",
        "Remove stop words and return result",
        "print(filename)",
        "Sentence splitting",
        "In[12]:",
        "Start by reading the documentation:",
        "In[14]:",
        "Apply tokenization and POS tagging",
        "List of verb tags (i.e. tags we are interested in)",
        "Create an empty list to collect all verbs:",
        "Iterating over all tagged tokens",
        "Checking if the tag is any of the verb tags",
        "if the condition is met, add it to the list we created above",
        "Open and read in file as a string, assign it to the variable `content`",
        "In[1]:",
        "Load the SpaCy model for English",
        "Define test sentence",
        "Visualize results using Pandas datafram",
        "In[4]:",
        "Tokenize and Pos-tag the sentence",
        "Specify which POS tags label verbs",
        "Collect all verb tokens",
        "Instantiate a lemmatizer object",
        "Create an empty list of verb lemmas:",
        "For this lemmatizer, we need to indicate the POS of the word (in this case, v = verb)",
        "In[7]:",
        "Lemmatizing (the proper way, accounting for different POS tags)",
        "Write a  function to translate penn tree bank tags to wordnet tags",
        "Create a lemmatizer instance",
        "create an empty list to collect lemmas",
        "Iterate over the list of tagged tokens obtained before",
        "convert Penn Treebank POS tag to WordNet POS tag",
        "Check if a wordnet tag was assigned",
        "we lemmatize using the translated wordnet tag",
        "if there is no wordnet tag, we apply default lemmatization",
        "add lemmas to list",
        "Inspect lemmas by printing them",
        "In[8]:",
        "Create a lemmatizer instance",
        "create an empty list to collect lemmas",
        "Iterate over the list of tagged tokens obtained before",
        "convert Penn Treebank POS tag to WordNet POS tag",
        "Check if a wordnet tag was assigned",
        "we lemmatize using the translated wordnet tag",
        "if there is no wordnet tag, we apply default lemmatization",
        "add (token, lemma, pos) lemmas to list",
        "Lemmatize a list of tagged tokens",
        "In[9]:",
        "Initialize spacy 'en' model, keeping only tagger component needed for lemmatization",
        "Parse the sentence using the loaded 'en' model object `nlp`",
        "Extract the lemma for each token and join",
        "In[34]:",
        "from nltk.parse.corenlp import CoreNLPParser",
        "split the content into sentences",
        "get all NP trees and extract their leaves",
        "Use help(nltk.tree.Tree) to find out which NLTK method you can use to do this",
        "In[29]:",
        "Named Entity Recognition (Using Stanford NLP)",
        "print(\"\\n SENTENCE %i : %s \\n \\n NE: \\n\"%(counter,sentence))",
        "named_entities.append(sentence_named_entities)",
        "In[16]:",
        "Named Entity Recognition (Using SPACY)",
        "In[ ]:"
    ],
    "docstrings": [
        "\"\"\"\n    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n    \"\"\""
    ],
    "functions": [
        "segment_and_tokenize",
        "remove_punctuation",
        "lower_case",
        "remove_stop_words",
        "get_ed_verbs",
        "penn_to_wn",
        "lemmatize_postagged_tokens"
    ],
    "classes": []
}