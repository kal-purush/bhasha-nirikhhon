{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "nn",
        "numpy",
        "np",
        "Encoding",
        "Encoder",
        "Decoding",
        "Decoder",
        "PositionalEncoding",
        "PositionalEncoding",
        "utils",
        "weights_utils",
        "initialize_xavier_weights",
        "utils",
        "mask_utils",
        "create_padding_mask",
        "create_look_ahead_mask",
        "nn",
        "Module",
        "vocab_size",
        "d_model",
        "d_ff",
        "num_heads",
        "num_layers",
        "dropout",
        "use_embed_weights",
        "set_seq_len",
        "device",
        "torch",
        "device",
        "Transformer",
        "d_model",
        "d_ff",
        "num_heads",
        "num_layers",
        "dropout",
        "np",
        "sqrt",
        "d_model",
        "nn",
        "Embedding",
        "vocab_size",
        "d_model",
        "nn",
        "init",
        "normal_",
        "embedding_module",
        "weight",
        "mean",
        "std",
        "d_model",
        "nn",
        "Dropout",
        "dropout",
        "use_embed_weights",
        "decoder_out",
        "embedding_module",
        "weight",
        "transpose",
        "decoder_out",
        "nn",
        "Linear",
        "d_model",
        "vocab_size",
        "initialize_xavier_weights",
        "project_to_vocabs",
        "PositionalEncoding",
        "d_model",
        "set_seq_len",
        "device",
        "Encoder",
        "d_model",
        "d_ff",
        "num_heads",
        "num_layers",
        "dropout",
        "Decoder",
        "d_model",
        "d_ff",
        "num_heads",
        "num_layers",
        "dropout",
        "input_sentences",
        "input_sentences",
        "device",
        "create_padding_mask",
        "input_sentences",
        "device",
        "device",
        "embedding_module",
        "input_sentences",
        "input_sentences",
        "masked_fill_",
        "padding_mask",
        "unsqueeze",
        "input_sentences",
        "mul_",
        "embedding_scale",
        "positional_encoder",
        "input_sentences",
        "embedding_dropout",
        "input_sentences",
        "encoder",
        "input_sentences",
        "padding_mask",
        "decoded_outputs",
        "encoder_outputs",
        "cache",
        "decoded_outputs",
        "device",
        "create_padding_mask",
        "decoded_outputs",
        "device",
        "device",
        "create_look_ahead_mask",
        "decoded_outputs",
        "padding_mask",
        "padding_mask",
        "embedding_module",
        "decoded_outputs",
        "decoded_outputs",
        "masked_fill_",
        "padding_mask",
        "unsqueeze",
        "decoded_outputs",
        "mul_",
        "embedding_scale",
        "positional_encoder",
        "decoded_outputs",
        "embedding_dropout",
        "decoded_outputs",
        "decoder",
        "decoded_outputs",
        "encoder_outputs",
        "padding_mask",
        "look_ahead_mask",
        "cache",
        "input_sentences",
        "decoded_outputs",
        "encode",
        "input_sentences",
        "decode",
        "decoded_outputs",
        "encoder_outputs",
        "decoder_outputs"
    ],
    "literals": [
        "\"cpu\""
    ],
    "variables": [
        "d_model",
        "d_ff",
        "num_heads",
        "num_layers",
        "dropout",
        "embedding_scale",
        "embedding_module",
        "embedding_dropout",
        "project_to_vocabs",
        "project_to_vocabs",
        "positional_encoder",
        "encoder",
        "decoder",
        "device",
        "padding_mask",
        "input_sentences",
        "input_sentences",
        "input_sentences",
        "device",
        "padding_mask",
        "look_ahead_mask",
        "decoded_outputs",
        "decoded_outputs",
        "decoded_outputs",
        "encoder_outputs",
        "decoder_outputs"
    ],
    "comments": [
        "configurations for our embedding module",
        "the paper suggests that we divide our embeddings by sqrt(d_model)",
        "we use a learn-able embedding module and initialize the weights to have mean 0 and standard deviation",
        "1/sqrt(d_model)",
        "regularization for embeddings",
        "if use_embed_weights is true, our output projection weights are tied to the embedding module's weights",
        "we define this as a lambda function that can be called -- we can also define a different method and point",
        "the variable to that method instead",
        "otherwise, our output projection is a linear layer which we use Xavier initialization on",
        "for more details about the following modules, please read their documentations",
        "we initialize the positional encoding module",
        "the encoder and decoder",
        "we need to create a mask which indicates the positions which a token is padding token and should be ignored",
        "we convert our input into word embeddings",
        "here, we use the padding mask to put a 0 wherever there is a padding token -- the word embedding value for",
        "these tokens should be 0 since they do not hold any meaning",
        "this does an inplace fill",
        "as suggested by the paper, we scale the magnitude of the word embeddings by sqrt(d_model)",
        "next step is to add positional information using positional encoding",
        "regularize the embeddings (as it is learned) using dropout",
        "similar to the encoder we create a padding mask to indicate the tokens that should be ignored as padding;",
        "we also use a look ahead mask to indicate the tokens that should be ignored when we are decoding the next",
        "token using the prefixes of the sentences -- this is what happens during inference so we need to set up this",
        "environment for training as well",
        "convert to word embeddings",
        "we zero-out the positions that are padding tokens in our word embeddings",
        "note that we do not use the look ahead mask to zero-out our word embeddings because the look ahead mask tells",
        "the attention mechanism to ignore future words in the decoded outputs while decoding, but those words (tokens)",
        "still have meaning, in contrast to padding tokens which have no meaning at all",
        "we scale the embeddings' magnitude by sqrt(d_model)",
        "we encode positional information to the word embeddings next",
        "dropout regularization"
    ],
    "docstrings": [
        "\"\"\"\n    The Transformer architecture as proposed by the paper \"Attention is All You Need\". This encoder-decoder architecture\n    consists of an encoder and decoder stack which implements the attention mechanism.\n\n    1. An input sentence is first tokenized and then converted into word embeddings. We then apply positional encoding\n    to the word embeddings obtained so that we can incorporate positional information to the word embeddings.\n\n    2. The positionally encoded word embeddings are then passed into the encoder, which converts these embeddings into\n    hidden representations after applying self-attention. The self-attention mechanism uses dot product to compute\n    compatibility between words within a sentence and uses this to scale the magnitude of the word embedding of each\n    word within that sentence.\n\n    3. The outputs from the encoder are given to the decoder, which decodes this into the output which we hopefully\n    desire. The decoder also gets as input, the sequence of tokens that are decoded up to the current time step. This\n    is because the Transformer is a language model, and so it decodes each word based on the given input, which the\n    encoder transforms into a hidden representation, and the decoded-so-far sequence. During training, we use teacher-\n    forcing, which is a technique that uses the true (expected) token as input to the next time step instead of the\n    predicted token. Therefore, we simply use the entire expected output as input into our decoder in training.\n\n    4. From the results outputted by our decoder, we perform a linear transformation where the output dimensions\n    corresponds to the size of our vocabulary. We then perform softmax on the tensor to convert these values into a\n    probability distribution. This is the output of the Transformer.\n\n    We can then use a search algorithm, the best one probably being Beam Search to convert these probability\n    distributions into actual sentences. This process is known as detokenization.\n    \"\"\"",
        "\"\"\"\n        vocab_size: the number of unique words in our vocabulary\n        d_model: the dimensionality of the outputs <hyperparameter>\n        d_ff: the number of hidden units for our feed forward layer <hyperparameter>\n        num_heads: the number of heads to split the attention down <hyperparameter>\n        num_layers: the number of encoder layers to apply <hyperparameter>\n        dropout: the rate of dropout regularization <hyperparameter>\n        use_embed_weights: whether or not to use the embeddings' weights as the final projection weights\n        set_seq_len: the longest sequence we ever have to process for pre-computation of the positional information\n        device: device which operations are performed on\n        \"\"\"",
        "\"\"\"\n        Encode the sentences by transforming them into hidden representations using the attention mechanism.\n\n        Input:\n            input_sentences: the batch of sentences which we will process through the encoder\n\n            The input should have shape (batch_size, seq_len)\n\n        Output:\n            The encoded representations of the input sentences.\n        \"\"\"",
        "\"\"\"\n        Using the inputs that are encoded by the encoder and the outputs that are decoded to the current time-step,\n        decode the next token. During training, this is done simultaneously for all time-steps as we utilize\n        teacher-forcing.\n\n        Inputs:\n            decoded_outputs: during training, this is simply the expected outputs, where as during inference, this is\n            the outputs which the model has decoded to a certain time step which is used to infer subsequent tokens\n            encoder_outputs: the hidden representation outputted by the encoder from the inputs\n            cache: for storing computed attention weights for later usage without re-computation\n\n            The inputs should have the following shape:\n            decoded_outputs: during training: (batch_size, [target] seq_len, d_model),\n                             during inference: (batch_size, curr_seq_len, d_model)\n            encoder_outputs: (batch_size, [input] seq_len, d_model)\n\n        Outputs:\n            The next time-step decoded output(s).\n        \"\"\"",
        "\"\"\"\n        Forwards the input sentences through the Transformer architecture by feeding it into the encoder to produce\n        a hidden representation that is used to decode the corresponding output(s).\n\n        Inputs:\n            Input:\n            input_sentences: the batch of sentences which we will process through the encoder\n            decoded_outputs: during training, this is simply the expected outputs, where as during inference, this is\n            the outputs which the model has decoded to a certain time step which is used to infer subsequent tokens\n\n            The inputs should have shape:\n            input_sentences: (batch_size, [input] seq_len)\n            decoded_outputs: during training: (batch_size, [target] seq_len, d_model),\n                             during inference: (batch_size, curr_seq_len, d_model)\n\n        Outputs:\n            The outputs decoded from the given input sentences.\n        \"\"\""
    ],
    "functions": [
        "encode",
        "decode",
        "forward"
    ],
    "classes": [
        "Transformer"
    ]
}