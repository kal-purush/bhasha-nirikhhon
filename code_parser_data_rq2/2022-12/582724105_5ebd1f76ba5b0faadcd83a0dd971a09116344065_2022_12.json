{
    "identifiers": [
        "torch",
        "wandb",
        "time",
        "os",
        "tqdm",
        "tqdm",
        "numpy",
        "np",
        "pandas",
        "pd",
        "tqdm",
        "pandas",
        "datasets",
        "load_dataset",
        "torch",
        "utils",
        "data",
        "Dataset",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "transformers",
        "AutoTokenizer",
        "pipeline",
        "AutoModelForSequenceClassification",
        "trl",
        "AutoModelForCausalLMWithValueHead",
        "trl",
        "ppo",
        "PPOTrainer",
        "trl",
        "core",
        "build_bert_batch_from_txt",
        "listify_batch",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "torch",
        "cuda",
        "is_available",
        "wandb",
        "init",
        "name",
        "project",
        "config",
        "config",
        "pd",
        "read_csv",
        "AutoTokenizer",
        "from_pretrained",
        "config",
        "gpt2_tokenizer",
        "eos_token",
        "Dataset",
        "df",
        "verbose",
        "training",
        "df",
        "df",
        "content",
        "apply",
        "x",
        "x",
        "char_max_len",
        "tolist",
        "df",
        "title",
        "tolist",
        "t",
        "a",
        "t",
        "a",
        "text",
        "answer",
        "verbose",
        "len",
        "text",
        "index",
        "text",
        "index",
        "texts",
        "data",
        "training",
        "data",
        "gpt2_tokenizer",
        "batch_encode_plus",
        "texts",
        "padding",
        "max_length",
        "MAX_LEN",
        "truncation",
        "return_attention_mask",
        "return_tensors",
        "encoded",
        "clone",
        "detach",
        "encoded",
        "clone",
        "detach",
        "encoded",
        "target",
        "clone",
        "detach",
        "target",
        "gpt2_tokenizer",
        "pad_token_id",
        "input_ids",
        "attention_masks",
        "labels",
        "texts",
        "ConvDataset",
        "raw_df",
        "DataLoader",
        "train_dataset",
        "batch_size",
        "BATCH_SIZE",
        "drop_last",
        "shuffle",
        "num_workers",
        "collate_fn",
        "collate_fn",
        "config",
        "AutoTokenizer",
        "from_pretrained",
        "REWARD_MODEL_NAME",
        "cache_dir",
        "AutoModelForSequenceClassification",
        "from_pretrained",
        "REWARD_MODEL_NAME",
        "s",
        "AutoModelForCausalLMWithValueHead",
        "from_pretrained",
        "config",
        "AutoModelForCausalLMWithValueHead",
        "from_pretrained",
        "config",
        "wandb",
        "watch",
        "gpt2_model",
        "log",
        "gpt2_model",
        "to",
        "device",
        "gpt2_model_ref",
        "to",
        "device",
        "min_value",
        "max_value",
        "min_value",
        "max_value",
        "np",
        "random",
        "choice",
        "values",
        "LengthSampler",
        "config",
        "config",
        "LengthSampler",
        "config",
        "config",
        "sample",
        "gpt2_tokenizer",
        "encode",
        "sample",
        "input_size",
        "gpt2_tokenizer",
        "decode",
        "sample",
        "sample",
        "ds",
        "tokenize",
        "batched",
        "gpt2_tokenizer",
        "eos_token_id",
        "data",
        "key",
        "d",
        "key",
        "d",
        "data",
        "key",
        "data",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "ds",
        "batch_size",
        "config",
        "collate_fn",
        "collater",
        "PPOTrainer",
        "gpt2_model",
        "gpt2_model_ref",
        "gpt2_tokenizer",
        "config",
        "np",
        "ceil",
        "config",
        "config",
        "epoch",
        "batch",
        "tqdm",
        "total_ppo_epochs",
        "iter",
        "dataloader",
        "time",
        "time",
        "torch",
        "tensor",
        "t",
        "to",
        "device",
        "t",
        "batch",
        "time",
        "time",
        "i",
        "config",
        "output_size",
        "gpt2_model",
        "generate",
        "query_tensors",
        "i",
        "unsqueeze",
        "dim",
        "max_new_tokens",
        "gen_len",
        "gen_kwargs",
        "response_tensors",
        "append",
        "response",
        "squeeze",
        "gen_len",
        "gpt2_tokenizer",
        "decode",
        "r",
        "squeeze",
        "r",
        "response_tensors",
        "time",
        "time",
        "t",
        "time",
        "time",
        "q",
        "r",
        "q",
        "r",
        "batch",
        "batch",
        "sentiment_pipe",
        "texts",
        "sent_kwargs",
        "torch",
        "tensor",
        "output",
        "output",
        "pipe_outputs",
        "to",
        "device",
        "time",
        "time",
        "t",
        "time",
        "time",
        "ppo_trainer",
        "step",
        "query_tensors",
        "response_tensors",
        "rewards",
        "time",
        "time",
        "t",
        "time",
        "time",
        "t0",
        "r",
        "r",
        "batch",
        "batch",
        "rewards",
        "cpu",
        "tolist",
        "logs",
        "update",
        "wandb",
        "Table",
        "columns",
        "rows",
        "table_rows",
        "logs",
        "update",
        "timing",
        "logs",
        "update",
        "stats",
        "torch",
        "mean",
        "rewards",
        "cpu",
        "numpy",
        "torch",
        "std",
        "rewards",
        "cpu",
        "numpy",
        "rewards",
        "cpu",
        "numpy",
        "wandb",
        "log",
        "logs"
    ],
    "literals": [
        "\"model_name\"",
        "\"/home/ubuntu/cloudfs/saved_models/__home__ubuntu__cloudfs__saved_models____home__ubuntu__cloudfs__huggingfacecache__bloom-1b7_1670552399_e1_step151891_loss2.337894916534424_1671125974_e1_step43135_loss0.7372605204582214\"",
        "\"cls_model_name\"",
        "\"lvwerra/distilbert-imdb\"",
        "\"steps\"",
        "\"batch_size\"",
        "\"forward_batch_size\"",
        "\"ppo_epochs\"",
        "\"txt_in_min_len\"",
        "\"txt_in_max_len\"",
        "\"txt_out_min_len\"",
        "\"txt_out_max_len\"",
        "\"lr\"",
        "\"init_kl_coef\"",
        "\"target\"",
        "\"horizon\"",
        "\"gamma\"",
        "\"lam\"",
        "\"cliprange\"",
        "\"cliprange_value\"",
        "\"vf_coef\"",
        "\"cuda\"",
        "\"cpu\"",
        "'run-1'",
        "'bloomz-suspence-ppo'",
        "'/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_1208_1670534928.csv'",
        "'model_name'",
        "f\"根据指定内容，撰写爆款小红书笔记标题：\\n：需要起标题的内容：[{t}]\\n小红书标题：[{a}]\"",
        "'longest'",
        "\"pt\"",
        "'input_ids'",
        "'attention_mask'",
        "'input_ids'",
        "\"return_all_scores\"",
        "\"function_to_apply\"",
        "\"none\"",
        "\"batch_size\"",
        "\"forward_batch_size\"",
        "'IDEA-CCNL/Erlangshen-DeBERTa-v2-710M-Chinese'",
        "\"/home/ubuntu/cloudfs/huggingfacecache\"",
        "'/home/ubuntu/cloudfs/saved_models/IDEA-CCNL__Erlangshen-DeBERTa-v2-710M-Chinese_1672156931_e4_step499_loss0.0991830825805664'",
        "'model_name'",
        "'model_name'",
        "'all'",
        "\"txt_in_min_len\"",
        "\"txt_in_max_len\"",
        "\"txt_out_min_len\"",
        "\"txt_out_max_len\"",
        "\"tokens\"",
        "\"review\"",
        "\"query\"",
        "\"tokens\"",
        "\"min_length\"",
        "\"top_k\"",
        "\"top_p\"",
        "\"do_sample\"",
        "\"pad_token_id\"",
        "'batch_size'",
        "\"steps\"",
        "'batch_size'",
        "\"tokens\"",
        "'batch_size'",
        "'response'",
        "'time/get_response'",
        "'query'",
        "'response'",
        "\"score\"",
        "'time/get_sentiment_preds'",
        "'time/optimization'",
        "'time/epoch'",
        "'query'",
        "'response'",
        "'game_log'",
        "'query'",
        "'response'",
        "'reward'",
        "'env/reward_mean'",
        "'env/reward_std'",
        "'env/reward_dist'"
    ],
    "variables": [
        "config",
        "device",
        "pipe_device",
        "raw_df",
        "gpt2_tokenizer",
        "gpt2_tokenizer",
        "pad_token",
        "MAX_LEN",
        "MAX_TARGET_LEN",
        "BATCH_SIZE",
        "char_max_len",
        "df",
        "text",
        "answer",
        "text",
        "verbose",
        "texts",
        "texts",
        "encoded",
        "input_ids",
        "attention_masks",
        "target",
        "labels",
        "labels",
        "train_dataset",
        "train_loader",
        "sent_kwargs",
        "REWARD_MODEL_NAME",
        "reward_tokenizer",
        "REWARD_MODEL_NAME",
        "reward_model",
        "gpt2_model",
        "gpt2_model_ref",
        "values",
        "input_size",
        "output_size",
        "sample",
        "sample",
        "ds",
        "gen_kwargs",
        "dataloader",
        "ppo_trainer",
        "total_ppo_epochs",
        "logs",
        "timing",
        "t0",
        "query_tensors",
        "t",
        "response_tensors",
        "gen_len",
        "response",
        "batch",
        "timing",
        "t",
        "texts",
        "pipe_outputs",
        "rewards",
        "timing",
        "t",
        "stats",
        "timing",
        "timing",
        "table_rows",
        "logs",
        "logs",
        "logs"
    ],
    "comments": [
        "get dataset",
        "",
        "40 #192#1077 #938 #2709 #267#248",
        "330 #1000 #110 #120 #140 #192",
        "2",
        "make sure it won't truncate the title part.",
        "self.questions = df.t5_text.apply(lambda x: x[:x.index('chat history:')]).tolist()",
        "question = self.questions[index]",
        "print(texts)",
        "load imdb with datasets",
        "ds = load_dataset('imdb', split='train')",
        "ds = ds.rename_columns({'text': 'review', 'label': 'sentiment'})",
        "ds = ds.filter(lambda x: len(x[\"review\"])>200, batched=False)",
        "get reward score...",
        "",
        "entiment_pipe = pipeline(\"sentiment-analysis\",\"lvwerra/distilbert-imdb\", device=pipe_device)",
        "Get response from gpt2",
        "Compute sentiment score",
        "Run PPO step",
        "Log everything"
    ],
    "docstrings": [],
    "functions": [
        "__len__",
        "__getitem__",
        "collate_fn",
        "__call__",
        "tokenize",
        "collater"
    ],
    "classes": [
        "ConvDataset",
        "LengthSampler"
    ]
}