{
    "identifiers": [
        "math",
        "numpy",
        "np",
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "nn",
        "Module",
        "d_model",
        "nhead",
        "num_layers",
        "i",
        "num_layers",
        "layers",
        "append",
        "nn",
        "TransformerEncoderLayer",
        "d_model",
        "nhead",
        "batch_first",
        "nn",
        "Sequential",
        "layers",
        "latent",
        "processlayer",
        "latent",
        "nn",
        "Module",
        "vocab_size",
        "d_model",
        "nhead",
        "num_layers",
        "nn",
        "Embedding",
        "vocab_size",
        "d_model",
        "PositionalEncoding",
        "d_model",
        "nn",
        "TransformerDecoderLayer",
        "d_model",
        "nhead",
        "batch_first",
        "latent",
        "input",
        "read_latent_size",
        "embed",
        "input",
        "pos_enc",
        "input",
        "readlayer",
        "latent",
        "read_latent_size",
        "input",
        "torch",
        "cat",
        "latent",
        "read_latent_size",
        "read_latent",
        "dim",
        "nn",
        "Module",
        "d_model",
        "nhead",
        "num_layers",
        "nn",
        "TransformerDecoderLayer",
        "d_model",
        "nhead",
        "batch_first",
        "latent",
        "memory",
        "read_latent_size",
        "readlayer",
        "latent",
        "read_latent_size",
        "memory",
        "torch",
        "cat",
        "latent",
        "read_latent_size",
        "read_latent",
        "dim",
        "nn",
        "Module",
        "d_model",
        "nhead",
        "vocab_size",
        "num_layers",
        "nn",
        "TransformerDecoderLayer",
        "d_model",
        "nhead",
        "tgt_is_causal",
        "nn",
        "Linear",
        "d_model",
        "vocab_size",
        "context",
        "latent",
        "outputlayer",
        "context",
        "latent",
        "token_embed",
        "temperature",
        "top_k",
        "top_p",
        "proj",
        "token_embed",
        "logit",
        "temperature",
        "torch",
        "argmax",
        "logit",
        "dim",
        "token",
        "nn",
        "Module",
        "d_model",
        "dropout",
        "max_len",
        "nn",
        "Dropout",
        "p",
        "dropout",
        "torch",
        "zeros",
        "max_len",
        "d_model",
        "torch",
        "arange",
        "max_len",
        "dtype",
        "torch",
        "unsqueeze",
        "torch",
        "exp",
        "torch",
        "arange",
        "d_model",
        "math",
        "log",
        "d_model",
        "torch",
        "sin",
        "position",
        "div_term",
        "torch",
        "cos",
        "position",
        "div_term",
        "pe",
        "unsqueeze",
        "transpose",
        "register_buffer",
        "pe",
        "x",
        "step_pos",
        "step_pos",
        "x",
        "pe",
        "step_pos",
        "x",
        "pe",
        "x",
        "size",
        "dropout",
        "x",
        "nn",
        "Module",
        "d_model",
        "nhead",
        "vocab_size",
        "ProcessLayer",
        "d_model",
        "nhead",
        "ReadLayer",
        "vocab_size",
        "d_model",
        "nhead",
        "ReadLatentLayer",
        "d_model",
        "nhead",
        "OutputLayer",
        "d_model",
        "nhead",
        "vocab_size",
        "max_steps",
        "max_latent",
        "torch",
        "randn",
        "torch",
        "randn",
        "latent_size",
        "batch_size",
        "torch",
        "randn",
        "batch_size",
        "latent_size",
        "d_latent",
        "latent",
        "latent",
        "step_count",
        "latent_store",
        "latent",
        "torch",
        "cat",
        "latent_store",
        "latent",
        "dim",
        "long_term_latent",
        "long_term_latent",
        "inputext",
        "split",
        "wide_input",
        "latent_size",
        "inputext",
        "shape",
        "torch",
        "split",
        "inputext",
        "split",
        "np",
        "cumsum",
        "split",
        "inputext",
        "split",
        "init_latent",
        "latent_size",
        "batch_size",
        "wide_input",
        "forward_compression",
        "wide_input",
        "compressed_latents",
        "shape",
        "compressed_latents",
        "view",
        "B",
        "C",
        "L",
        "repeat",
        "B",
        "torch",
        "cat",
        "latent_store",
        "compressed_latents",
        "dim",
        "readlayer",
        "input_chunk",
        "latent",
        "processlayer",
        "latent",
        "store_latent",
        "latent",
        "i",
        "max_steps",
        "latent",
        "shape",
        "readlayer",
        "input_chunk",
        "latent",
        "rl",
        "processlayer",
        "latent",
        "readlatentlayer",
        "latent",
        "latent_store",
        "rl",
        "processlayer",
        "latent",
        "store_latent",
        "latent",
        "target_chunks",
        "i",
        "torch",
        "cat",
        "input_chunk",
        "target_chunk",
        "dim",
        "ouputlayer",
        "target_chunk",
        "latent",
        "tgt_is_causal",
        "outputs",
        "append",
        "ouputlayer",
        "get_token",
        "CategoricalCrossEntropyLoss",
        "out_tokens",
        "target_chunk",
        "ouputlayer",
        "proj",
        "inputext",
        "parents_logits",
        "i",
        "mse",
        "out_logits",
        "parents_logits",
        "outputs",
        "inputext",
        "latent",
        "inputext",
        "embed",
        "x",
        "pos_enc",
        "x",
        "readlayer",
        "x",
        "latent",
        "processlayer",
        "latent",
        "readlatentlayer",
        "latent",
        "latent_store",
        "processlayer",
        "latent",
        "store_latent",
        "latent",
        "latent",
        "latent",
        "max_token",
        "torch",
        "zeros",
        "torch",
        "zeros",
        "dtype",
        "torch",
        "i",
        "max_token",
        "pos_enc",
        "out_embed",
        "ouputlayer",
        "out_embed",
        "latent",
        "tgt_is_causal",
        "ouputlayer",
        "get_token",
        "out_embed",
        "embed",
        "out_token",
        "out_token",
        "out_token",
        "latent",
        "inputext",
        "latent_size",
        "inputext",
        "shape",
        "init_latent",
        "latent_size",
        "torch",
        "cat",
        "compress_token_signal_embed",
        "latent",
        "dim",
        "readlayer",
        "inputext",
        "latent",
        "processlayer",
        "latents",
        "readlayer",
        "inputext",
        "latents",
        "processlayer",
        "latents",
        "latents"
    ],
    "literals": [
        "'pe'"
    ],
    "variables": [
        "layers",
        "processlayer",
        "embed",
        "pos_enc",
        "readlayer",
        "input",
        "input",
        "read_latent",
        "readlayer",
        "read_latent",
        "outputlayer",
        "proj",
        "logit",
        "logit",
        "token",
        "dropout",
        "pe",
        "position",
        "div_term",
        "pe",
        "pe",
        "pe",
        "x",
        "processlayer",
        "readlayer",
        "readlatentlayer",
        "ouputlayer",
        "d_latent",
        "max_len",
        "max_steps",
        "max_latent",
        "max_latent_store",
        "latent_store",
        "permanent_latent_store",
        "compress_token_signal_embed",
        "step_count",
        "latent",
        "latent_store",
        "latent_store",
        "latent_store",
        "batch_size",
        "target_chunks",
        "split",
        "input_chunk",
        "latent",
        "compressed_latents",
        "B",
        "C",
        "L",
        "compressed_latents",
        "latent_store",
        "latent",
        "latent",
        "outputs",
        "rl",
        "latent",
        "latent",
        "latent",
        "latent",
        "target_chunk",
        "input_chunk",
        "out_tokens",
        "loss",
        "out_logits",
        "parents_logits",
        "loss",
        "x",
        "x",
        "x",
        "latent",
        "latent",
        "latent",
        "latent",
        "out_embed",
        "out_token",
        "out_embed",
        "out_embed",
        "out_token",
        "out_embed",
        "batch_size",
        "chunks",
        "lenght",
        "latent",
        "latent",
        "latents",
        "latents",
        "latents",
        "latents"
    ],
    "comments": [
        "latent[:, :read_latent_size, :] = self.readlayer(latent[:, :read_latent_size, :], input)",
        "todo: check if this code works",
        "todo: check if it perform well with nhead=1",
        "return self.readlayer(latent, memory)",
        "argmax to get the token",
        "th1nker model",
        "self.pos_enc = PositionalEncoding(d_model, max_len=200)",
        "self.permanent_latent_store_embed = torch.randn(512)",
        "self.medium_memory_latent_store_embed = torch.randn(512)",
        "add position encoding, same enconding on the whole vector",
        "latent = self.pos_enc(latent, step_pos=self.step_count)",
        "latent += self.medium_memory_latent_store_embed",
        "self.store_latent(compressed_latents)",
        "shape = B x C x L",
        "shape = B x B*C x L",
        "read latent length, can be randomize",
        "output generation",
        "this should go in the train loop",
        "standart loss computation",
        "loss with knowledge distillation from parent logit",
        "auto-regressive generation loop",
        "start token",
        "end token"
    ],
    "docstrings": [
        "'''\n# Th1nker model\n\nWhy?\nBecause AI today is not smart enough, for lazy guys like me\n\nWhat?\nTrain a model to learn a thinking process steps\n\nHow? by :\ngiving steps in the process\n2. splitting knowledge than think process\n3. by giving process mechanism to build/access knowledge\n\n-- ideas : rewrite a text in the format \"question? answer. question? answer. \" and use that text to teach a model to follow instructions with just LLMM standard AR and avoid complex RL\n\n## In details\nthe thinker can be a transformer base model, made with building blocks responsible for each task in the process\n\n### Blocks\nHere are the proposed building blocks:\n1. the step process is given by calling during inference:\n\t1. ReadLayer : read any kind of information\n\t2. ProcessLayer : process information, always the same\n\t3. OutputLayer : get output from\n\t4. Probe (optional) : predicting the block that will be used in the next thinking step\n2. processing memory (or short-term memory)\n\t1. Latent : is as input in all layers mentioned before, it also the initial input of the model\n\t2. Input : is the input data we want to process, we may have TextInput, ImageInput ... , InputText + Latent -> ReadTextInputLayer\n\t3. Ouput : is the expected output, optionally autoregressively (output probe)\n3. medium-term memory component\n\t1. LatentStore : we store latent at each processing step, they will be read at inference with ReadLatentLayer\n\t2. \n4. long-term memory : is also built with latent but of the past run of the model\n\t1. it will be probably very large and need an indexer to get the useful one\n\t2. ReadLatentLayer here will be associated with a learned Indexer\n\t3. \n\n### Inference\n\n\n### Training\n1. learn how to do step:  \n\tjust train in forward pass-over step with random initial latent,  \n\tre-read input at each step, to avoid model divergence\n\tget output at each step a give the loss training signal\n\tfor autoregressive task  \n\tback-propagrade over many steps, use stop gradient after some step for the model to generalize how to handle/denoise any kind of latent in the middle of the thinking process, we can also simply add noise/mask/...\n\n2. learn how to use memory\n\t**short memory** represented by the Latent will be learning with the training mentioned before since Latent is the communication mechanism between step\n\t**medium memory** for this one :\n\t1. we store latent at each step\n\t2. we use a ReadLatentLayer to read the previous latent step\n\t3. backprob signal passing through the read latent, to the layer that produced it. that will give the learning signal to backprop that latent can be re-use, so that backprop can optimize it\n\t4. keep all those gradients might use a lot of memory\n\t\t1. we can stop after a certain number of steps\n\t\t2. we will start with a small to study how the number of steps affects the inference \n\n3. learn how to use/build long-term memory\n\tfor using long-term memory we can keep old latent in a store and read them during inference  \n\tbut since those latent are built to be reused, the learning could be slow  \n\tto accelerate it we can backdrop the signal like in the medium term memory. that can be challenging to do since the long term it's way larger. An approach is hold it in different GPUs, the required backward state for the backward gradient step, that can only at scale.  \n\n\tbut we will use a more efficient way to accelerate it. we assume that long term can be read by just reading a lot of input data and keeping them in compressed format in the memory. we can do that by running Input + Latent(compressed signal token) -> ReadInput -> Process -> Latent(compressed)  \n\twith this approach we can do backprop during training: eg. for text input we can run this on many text sections to get the compressed Latent knowledge and use during step with ReadLatentLayer. the training signal will be backprop thought process, reading and up to the compressed signal token (which a just one token, the other token can be random)  \n\twe can do this process hierarchically by compressing again the compressed token before giving them to the thinking steps. to avoid losing information, we can make the second step less compressive.  \n\t**important!** that will also teach the model how to synthesize his own knowledge, via extracting important information from what it already extracted. we might do many times during inference eg. 10 Levels of compression :-) to have contextual compression we can give the compression process the Latent in the current step. Also, that could slow down execution during training by making it more sequential and less parallel.  \n\n4. learn how to index long-term memory  \n\tindexing because long-term memory, can be very wide - then pre-selected the tokens that will be read by the model will help significantly  \n\tcan be done by learning how to predict the most useful block of memory  \n\tuseful block can be estimated by how much attention is given to them, but might not be meaningful  \n\tRL can we using considering memory reading block as action, so that the model will how to choose the best one.  \n\n\twe can make it easy for the model by caching the most frequently used memory block, maybe they might be general knowledge that are not stored in the language model weight. Like coding skills, some English vocabulary, ... maybe they might differ for a given task such as coding python, or coding java, or writing a novel.  \n\n'''",
        "'''\n\t\t\tinstead of unique d_model we may have:\n\t\t\t- d_latent = d_model\n\t\t\t- d_input = d_output\n\t\t\t- d_memory in case latent we have combined latents\n\t\t'''"
    ],
    "functions": [
        "forward",
        "forward",
        "forward",
        "forward",
        "get_token",
        "forward",
        "init_latent",
        "store_latent",
        "init_latent_store",
        "forward",
        "foward_step",
        "forward_output",
        "forward_compression"
    ],
    "classes": [
        "ProcessLayer",
        "ReadLayer",
        "ReadLatentLayer",
        "OutputLayer",
        "PositionalEncoding",
        "Thinker"
    ]
}