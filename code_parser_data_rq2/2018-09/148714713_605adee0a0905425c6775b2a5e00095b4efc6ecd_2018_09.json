{
    "identifiers": [
        "argparse",
        "json",
        "os",
        "argparse",
        "ArgumentParser",
        "description",
        "parser",
        "add_argument",
        "required",
        "help",
        "parser",
        "add_argument",
        "required",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "add_argument",
        "help",
        "parser",
        "parse_args",
        "os",
        "path",
        "isfile",
        "args",
        "benchmark_output",
        "format",
        "args",
        "benchmark_output",
        "os",
        "path",
        "isfile",
        "args",
        "labels",
        "format",
        "args",
        "labels",
        "filename",
        "open",
        "filename",
        "f",
        "f",
        "read",
        "content",
        "strip",
        "split",
        "item",
        "item",
        "batch",
        "strip",
        "split",
        "batch",
        "batches",
        "data",
        "values",
        "data",
        "metric",
        "unit",
        "values",
        "len",
        "values",
        "data",
        "data",
        "data",
        "data",
        "data",
        "data",
        "unit",
        "metric",
        "json",
        "dumps",
        "entry",
        "sort_keys",
        "args",
        "metric_keyword",
        "args",
        "metric_keyword",
        "s",
        "s",
        "results",
        "item",
        "item",
        "results",
        "sum",
        "values",
        "num_corrects",
        "len",
        "values",
        "writeOneResult",
        "values",
        "num_corrects",
        "writeOneResult",
        "values",
        "percent",
        "getData",
        "args",
        "benchmark_output",
        "open",
        "args",
        "labels",
        "f",
        "f",
        "read",
        "item",
        "strip",
        "split",
        "item",
        "content",
        "strip",
        "split",
        "item",
        "item",
        "item",
        "item",
        "golden_lines",
        "len",
        "benchmark_data",
        "len",
        "golden_data",
        "format",
        "len",
        "benchmark_data",
        "format",
        "len",
        "golden_data",
        "elem",
        "elem",
        "i",
        "len",
        "benchmark_data",
        "benchmark_data",
        "i",
        "golden_data",
        "i",
        "j",
        "benchmark_one_entry",
        "j",
        "j",
        "len",
        "benchmark_one_entry",
        "benchmark_result",
        "sort",
        "reverse",
        "key",
        "sort_key",
        "golden_one_entry",
        "item",
        "item",
        "benchmark_result",
        "args",
        "top",
        "writeResult",
        "golden_data",
        "OutputCompare",
        "app",
        "compare"
    ],
    "literals": [
        "\"Output compare\"",
        "\"--benchmark-output\"",
        "\"The output of the benchmark.\"",
        "\"--labels\"",
        "\"The golden output.\"",
        "\"--metric-keyword\"",
        "\"The keyword prefix each metric so that the harness can parse.\"",
        "\"--result-file\"",
        "\"Write the prediction result to a file for debugging purpose.\"",
        "\"--top\"",
        "\"Integer indicating whether it is a top one or top five.\"",
        "\"Benchmark output file {} doesn't exist\"",
        "\"Labels file {} doesn't exist\"",
        "\"r\"",
        "'\\n'",
        "','",
        "\"type\"",
        "\"model\"",
        "\"values\"",
        "\"summary\"",
        "\"num_runs\"",
        "\"p0\"",
        "\"p10\"",
        "\"p50\"",
        "\"p90\"",
        "\"p100\"",
        "\"mean\"",
        "\"unit\"",
        "\"metric\"",
        "\" \"",
        "\"predict\"",
        "\"number of corrects\"",
        "\"number\"",
        "\"percent of corrects\"",
        "\"percent\"",
        "\"r\"",
        "','",
        "'\\n'",
        "\"index\"",
        "\"label\"",
        "\"path\"",
        "\"Benchmark data has {} entries, \"",
        "\"but genden data has {} entries\"",
        "\"value\"",
        "\"index\"",
        "\"value\"",
        "\"predict\"",
        "\"index\"",
        "\"index\"",
        "\"__main__\""
    ],
    "variables": [
        "parser",
        "args",
        "content",
        "batches",
        "data",
        "entry",
        "s",
        "s",
        "values",
        "num_corrects",
        "percent",
        "benchmark_data",
        "content",
        "golden_lines",
        "golden_data",
        "benchmark_one_entry",
        "golden_one_entry",
        "benchmark_result",
        "golden_one_entry",
        "app"
    ],
    "comments": [
        "!/usr/bin/env python",
        "",
        "Copyright 2017-present, Facebook, Inc.",
        "All rights reserved.",
        "",
        "This source code is licensed under the license found in the",
        "LICENSE file in the root directory of this source tree.",
        "",
        "this library is to compare the output of the benchmark and the golden output",
        "for image classification tasks, if the golden is 1, expecting the benchmark",
        "is the closest to that.",
        "separate out for debugging purpose"
    ],
    "docstrings": [
        "'''\n        data = []\n        for batch in batches:\n            one_batch = batch.strip().split(',')\n            print(batch)\n            import pdb; pdb.set_trace()\n            one_data = []\n            for item in one_batch:\n                print(item + \" <--\")\n                one_data.append(float(item.strip()))\n            data.append(one_data)\n        '''"
    ],
    "functions": [
        "getData",
        "writeOneResult",
        "writeResult",
        "compare",
        "sort_key"
    ],
    "classes": [
        "OutputCompare"
    ]
}