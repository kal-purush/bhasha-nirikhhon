{
    "identifiers": [
        "random",
        "math",
        "environment",
        "Agent",
        "Environment",
        "planner",
        "RoutePlanner",
        "simulator",
        "Simulator",
        "random",
        "Agent",
        "env",
        "learning",
        "epsilon",
        "alpha",
        "LearningAgent",
        "env",
        "RoutePlanner",
        "env",
        "env",
        "valid_actions",
        "learning",
        "epsilon",
        "alpha",
        "destination",
        "testing",
        "planner",
        "route_to",
        "destination",
        "testing",
        "t",
        "math",
        "fabs",
        "math",
        "cos",
        "alpha",
        "t",
        "planner",
        "next_waypoint",
        "env",
        "sense",
        "env",
        "get_deadline",
        "waypoint",
        "inputs",
        "inputs",
        "inputs",
        "state",
        "state",
        "max",
        "Q",
        "state",
        "key",
        "x",
        "Q",
        "state",
        "x",
        "Q",
        "state",
        "max_action",
        "maxQ",
        "state",
        "state",
        "Q",
        "Q",
        "state",
        "state",
        "planner",
        "next_waypoint",
        "learning",
        "random",
        "random",
        "epsilon",
        "random",
        "choice",
        "valid_actions",
        "get_maxQ",
        "state",
        "act",
        "Q",
        "state",
        "Q",
        "state",
        "act",
        "maxQ",
        "maxQ_list",
        "append",
        "act",
        "random",
        "choice",
        "maxQ_list",
        "action",
        "state",
        "action",
        "reward",
        "learning",
        "Q",
        "state",
        "Q",
        "state",
        "action",
        "alpha",
        "reward",
        "Q",
        "state",
        "action",
        "build_state",
        "createQ",
        "state",
        "choose_action",
        "state",
        "env",
        "act",
        "action",
        "learn",
        "state",
        "action",
        "reward",
        "Environment",
        "env",
        "create_agent",
        "LearningAgent",
        "alpha",
        "env",
        "set_primary_agent",
        "agent",
        "enforce_deadline",
        "Simulator",
        "env",
        "log_metrics",
        "update_delay",
        "optimized",
        "display",
        "sim",
        "run",
        "n_test",
        "tolerance",
        "run"
    ],
    "literals": [
        "'light'",
        "'left'",
        "'oncoming'",
        "'forward'",
        "'left'",
        "'right'",
        "'__main__'"
    ],
    "variables": [
        "planner",
        "valid_actions",
        "learning",
        "Q",
        "epsilon",
        "alpha",
        "t",
        "epsilon",
        "alpha",
        "t",
        "epsilon",
        "waypoint",
        "inputs",
        "deadline",
        "state",
        "max_action",
        "maxQ",
        "state",
        "state",
        "next_waypoint",
        "action",
        "action",
        "maxQ",
        "maxQ_list",
        "action",
        "action",
        "state",
        "action",
        "reward",
        "env",
        "agent",
        "sim"
    ],
    "comments": [
        "Set the agent in the evironment",
        "Create a route planner",
        "The set of valid actions",
        "Set parameters of the learning agent",
        "Whether the agent is expected to learn",
        "Create a Q-table which will be a dictionary of tuples",
        "Random exploration factor",
        "Learning factor",
        "",
        "TO DO ##",
        "",
        "Set any additional class parameters as needed",
        "Select the destination as the new location to route to",
        "",
        "TO DO ##",
        "",
        "Update epsilon using a decay function of your choice",
        "Update additional class parameters as needed",
        "If 'testing' is True, set epsilon and alpha to 0",
        "self.epsilon = self.epsilon - 0.05",
        "Collect data about the environment",
        "The next waypoint",
        "Visual input - intersection light and traffic",
        "Remaining deadline",
        "",
        "TO DO ##",
        "",
        "NOTE : you are not allowed to engineer features outside of the inputs available.",
        "Because the aim of this project is to teach Reinforcement Learning, we have placed",
        "constraints in order for you to learn how to adjust epsilon and alpha, and thus learn about the balance between exploration and exploitation.",
        "With the hand-engineered features, this learning process gets entirely negated.",
        "Set 'state' as a tuple of relevant data for the agent",
        "",
        "TO DO ##",
        "",
        "Calculate the maximum Q-value of all actions for a given state",
        "",
        "TO DO ##",
        "",
        "When learning, check if the 'state' is not in the Q-table",
        "If it is not, create a new dictionary for that state",
        "Then, for each action available, set the initial Q-value to 0.0",
        "Set the agent state and default action",
        "",
        "TO DO ##",
        "",
        "When not learning, choose a random action",
        "When learning, choose a random action with 'epsilon' probability",
        "Otherwise, choose an action with the highest Q-value for the current state",
        "Be sure that when choosing an action with highest Q-value that you randomly select between actions that \"tie\".",
        "",
        "TO DO ##",
        "",
        "When learning, implement the value iteration update rule",
        "Use only the learning rate 'alpha' (do not use the discount factor 'gamma')",
        "Get current state",
        "Create 'state' in Q-table",
        "Choose an action",
        "Receive a reward",
        "Q-learn",
        "",
        "Create the environment",
        "Flags:",
        "verbose     - set to True to display additional output from the simulation",
        "num_dummies - discrete number of dummy agents in the environment, default is 100",
        "grid_size   - discrete number of intersections (columns, rows), default is (8, 6)",
        "",
        "Create the driving agent",
        "Flags:",
        "learning   - set to True to force the driving agent to use Q-learning",
        "",
        "Follow the driving agent",
        "Flags:",
        "enforce_deadline - set to True to enforce a deadline metric",
        "",
        "Create the simulation",
        "Flags:",
        "update_delay - continuous time (in seconds) between actions, default is 2.0 seconds",
        "display      - set to False to disable the GUI if PyGame is enabled",
        "log_metrics  - set to True to log trial and simulation results to /logs",
        "optimized    - set to True to change the default log file name",
        "",
        "Run the simulator",
        "Flags:",
        "tolerance  - epsilon tolerance before beginning testing, default is 0.05",
        "n_test     - discrete number of testing trials to perform, default is 0"
    ],
    "docstrings": [
        "\"\"\" An agent that learns to drive in the Smartcab world.\n        This is the object you will be modifying. \"\"\"",
        "\"\"\" The reset function is called at the beginning of each trial.\n            'testing' is set to True if testing trials are being used\n            once training trials have completed. \"\"\"",
        "\"\"\" The build_state function is called when the agent requests data from the \n            environment. The next waypoint, the intersection inputs, and the deadline \n            are all features available to the agent. \"\"\"",
        "\"\"\" The get_maxQ function is called when the agent is asked to find the\n            maximum Q-value of all actions based on the 'state' the smartcab is in. \"\"\"",
        "\"\"\" The createQ function is called when a state is generated by the agent. \"\"\"",
        "\"\"\" The choose_action function is called when the agent is asked to choose\n            which action to take, based on the 'state' the smartcab is in. \"\"\"",
        "\"\"\" The learn function is called after the agent completes an action and\n            receives a reward. This function does not consider future rewards \n            when conducting learning. \"\"\"",
        "\"\"\" The update function is called when a time step is completed in the \n            environment for a given trial. This function will build the agent\n            state, choose an action, receive a reward, and learn if enabled. \"\"\"",
        "\"\"\" Driving function for running the simulation. \n        Press ESC to close the simulation, or [SPACE] to pause the simulation. \"\"\"",
        "* epsilon - continuous value for the exploration factor, default is 1",
        "* alpha   - continuous value for the learning rate, default is 0.5"
    ],
    "functions": [
        "reset",
        "build_state",
        "get_maxQ",
        "createQ",
        "choose_action",
        "learn",
        "update",
        "run"
    ],
    "classes": [
        "LearningAgent"
    ]
}