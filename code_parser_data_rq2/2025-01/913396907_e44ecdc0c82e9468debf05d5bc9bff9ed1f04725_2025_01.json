{
    "identifiers": [
        "math",
        "dataclasses",
        "dataclass",
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "x",
        "x",
        "torch",
        "tanh",
        "math",
        "sqrt",
        "math",
        "pi",
        "x",
        "torch",
        "pow",
        "x",
        "nn",
        "Module",
        "repr_dim",
        "nhead",
        "nlayers",
        "use_buffer_token",
        "nn",
        "TransformerDecoder",
        "nn",
        "TransformerDecoderLayer",
        "d_model",
        "repr_dim",
        "nhead",
        "nhead",
        "dim_feedforward",
        "repr_dim",
        "batch_first",
        "num_layers",
        "nlayers",
        "use_buffer_token",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "repr_dim",
        "use_buffer_token",
        "feat",
        "cond",
        "use_buffer_token",
        "feat",
        "size",
        "buffer_token",
        "expand",
        "batch_size",
        "torch",
        "cat",
        "buffer_token",
        "cond",
        "dim",
        "tf_decoder",
        "feat",
        "cond_with_buffer",
        "tf_decoder",
        "feat",
        "cond",
        "nn",
        "Module",
        "config",
        "config",
        "n_embd",
        "config",
        "n_head",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "n_embd",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "n_embd",
        "nn",
        "Dropout",
        "config",
        "dropout",
        "nn",
        "Dropout",
        "config",
        "dropout",
        "config",
        "n_head",
        "config",
        "n_embd",
        "n_embd",
        "n_head",
        "config",
        "max_cache_len",
        "nn",
        "Conv1d",
        "in_channels",
        "head_dim",
        "out_channels",
        "head_dim",
        "kernel_size",
        "config",
        "compress_kernel_size",
        "stride",
        "config",
        "compress_stride",
        "padding",
        "config",
        "compress_padding",
        "groups",
        "head_dim",
        "bias",
        "nn",
        "Conv1d",
        "in_channels",
        "head_dim",
        "out_channels",
        "head_dim",
        "kernel_size",
        "config",
        "compress_kernel_size",
        "stride",
        "config",
        "compress_stride",
        "padding",
        "config",
        "compress_padding",
        "groups",
        "head_dim",
        "bias",
        "nn",
        "LayerNorm",
        "head_dim",
        "nn",
        "LayerNorm",
        "head_dim",
        "batch_size",
        "next",
        "parameters",
        "device",
        "max_cache_length",
        "num_tokens_per_time_step",
        "torch",
        "zeros",
        "batch_size",
        "n_head",
        "max_cache_tokens",
        "head_dim",
        "device",
        "device",
        "torch",
        "zeros",
        "batch_size",
        "n_head",
        "max_cache_tokens",
        "head_dim",
        "device",
        "device",
        "torch",
        "zeros",
        "batch_size",
        "dtype",
        "torch",
        "device",
        "device",
        "x",
        "attn_mask",
        "x",
        "size",
        "x",
        "device",
        "T_tokens",
        "cache_initialized",
        "max_cache_length",
        "T_tokens",
        "torch",
        "zeros",
        "B",
        "n_head",
        "max_cache_tokens",
        "head_dim",
        "device",
        "device",
        "torch",
        "zeros",
        "B",
        "n_head",
        "max_cache_tokens",
        "head_dim",
        "device",
        "device",
        "torch",
        "zeros",
        "B",
        "dtype",
        "torch",
        "device",
        "device",
        "c_attn",
        "x",
        "split",
        "n_embd",
        "dim",
        "q",
        "view",
        "B",
        "T_tokens",
        "n_head",
        "head_dim",
        "transpose",
        "k",
        "view",
        "B",
        "T_tokens",
        "n_head",
        "head_dim",
        "transpose",
        "v",
        "view",
        "B",
        "T_tokens",
        "n_head",
        "head_dim",
        "transpose",
        "cache_lengths",
        "cached_k",
        "cache_lengths",
        "cached_v",
        "cache_lengths",
        "prev_k",
        "permute",
        "reshape",
        "B",
        "n_head",
        "head_dim",
        "prev_v",
        "permute",
        "reshape",
        "B",
        "n_head",
        "head_dim",
        "compress_k",
        "prev_k_reshaped",
        "compress_v",
        "prev_v_reshaped",
        "compressed_prev_k",
        "reshape",
        "B",
        "n_head",
        "head_dim",
        "permute",
        "compressed_prev_v",
        "reshape",
        "B",
        "n_head",
        "head_dim",
        "permute",
        "norm_compress_k",
        "compressed_prev_k",
        "norm_compress_v",
        "compressed_prev_v",
        "compressed_prev_k",
        "detach",
        "compressed_prev_v",
        "detach",
        "torch",
        "cat",
        "compressed_prev_k",
        "k",
        "dim",
        "torch",
        "cat",
        "compressed_prev_v",
        "v",
        "dim",
        "q",
        "size",
        "k",
        "size",
        "torch",
        "tril",
        "torch",
        "ones",
        "T_q",
        "T_k",
        "device",
        "device",
        "view",
        "T_q",
        "T_k",
        "attn_mask",
        "attn_mask",
        "T_k",
        "causal_mask",
        "causal_mask",
        "q",
        "k",
        "transpose",
        "math",
        "sqrt",
        "head_dim",
        "att",
        "masked_fill",
        "attn_mask",
        "F",
        "softmax",
        "att",
        "dim",
        "attn_dropout",
        "att",
        "att",
        "v",
        "y",
        "transpose",
        "contiguous",
        "view",
        "B",
        "T_q",
        "n_embd",
        "resid_dropout",
        "c_proj",
        "y",
        "k",
        "size",
        "compressed_prev_k",
        "size",
        "cache_lengths",
        "k",
        "current_kv_len",
        "v",
        "current_kv_len",
        "i",
        "B",
        "cache_lengths",
        "i",
        "current_kv_len",
        "cached_k",
        "size",
        "required_capacity",
        "cache_capacity",
        "required_capacity",
        "cache_capacity",
        "overflow",
        "cache_lengths",
        "i",
        "cached_k",
        "cache_lengths",
        "i",
        "overflow",
        "cached_k",
        "i",
        "overflow",
        "cache_lengths",
        "i",
        "cached_v",
        "cache_lengths",
        "i",
        "overflow",
        "cached_v",
        "i",
        "overflow",
        "cache_lengths",
        "i",
        "cache_lengths",
        "i",
        "overflow",
        "cache_lengths",
        "cache_lengths",
        "i",
        "item",
        "start",
        "current_kv_len",
        "cached_k",
        "start",
        "end",
        "current_k",
        "i",
        "cached_v",
        "start",
        "end",
        "current_v",
        "i",
        "cache_lengths",
        "i",
        "current_kv_len",
        "y",
        "nn",
        "Module",
        "config",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "n_embd",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "n_embd",
        "nn",
        "Dropout",
        "config",
        "dropout",
        "x",
        "c_fc",
        "x",
        "new_gelu",
        "x",
        "c_proj",
        "x",
        "dropout",
        "x",
        "x",
        "nn",
        "Module",
        "config",
        "nn",
        "LayerNorm",
        "config",
        "n_embd",
        "CausalSelfAttention",
        "config",
        "nn",
        "LayerNorm",
        "config",
        "n_embd",
        "MLP",
        "config",
        "x",
        "mask",
        "x",
        "attn",
        "ln_1",
        "x",
        "attn_mask",
        "mask",
        "x",
        "mlp",
        "ln_2",
        "x",
        "x",
        "dataclass",
        "dataclass",
        "nn",
        "Module",
        "config",
        "config",
        "input_dim",
        "config",
        "output_dim",
        "config",
        "block_size",
        "config",
        "nn",
        "ModuleDict",
        "wte",
        "nn",
        "Linear",
        "config",
        "input_dim",
        "config",
        "n_embd",
        "wpe",
        "nn",
        "Embedding",
        "config",
        "block_size",
        "config",
        "n_embd",
        "drop",
        "nn",
        "Dropout",
        "config",
        "dropout",
        "h",
        "nn",
        "ModuleList",
        "Block",
        "config",
        "_",
        "config",
        "n_layer",
        "ln_f",
        "nn",
        "LayerNorm",
        "config",
        "n_embd",
        "nn",
        "Linear",
        "config",
        "n_embd",
        "config",
        "output_dim",
        "bias",
        "apply",
        "_init_weights",
        "pn",
        "p",
        "named_parameters",
        "pn",
        "endswith",
        "torch",
        "nn",
        "init",
        "normal_",
        "p",
        "mean",
        "std",
        "math",
        "sqrt",
        "config",
        "n_layer",
        "sum",
        "p",
        "numel",
        "p",
        "parameters",
        "n_params",
        "input",
        "targets",
        "mask",
        "input",
        "device",
        "input",
        "size",
        "t",
        "config",
        "block_size",
        "t",
        "config",
        "block_size",
        "torch",
        "arange",
        "t",
        "dtype",
        "torch",
        "device",
        "device",
        "unsqueeze",
        "transformer",
        "wte",
        "input",
        "transformer",
        "wpe",
        "pos",
        "transformer",
        "drop",
        "tok_emb",
        "pos_emb",
        "block",
        "transformer",
        "h",
        "block",
        "x",
        "mask",
        "mask",
        "transformer",
        "ln_f",
        "x",
        "lm_head",
        "x",
        "logits",
        "isinstance",
        "nn",
        "Linear",
        "torch",
        "nn",
        "init",
        "normal_",
        "weight",
        "mean",
        "std",
        "bias",
        "torch",
        "nn",
        "init",
        "zeros_",
        "bias",
        "isinstance",
        "nn",
        "Embedding",
        "torch",
        "nn",
        "init",
        "normal_",
        "weight",
        "mean",
        "std",
        "isinstance",
        "nn",
        "LayerNorm",
        "torch",
        "nn",
        "init",
        "zeros_",
        "bias",
        "torch",
        "nn",
        "init",
        "ones_",
        "weight",
        "block_size",
        "block_size",
        "config",
        "block_size",
        "config",
        "block_size",
        "transformer",
        "wpe",
        "nn",
        "Parameter",
        "transformer",
        "wpe",
        "weight",
        "block_size",
        "block",
        "transformer",
        "h",
        "block",
        "attn",
        "block",
        "attn",
        "bias",
        "block_size",
        "block_size",
        "weight_decay",
        "learning_rate",
        "betas",
        "torch",
        "nn",
        "Linear",
        "torch",
        "nn",
        "LayerNorm",
        "torch",
        "nn",
        "Embedding",
        "mn",
        "m",
        "named_modules",
        "pn",
        "p",
        "m",
        "named_parameters",
        "mn",
        "pn",
        "mn",
        "pn",
        "pn",
        "endswith",
        "no_decay",
        "add",
        "fpn",
        "pn",
        "endswith",
        "isinstance",
        "m",
        "whitelist_weight_modules",
        "decay",
        "add",
        "fpn",
        "pn",
        "endswith",
        "isinstance",
        "m",
        "blacklist_weight_modules",
        "no_decay",
        "add",
        "fpn",
        "pn",
        "p",
        "pn",
        "p",
        "named_parameters",
        "decay",
        "no_decay",
        "decay",
        "no_decay",
        "len",
        "inter_params",
        "inter_params",
        "len",
        "param_dict",
        "keys",
        "union_params",
        "param_dict",
        "keys",
        "union_params",
        "param_dict",
        "pn",
        "pn",
        "sorted",
        "decay",
        "weight_decay",
        "param_dict",
        "pn",
        "pn",
        "sorted",
        "no_decay",
        "torch",
        "optim",
        "Adam",
        "optim_groups",
        "lr",
        "learning_rate",
        "betas",
        "betas",
        "optimizer",
        "block",
        "transformer",
        "h",
        "block",
        "attn",
        "reset_cache"
    ],
    "literals": [
        "\"-inf\"",
        "\"c_proj.weight\"",
        "\"number of parameters: %.2fM\"",
        "f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"",
        "\"%s.%s\"",
        "\"bias\"",
        "\"weight\"",
        "\"weight\"",
        "\"parameters %s made it into both decay/no_decay sets!\"",
        "\"parameters %s were not separated into either decay/no_decay set!\"",
        "\"params\"",
        "\"weight_decay\"",
        "\"params\"",
        "\"weight_decay\""
    ],
    "variables": [
        "tf_decoder",
        "buffer_token",
        "use_buffer_token",
        "batch_size",
        "buffer_token",
        "cond_with_buffer",
        "c_attn",
        "c_proj",
        "attn_dropout",
        "resid_dropout",
        "n_head",
        "n_embd",
        "head_dim",
        "max_cache_length",
        "cache_initialized",
        "compress_k",
        "compress_v",
        "norm_compress_k",
        "norm_compress_v",
        "device",
        "max_cache_tokens",
        "cached_k",
        "cached_v",
        "cache_lengths",
        "cache_initialized",
        "B",
        "T_tokens",
        "C",
        "device",
        "num_tokens_per_time_step",
        "max_cache_tokens",
        "cached_k",
        "cached_v",
        "cache_lengths",
        "cache_initialized",
        "q",
        "k",
        "v",
        "q",
        "k",
        "v",
        "prev_k",
        "prev_v",
        "prev_k_reshaped",
        "prev_v_reshaped",
        "compressed_prev_k",
        "compressed_prev_v",
        "compressed_prev_k",
        "compressed_prev_v",
        "compressed_prev_k",
        "compressed_prev_v",
        "compressed_prev_k",
        "compressed_prev_v",
        "k",
        "v",
        "T_q",
        "T_k",
        "causal_mask",
        "attn_mask",
        "attn_mask",
        "att",
        "att",
        "att",
        "att",
        "y",
        "y",
        "y",
        "current_kv_len",
        "current_k",
        "current_v",
        "required_capacity",
        "cache_capacity",
        "overflow",
        "i",
        "i",
        "i",
        "start",
        "end",
        "i",
        "i",
        "c_fc",
        "c_proj",
        "dropout",
        "x",
        "x",
        "x",
        "x",
        "ln_1",
        "attn",
        "ln_2",
        "mlp",
        "x",
        "x",
        "block_size",
        "input_dim",
        "output_dim",
        "n_layer",
        "n_head",
        "n_embd",
        "dropout",
        "max_cache_len",
        "compress_kernel_size",
        "compress_stride",
        "compress_padding",
        "config",
        "transformer",
        "lm_head",
        "n_params",
        "device",
        "b",
        "t",
        "d",
        "pos",
        "tok_emb",
        "pos_emb",
        "x",
        "x",
        "x",
        "logits",
        "block_size",
        "weight",
        "bias",
        "decay",
        "no_decay",
        "whitelist_weight_modules",
        "blacklist_weight_modules",
        "fpn",
        "param_dict",
        "inter_params",
        "union_params",
        "optim_groups",
        "optimizer"
    ],
    "comments": [
        "@torch.jit.script # good to enable when not using torch.compile, disable when using (our default)",
        "add buffer token to the beginning of the sequence",
        "Key, query, value projections for all heads",
        "Output projection",
        "Regularization",
        "Head size",
        "Initialize cache for keys and values",
        "Maximum number of time steps to cache",
        "Flag to check if cache is initialized",
        "Compression modules",
        "Depthwise convolution",
        "Initialize fixed-size cache tensors",
        "Total tokens in cache",
        "x: (B, T_tokens, C)",
        "Number of tokens per time step",
        "if not self.cache_initialized or (self.cached_k is not None and self.cached_k.size(0) != B):",
        "Initialize fixed-size cache tensors",
        "Compute query, key, and value projections",
        "(B, nh, T_tokens, hs)",
        "Concatenate cached keys and values",
        "Previous uncompressed keys and values",
        "(B, nh, L_cache, hs)",
        "Compress the previous keys and values",
        "Reshape for convolution: (B * nh, hs, L_cache)",
        "Compress",
        "(B * nh, hs, L_comp)",
        "Reshape back to (B, nh, L_comp, hs)",
        "Apply layer normalization",
        "Detach compressed keys and values to prevent gradients",
        "Concatenate compressed cached keys and values with current k and v",
        "(B, nh, L_total, hs)",
        "No cached keys and values",
        "k and v remain as they are",
        "Update attention mask",
        "Compute attention scores",
        "Attention output",
        "(B, nh, T_q, hs)",
        "(B, T_q, C)",
        "Output projection",
        "Update cache with current uncompressed keys and values",
        "(B, nh, T_tokens, hs)",
        "(B, nh, T_tokens, hs)",
        "Remove oldest entries to make space",
        "Shift cache to remove oldest entries",
        "Cache is too small, reset cache",
        "Update cache",
        "class GPTConfig:",
        "block_size: int = 1024",
        "input_dim: int = 256",
        "output_dim: int = 256",
        "n_layer: int = 12",
        "n_head: int = 12",
        "n_embd: int = 768",
        "dropout: float = 0.1",
        "Maximum cache length (number of previous frames)",
        "init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper",
        "report number of parameters",
        "shape (1, t)",
        "forward the GPT model itself",
        "token embeddings of shape (b, t, n_embd)",
        "position embeddings of shape (1, t, n_embd)",
        "separate out all parameters to those that will and won't experience regularizing weight decay",
        "full param name",
        "all biases will not be decayed",
        "weights of whitelist modules will be weight decayed",
        "weights of blacklist modules will NOT be weight decayed",
        "validate that we considered every parameter",
        "create the pytorch optimizer object",
        "optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)",
        "Iterate over all transformer blocks and reset their caches"
    ],
    "docstrings": [
        "\"\"\"\nAn adaptation of Andrej Karpathy's nanoGPT implementation in PyTorch.\nOriginal source: https://github.com/karpathy/nanoGPT\n\nOriginal License:\nMIT License\n\nCopyright (c) 2022 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nOriginal comments:\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"",
        "\"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"",
        "\"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\""
    ],
    "functions": [
        "new_gelu",
        "forward",
        "reset_cache",
        "forward",
        "forward",
        "forward",
        "forward",
        "_init_weights",
        "crop_block_size",
        "configure_optimizers",
        "reset_cache"
    ],
    "classes": [
        "CrossAttention",
        "CausalSelfAttention",
        "MLP",
        "Block",
        "GPTConfig",
        "GPT"
    ]
}