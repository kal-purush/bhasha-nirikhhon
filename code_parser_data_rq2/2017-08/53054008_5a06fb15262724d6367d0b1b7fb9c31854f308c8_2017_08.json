{
    "identifiers": [
        "cPickle",
        "pickle",
        "write_functions",
        "w",
        "nltk",
        "corpus",
        "pickle",
        "load",
        "open",
        "filename",
        "len",
        "unprocessed_sentences",
        "len",
        "unprocessed_sentences",
        "nltk",
        "corpus",
        "stopwords",
        "words",
        "stopwords",
        "extend",
        "punctuation",
        "stopwords",
        "append",
        "dataset",
        "samplesize",
        "random",
        "random",
        "sample",
        "unprocessed_sentences",
        "samplesize",
        "i",
        "rand_list",
        "items",
        "append",
        "i",
        "replace",
        "decode",
        "items",
        "tokenset_a",
        "tokenset_b",
        "threshold",
        "len",
        "tokenset_a",
        "intersection",
        "tokenset_b",
        "len",
        "tokenset_a",
        "union",
        "tokenset_b",
        "ratio",
        "threshold",
        "sentence",
        "token",
        "lower",
        "strip",
        "punctuation",
        "token",
        "nltk",
        "word_tokenize",
        "sentence",
        "token",
        "lower",
        "strip",
        "punctuation",
        "stopwords",
        "tokenset",
        "i",
        "unprocessed_sentences",
        "tokensets",
        "append",
        "generate_bow",
        "i",
        "i",
        "len",
        "unprocessed_sentences",
        "item",
        "i",
        "len",
        "unprocessed_sentences",
        "is_ci_token_stopword_set_match",
        "tokensets",
        "i",
        "tokensets",
        "item",
        "unique_item",
        "keep_items",
        "append",
        "unprocessed_sentences",
        "i",
        "i",
        "i",
        "len",
        "unprocessed_sentences",
        "len",
        "keep_items",
        "w",
        "write_to_pickle",
        "keep_items"
    ],
    "literals": [
        "\"raw_datasets/input_file_name.pickle\"",
        "\"File succesfully imported. The file contains %d sentences.\"",
        "\"%d lines were imported and are ready for deduplication!\"",
        "'english'",
        "''",
        "'\\n'",
        "''",
        "'utf8'",
        "'replace'",
        "\"Finished creating bow representations.\"",
        "\"Processed sentences:\\t %d/%d\"",
        "\"Amount of kept items: %d\"",
        "\"output_file_name\""
    ],
    "variables": [
        "filename",
        "unprocessed_sentences",
        "stopwords",
        "items",
        "rand_list",
        "ratio",
        "tokenset",
        "tokensets",
        "keep_items",
        "unique_item",
        "unique_item"
    ],
    "comments": [
        "-*- coding: utf-8 -*-",
        "Imports",
        "Import the sentences that need processing from a pickle file",
        "# Uncomment in case more pickle files need to be imported",
        "filename2 = 'filename2.pickle'",
        "unprocessed_sentences2 = pickle.load(open(filename2))",
        "print \"File succesfully imported. The file contains %d sentences.\" %len(unprocessed_sentences2)",
        "for i in unprocessed_sentences2:",
        "unprocessed_sentences.append(i)",
        "Get default English stopwords and extend with punctuation",
        "Use if a random set needs to be drawn",
        "Generate all BoW's",
        "Compare all sentences s for similarity with the sentences on index > s",
        "Used to track progress of the deduplication",
        "Write away processed data"
    ],
    "docstrings": [
        "\"\"\"Check if a and b are matches, using jaccard coefficient.\"\"\""
    ],
    "functions": [
        "draw_from_list_randomly",
        "is_ci_token_stopword_set_match",
        "generate_bow"
    ],
    "classes": []
}