{
    "identifiers": [
        "tokenizers",
        "Tokenizer",
        "tokenizers",
        "models",
        "BPE",
        "tokenizers",
        "trainers",
        "BpeTrainer",
        "tokenizers",
        "pre_tokenizers",
        "Whitespace",
        "tokenizers",
        "processors",
        "TemplateProcessing",
        "file_name",
        "open",
        "file_name",
        "encoding",
        "newline",
        "data_file",
        "line",
        "data_file",
        "line",
        "line",
        "line",
        "line",
        "line",
        "rstrip",
        "split",
        "len",
        "line_text",
        "append",
        "line_text",
        "lines",
        "append",
        "line_text",
        "lines",
        "file_name",
        "read_data",
        "file_name",
        "Tokenizer",
        "BPE",
        "unk_token",
        "tokenizer",
        "enable_padding",
        "direction",
        "pad_id",
        "pad_token",
        "length",
        "WINDOW_SIZE",
        "BpeTrainer",
        "special_tokens",
        "Whitespace",
        "tokenizer",
        "train",
        "file_name",
        "trainer",
        "TemplateProcessing",
        "single",
        "pair",
        "special_tokens",
        "tokenizer",
        "token_to_id",
        "tokenizer",
        "token_to_id",
        "tokenizer",
        "token_to_id",
        "i",
        "len",
        "lines",
        "tokenizer",
        "encode",
        "chars",
        "i",
        "lines",
        "i",
        "all_tokens",
        "append",
        "output",
        "tokens",
        "all_ids",
        "append",
        "output",
        "ids",
        "get_data"
    ],
    "literals": [
        "'rt'",
        "'latin'",
        "\"\"",
        "'['",
        "'('",
        "\"THE ONE\"",
        "\"Written by:\"",
        "'\\n'",
        "\": \"",
        "\"[UNK]\"",
        "\"right\"",
        "'[PAD]'",
        "\"[UNK]\"",
        "\"[BOS]\"",
        "\"[EOS]\"",
        "\"[DELIM]\"",
        "\"[PAD]\"",
        "\"[BOS] $A [EOS]\"",
        "\"[BOS] $A [DELIM] $B:1 [EOS]:1\"",
        "\"[BOS]\"",
        "\"[BOS]\"",
        "\"[EOS]\"",
        "\"[EOS]\"",
        "\"[DELIM]\"",
        "\"[DELIM]\"",
        "\"../data/friends_transcript.txt\""
    ],
    "variables": [
        "WINDOW_SIZE",
        "lines",
        "line_text",
        "chars",
        "lines",
        "tokenizer",
        "trainer",
        "tokenizer",
        "pre_tokenizer",
        "tokenizer",
        "post_processor",
        "all_tokens",
        "all_ids",
        "output"
    ],
    "comments": [],
    "docstrings": [
        "\"\"\"\n  DO NOT CHANGE\n  Load text data from file\n  :param file_name:  string, name of data file\n  :return: list of sentences, each a list of words split on whitespace\n  \"\"\""
    ],
    "functions": [
        "read_data",
        "get_data"
    ],
    "classes": []
}