{
    "identifiers": [
        "utils",
        "typing",
        "Tuple",
        "random",
        "collections",
        "Counter",
        "x",
        "x_tok",
        "x_indexed",
        "y",
        "y_tok",
        "y_indexed",
        "x",
        "x_tok",
        "x_indexed",
        "y",
        "y_tok",
        "y_indexed",
        "join",
        "x_tok",
        "join",
        "y_tok",
        "repr",
        "x_indexed",
        "repr",
        "y_indexed",
        "__repr__",
        "example",
        "Example",
        "p",
        "y_toks",
        "example",
        "p",
        "y_toks",
        "y_toks",
        "p",
        "__str__",
        "train_path",
        "dev_path",
        "test_path",
        "domain",
        "Tuple",
        "Tuple",
        "Tuple",
        "load_dataset",
        "train_path",
        "domain",
        "domain",
        "load_dataset",
        "dev_path",
        "domain",
        "domain",
        "load_dataset",
        "test_path",
        "domain",
        "domain",
        "train_raw",
        "dev_raw",
        "test_raw",
        "filename",
        "domain",
        "Tuple",
        "open",
        "filename",
        "f",
        "line",
        "f",
        "line",
        "rstrip",
        "split",
        "domain",
        "geoquery_preprocess_lf",
        "y",
        "dataset",
        "append",
        "x",
        "y",
        "len",
        "dataset",
        "filename",
        "dataset",
        "x",
        "x",
        "split",
        "x_tok",
        "indexer",
        "Indexer",
        "indexer",
        "index_of",
        "xi",
        "indexer",
        "index_of",
        "xi",
        "indexer",
        "index_of",
        "UNK_SYMBOL",
        "xi",
        "x_tok",
        "data",
        "input_indexer",
        "Indexer",
        "output_indexer",
        "Indexer",
        "example_len_limit",
        "x",
        "y",
        "data",
        "tokenize",
        "x",
        "tokenize",
        "y",
        "example_len_limit",
        "data_indexed",
        "append",
        "Example",
        "x",
        "x_tok",
        "index",
        "x_tok",
        "input_indexer",
        "y",
        "y_tok",
        "index",
        "y_tok",
        "output_indexer",
        "output_indexer",
        "index_of",
        "EOS_SYMBOL",
        "data_indexed",
        "train_data",
        "dev_data",
        "test_data",
        "example_len_limit",
        "unk_threshold",
        "Example",
        "Example",
        "Example",
        "Indexer",
        "Indexer",
        "Counter",
        "x",
        "y",
        "train_data",
        "word",
        "tokenize",
        "x",
        "input_word_counts",
        "word",
        "Indexer",
        "Indexer",
        "input_indexer",
        "add_and_get_index",
        "PAD_SYMBOL",
        "input_indexer",
        "add_and_get_index",
        "UNK_SYMBOL",
        "output_indexer",
        "add_and_get_index",
        "PAD_SYMBOL",
        "output_indexer",
        "add_and_get_index",
        "SOS_SYMBOL",
        "output_indexer",
        "add_and_get_index",
        "EOS_SYMBOL",
        "word",
        "input_word_counts",
        "keys",
        "input_word_counts",
        "word",
        "unk_threshold",
        "input_indexer",
        "add_and_get_index",
        "word",
        "x",
        "y",
        "train_data",
        "y_tok",
        "tokenize",
        "y",
        "output_indexer",
        "add_and_get_index",
        "y_tok",
        "index_data",
        "train_data",
        "input_indexer",
        "output_indexer",
        "example_len_limit",
        "index_data",
        "dev_data",
        "input_indexer",
        "output_indexer",
        "example_len_limit",
        "index_data",
        "test_data",
        "input_indexer",
        "output_indexer",
        "example_len_limit",
        "train_data_indexed",
        "dev_data_indexed",
        "test_data_indexed",
        "input_indexer",
        "output_indexer",
        "test_data",
        "selected_derivs",
        "denotation_correct",
        "example_freq",
        "print_output",
        "i",
        "ex",
        "test_data",
        "selected_derivs",
        "i",
        "y_toks",
        "i",
        "len",
        "selected_derivs",
        "print_output",
        "i",
        "example_freq",
        "example_freq",
        "i",
        "ex",
        "x",
        "ex",
        "y_tok",
        "pred_y_toks",
        "join",
        "pred_y_toks",
        "y_pred",
        "join",
        "ex",
        "y_tok",
        "num_exact_match",
        "num_tokens_correct",
        "sum",
        "a",
        "b",
        "a",
        "b",
        "pred_y_toks",
        "ex",
        "y_tok",
        "total_tokens",
        "len",
        "ex",
        "y_tok",
        "denotation_correct",
        "i",
        "num_denotation_match",
        "print_output",
        "render_ratio",
        "num_exact_match",
        "len",
        "test_data",
        "render_ratio",
        "num_tokens_correct",
        "total_tokens",
        "render_ratio",
        "num_denotation_match",
        "len",
        "test_data",
        "num_exact_match",
        "len",
        "test_data",
        "num_tokens_correct",
        "total_tokens",
        "num_denotation_match",
        "len",
        "test_data",
        "numer",
        "denom",
        "numer",
        "denom",
        "numer",
        "denom",
        "lf",
        "lf",
        "split",
        "w",
        "toks",
        "w",
        "isalpha",
        "len",
        "w",
        "w",
        "cur_vars",
        "len",
        "cur_vars",
        "cur_vars",
        "index",
        "w",
        "new_toks",
        "append",
        "ind_from_end",
        "cur_vars",
        "append",
        "w",
        "new_toks",
        "append",
        "new_toks",
        "append",
        "w",
        "join",
        "new_toks"
    ],
    "literals": [
        "\" \"",
        "\" => \"",
        "\" \"",
        "\"\\n   indexed as: \"",
        "\" => \"",
        "\"%s (%s)\"",
        "\"<PAD>\"",
        "\"<UNK>\"",
        "\"<SOS>\"",
        "\"<EOS>\"",
        "\"geo\"",
        "'\\n'",
        "'\\t'",
        "\"geo\"",
        "\"Loaded %i exs from file %s\"",
        "\"\"",
        "'Example %d'",
        "'  x      = \"%s\"'",
        "'  y_tok  = \"%s\"'",
        "'  y_pred = \"%s\"'",
        "' '",
        "' '",
        "\"Exact logical form matches: %s\"",
        "\"Token-level accuracy: %s\"",
        "\"Denotation matches: %s\"",
        "\"%i / %i = %.3f\"",
        "' '",
        "'V%d'",
        "'NV'",
        "' '"
    ],
    "variables": [
        "x",
        "x_tok",
        "x_indexed",
        "y",
        "y_tok",
        "y_indexed",
        "example",
        "p",
        "y_toks",
        "PAD_SYMBOL",
        "UNK_SYMBOL",
        "SOS_SYMBOL",
        "EOS_SYMBOL",
        "train_raw",
        "dev_raw",
        "test_raw",
        "dataset",
        "x",
        "y",
        "y",
        "data_indexed",
        "x_tok",
        "y_tok",
        "input_word_counts",
        "input_indexer",
        "output_indexer",
        "train_data_indexed",
        "dev_data_indexed",
        "test_data_indexed",
        "num_exact_match",
        "num_tokens_correct",
        "num_denotation_match",
        "total_tokens",
        "pred_y_toks",
        "y_pred",
        "cur_vars",
        "toks",
        "new_toks",
        "ind_from_end"
    ],
    "comments": [
        "",
        "Geoquery features some additional preprocessing of the logical form",
        "Count words and build the indexers",
        "Reserve 0 for the pad symbol for convenience",
        "Index all input words above the UNK threshold",
        "Index all output tokens in train",
        "Index things",
        "",
        "YOU SHOULD NOT NEED TO LOOK AT THESE FUNCTIONS #",
        "",
        "Compute accuracy metrics",
        "Check exact match",
        "Check position-by-position token correctness",
        "Check correctness of the denotation"
    ],
    "docstrings": [
        "\"\"\"\n    Wrapper class for a single (natural language, logical form) input/output (x/y) pair\n    Attributes:\n        x: the natural language as one string\n        x_tok: tokenized natural language as a list of strings\n        x_indexed: indexed tokens, a list of ints\n        y: the raw logical form as a string\n        y_tok: tokenized logical form, a list of strings\n        y_indexed: indexed logical form, a list of ints\n    \"\"\"",
        "\"\"\"\n    Wrapper for a possible solution returned by the model associated with an Example. Note that y_toks here is a\n    predicted y_toks, and the Example itself contains the gold y_toks.\n    Attributes:\n          example: The underlying Example we're predicting on\n          p: the probability associated with this prediction\n          y_toks: the tokenized output prediction\n    \"\"\"",
        "\"\"\"\n    Reads the training, dev, and test data from the corresponding files.\n    :param train_path:\n    :param dev_path:\n    :param test_path:\n    :param domain: Ignore this parameter\n    :return:\n    \"\"\"",
        "\"\"\"\n    Reads a dataset in from the given file.\n    :param filename:\n    :param domain: Ignore this parameter\n    :return: a list of untokenized, unindexed (natural language, logical form) pairs\n    \"\"\"",
        "\"\"\"\n    :param x: string to tokenize\n    :return: x tokenized with whitespace tokenization\n    \"\"\"",
        "\"\"\"\n    Indexes the given data\n    :param data:\n    :param input_indexer:\n    :param output_indexer:\n    :param example_len_limit:\n    :return:\n    \"\"\"",
        "\"\"\"\n    Indexes train and test datasets where all words occurring less than or equal to unk_threshold times are\n    replaced by UNK tokens.\n    :param train_data:\n    :param dev_data:\n    :param test_data:\n    :param example_len_limit:\n    :param unk_threshold: threshold below which words are replaced with unks. If 0.0, the model doesn't see any\n    UNKs at train time\n    :return:\n\n    example:\n    what are the rivers of montana ? => _answer ( NV , ( _river ( V0 ) , _loc ( V0 , NV ) , _const ( V0 , _stateid ( montana ) ) ) )\n    indexed as: [2, 3, 4, 5, 6, 7, 8] => [3, 4, 5, 6, 4, 7, 4, 8, 9, 6, 10, 4, 8, 6, 5, 9, 6, 11, 4, 8, 6, 12, 4, 13, 9, 9, 9, 9, 2]\n\n    \"\"\"",
        "\"\"\"\n    Prints output and accuracy. YOU SHOULD NOT NEED TO CALL THIS DIRECTLY -- instead call evaluate in main.py, which\n    wraps this.\n    :param test_data:\n    :param selected_derivs:\n    :param denotation_correct:\n    :param example_freq: How often to print output\n    :param print_output: True if we should print the scores, false otherwise (you should never need to set this False)\n    :return: List[float] which is [exact matches, token level accuracy, denotation matches]\n    \"\"\"",
        "\"\"\"\n    Geoquery preprocessing adapted from Jia and Liang. Standardizes variable names with De Brujin indices -- just a\n    smarter way of indexing variables in statements to make parsing easier.\n    :param lf:\n    :return:\n    \"\"\""
    ],
    "functions": [
        "__repr__",
        "__str__",
        "__str__",
        "__repr__",
        "load_datasets",
        "load_dataset",
        "tokenize",
        "index",
        "index_data",
        "index_datasets",
        "print_evaluation_results",
        "render_ratio",
        "geoquery_preprocess_lf"
    ],
    "classes": [
        "Example",
        "Derivation"
    ]
}