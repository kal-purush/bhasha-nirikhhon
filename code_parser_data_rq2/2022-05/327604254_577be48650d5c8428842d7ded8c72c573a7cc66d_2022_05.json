{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "transformers",
        "modeling_outputs",
        "BaseModelOutputWithPoolingAndCrossAttentions",
        "QuestionAnsweringModelOutput",
        "transformers",
        "models",
        "bert",
        "modeling_bert",
        "BertPreTrainedModel",
        "BertEmbeddings",
        "BertEncoder",
        "BertPooler",
        "transformers",
        "models",
        "dpr",
        "modeling_dpr",
        "DPRPretrainedContextEncoder",
        "DPRContextEncoder",
        "BertPreTrainedModel",
        "config",
        "config",
        "config",
        "BertEmbeddings",
        "config",
        "BertEncoder",
        "config",
        "BertPooler",
        "config",
        "nn",
        "Linear",
        "config",
        "hidden_size",
        "init_weights",
        "input_ids",
        "attention_mask",
        "token_type_ids",
        "position_ids",
        "head_mask",
        "inputs_embeds",
        "encoder_hidden_states",
        "encoder_attention_mask",
        "past_key_values",
        "use_cache",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "output_attentions",
        "output_attentions",
        "config",
        "output_attentions",
        "output_hidden_states",
        "output_hidden_states",
        "config",
        "output_hidden_states",
        "return_dict",
        "return_dict",
        "config",
        "use_return_dict",
        "config",
        "is_decoder",
        "use_cache",
        "use_cache",
        "config",
        "use_cache",
        "input_ids",
        "inputs_embeds",
        "ValueError",
        "input_ids",
        "input_ids",
        "size",
        "inputs_embeds",
        "inputs_embeds",
        "size",
        "ValueError",
        "input_shape",
        "input_ids",
        "device",
        "input_ids",
        "inputs_embeds",
        "device",
        "past_key_values",
        "shape",
        "past_key_values",
        "attention_mask",
        "torch",
        "ones",
        "batch_size",
        "seq_length",
        "past_key_values_length",
        "device",
        "device",
        "token_type_ids",
        "hasattr",
        "embeddings",
        "embeddings",
        "token_type_ids",
        "seq_length",
        "buffered_token_type_ids",
        "expand",
        "batch_size",
        "seq_length",
        "buffered_token_type_ids_expanded",
        "torch",
        "zeros",
        "input_shape",
        "dtype",
        "torch",
        "device",
        "device",
        "torch",
        "Tensor",
        "get_extended_attention_mask",
        "attention_mask",
        "input_shape",
        "device",
        "config",
        "is_decoder",
        "encoder_hidden_states",
        "encoder_hidden_states",
        "size",
        "encoder_batch_size",
        "encoder_sequence_length",
        "encoder_attention_mask",
        "torch",
        "ones",
        "encoder_hidden_shape",
        "device",
        "device",
        "invert_attention_mask",
        "encoder_attention_mask",
        "get_head_mask",
        "head_mask",
        "config",
        "num_hidden_layers",
        "embeddings",
        "input_ids",
        "input_ids",
        "position_ids",
        "position_ids",
        "token_type_ids",
        "token_type_ids",
        "inputs_embeds",
        "inputs_embeds",
        "past_key_values_length",
        "past_key_values_length",
        "encoder",
        "embedding_output",
        "attention_mask",
        "extended_attention_mask",
        "head_mask",
        "head_mask",
        "encoder_hidden_states",
        "encoder_hidden_states",
        "encoder_attention_mask",
        "encoder_extended_attention_mask",
        "past_key_values",
        "past_key_values",
        "use_cache",
        "use_cache",
        "output_attentions",
        "output_attentions",
        "output_hidden_states",
        "output_hidden_states",
        "return_dict",
        "return_dict",
        "encoder_outputs",
        "pooler",
        "sequence_output",
        "pooler",
        "span_prediction_head",
        "sequence_output",
        "logits",
        "split",
        "dim",
        "start_logits",
        "squeeze",
        "contiguous",
        "end_logits",
        "squeeze",
        "contiguous",
        "return_dict",
        "sequence_output",
        "pooled_output",
        "encoder_outputs",
        "start_logits",
        "end_logits",
        "encoder_outputs",
        "representation_outputs",
        "span_prediction_outputs",
        "BaseModelOutputWithPoolingAndCrossAttentions",
        "last_hidden_state",
        "sequence_output",
        "pooler_output",
        "pooled_output",
        "past_key_values",
        "encoder_outputs",
        "past_key_values",
        "hidden_states",
        "encoder_outputs",
        "hidden_states",
        "attentions",
        "encoder_outputs",
        "attentions",
        "cross_attentions",
        "encoder_outputs",
        "cross_attentions",
        "QuestionAnsweringModelOutput",
        "start_logits",
        "start_logits",
        "end_logits",
        "end_logits",
        "hidden_states",
        "encoder_outputs",
        "hidden_states",
        "attentions",
        "encoder_outputs",
        "attentions",
        "representation_outputs",
        "span_prediction_outputs"
    ],
    "literals": [
        "r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"",
        "\"You cannot specify both input_ids and inputs_embeds at the same time\"",
        "\"You have to specify either input_ids or inputs_embeds\"",
        "\"token_type_ids\""
    ],
    "variables": [
        "config",
        "embeddings",
        "encoder",
        "representation_head",
        "span_prediction_head",
        "output_attentions",
        "output_hidden_states",
        "return_dict",
        "use_cache",
        "use_cache",
        "input_shape",
        "input_shape",
        "batch_size",
        "seq_length",
        "device",
        "past_key_values_length",
        "attention_mask",
        "buffered_token_type_ids",
        "buffered_token_type_ids_expanded",
        "token_type_ids",
        "token_type_ids",
        "extended_attention_mask",
        "encoder_batch_size",
        "encoder_sequence_length",
        "_",
        "encoder_hidden_shape",
        "encoder_attention_mask",
        "encoder_extended_attention_mask",
        "encoder_extended_attention_mask",
        "head_mask",
        "embedding_output",
        "encoder_outputs",
        "sequence_output",
        "pooled_output",
        "logits",
        "start_logits",
        "end_logits",
        "start_logits",
        "end_logits",
        "representation_outputs",
        "span_prediction_outputs",
        "representation_outputs",
        "span_prediction_outputs"
    ],
    "comments": [
        "Does this work?",
        "past_key_values_length",
        "We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
        "ourselves in which case we just need to make it broadcastable to all heads.",
        "If a 2D or 3D attention mask is provided for the cross-attention",
        "we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]",
        "Prepare head mask if needed",
        "1.0 in head_mask indicate we keep the head",
        "attention_probs has shape bsz x n_heads x N x N",
        "input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]",
        "and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]"
    ],
    "docstrings": [],
    "functions": [
        "forward"
    ],
    "classes": [
        "PretrainRetrievalContextEncoder"
    ]
}