{
    "identifiers": [
        "marquez",
        "spark",
        "agent",
        "lifecycle",
        "plan",
        "java",
        "util",
        "java",
        "util",
        "Arrays",
        "java",
        "util",
        "java",
        "util",
        "java",
        "util",
        "stream",
        "Collectors",
        "marquez",
        "spark",
        "agent",
        "client",
        "LineageEvent",
        "Dataset",
        "org",
        "apache",
        "hadoop",
        "conf",
        "Configuration",
        "org",
        "apache",
        "hadoop",
        "fs",
        "Path",
        "org",
        "apache",
        "hadoop",
        "mapred",
        "FileInputFormat",
        "org",
        "apache",
        "spark",
        "Dependency",
        "org",
        "apache",
        "spark",
        "rdd",
        "HadoopRDD",
        "org",
        "apache",
        "spark",
        "rdd",
        "RDD",
        "org",
        "apache",
        "spark",
        "sql",
        "catalyst",
        "plans",
        "logical",
        "LogicalPlan",
        "org",
        "apache",
        "spark",
        "sql",
        "execution",
        "LogicalRDD",
        "scala",
        "collection",
        "Seq",
        "scala",
        "runtime",
        "AbstractPartialFunction",
        "Override",
        "x",
        "x",
        "findHadoopRdds",
        "x",
        "isEmpty",
        "rdd",
        "root",
        "rdd",
        "rdd",
        "ret",
        "deps",
        "deps",
        "add",
        "root",
        "deps",
        "isEmpty",
        "cur",
        "deps",
        "pop",
        "dependencies",
        "cur",
        "getDependencies",
        "deps",
        "addAll",
        "ScalaConversionUtils",
        "fromSeq",
        "dependencies",
        "stream",
        "Dependency",
        "rdd",
        "collect",
        "Collectors",
        "toList",
        "cur",
        "ret",
        "add",
        "cur",
        "ret",
        "Override",
        "x",
        "logicalRdd",
        "x",
        "hadoopRdds",
        "findHadoopRdds",
        "logicalRdd",
        "hadoopRdds",
        "stream",
        "flatMap",
        "rdd",
        "inputPaths",
        "FileInputFormat",
        "getInputPaths",
        "rdd",
        "getJobConf",
        "hadoopConf",
        "rdd",
        "getConf",
        "Arrays",
        "stream",
        "inputPaths",
        "p",
        "PlanUtils",
        "getDirectoryPath",
        "p",
        "hadoopConf",
        "distinct",
        "p",
        "PlanUtils",
        "getDataset",
        "p",
        "toUri",
        "logicalRdd",
        "schema",
        "collect",
        "Collectors",
        "toList"
    ],
    "literals": [],
    "variables": [],
    "comments": [
        "TODO- refactor this to return a single partitioned dataset based on static",
        "static partitions in the relation"
    ],
    "docstrings": [
        "* {@link LogicalPlan} visitor that attempts to extract {@link Path}s from a {@link HadoopRDD}\n * wrapped in a {@link LogicalRDD}.The logic is mostly the same as the {@link\n * org.apache.spark.sql.execution.datasources.HadoopFsRelation}, but works with {@link RDD}s that\n * are converted to {@link org.apache.spark.sql.Dataset}s."
    ],
    "functions": [
        "isDefinedAt",
        "findHadoopRdds",
        "apply"
    ],
    "classes": [
        "LogicalRDDVisitor"
    ]
}