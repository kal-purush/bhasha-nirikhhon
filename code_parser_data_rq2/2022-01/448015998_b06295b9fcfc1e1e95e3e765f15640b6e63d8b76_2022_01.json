{
    "identifiers": [
        "copy",
        "numpy",
        "np",
        "torch",
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "torch",
        "device",
        "torch",
        "cuda",
        "is_available",
        "nn",
        "Module",
        "state_dim",
        "action_dim",
        "max_action",
        "Actor",
        "action_dim",
        "nn",
        "Linear",
        "state_dim",
        "nn",
        "BatchNorm1d",
        "nn",
        "Linear",
        "nn",
        "BatchNorm1d",
        "nn",
        "Linear",
        "action_dim",
        "max_action",
        "state",
        "wei",
        "F",
        "relu",
        "bn1",
        "l1",
        "state",
        "F",
        "relu",
        "bn2",
        "l2",
        "a",
        "l3",
        "a",
        "np",
        "sqrt",
        "action_dim",
        "j",
        "len",
        "a",
        "a",
        "j",
        "wei",
        "j",
        "ll",
        "a",
        "view",
        "a",
        "size",
        "np",
        "sqrt",
        "action_dim",
        "max_action",
        "F",
        "softmax",
        "a",
        "a",
        "view",
        "a",
        "size",
        "action_dim",
        "a",
        "nn",
        "Module",
        "state_dim",
        "action_dim",
        "Critic",
        "nn",
        "Linear",
        "state_dim",
        "action_dim",
        "nn",
        "Linear",
        "nn",
        "Linear",
        "nn",
        "Linear",
        "state_dim",
        "action_dim",
        "nn",
        "Linear",
        "nn",
        "Linear",
        "state",
        "action",
        "torch",
        "cat",
        "state",
        "action",
        "F",
        "relu",
        "l1",
        "sa",
        "F",
        "relu",
        "l2",
        "q1",
        "l3",
        "q1",
        "F",
        "relu",
        "l4",
        "sa",
        "F",
        "relu",
        "l5",
        "q2",
        "l6",
        "q2",
        "q1",
        "q2",
        "state",
        "action",
        "torch",
        "cat",
        "state",
        "action",
        "F",
        "relu",
        "l1",
        "sa",
        "F",
        "relu",
        "l2",
        "q1",
        "l3",
        "q1",
        "q1",
        "state_dim",
        "action_dim",
        "max_action",
        "discount",
        "tau",
        "policy_noise",
        "noise_clip",
        "policy_freq",
        "Actor",
        "state_dim",
        "action_dim",
        "max_action",
        "to",
        "device",
        "copy",
        "deepcopy",
        "actor",
        "torch",
        "optim",
        "Adam",
        "actor",
        "parameters",
        "lr",
        "Critic",
        "state_dim",
        "action_dim",
        "to",
        "device",
        "copy",
        "deepcopy",
        "critic",
        "torch",
        "optim",
        "Adam",
        "critic",
        "parameters",
        "lr",
        "max_action",
        "discount",
        "tau",
        "policy_noise",
        "noise_clip",
        "policy_freq",
        "state",
        "wei",
        "torch",
        "FloatTensor",
        "state",
        "reshape",
        "to",
        "device",
        "actor",
        "eval",
        "actor",
        "state",
        "wei",
        "cpu",
        "data",
        "numpy",
        "flatten",
        "actor",
        "train",
        "ans",
        "replay_buffer",
        "wei",
        "batch_size",
        "total_it",
        "replay_buffer",
        "sample",
        "batch_size",
        "torch",
        "no_grad",
        "torch",
        "randn_like",
        "action",
        "policy_noise",
        "clamp",
        "noise_clip",
        "noise_clip",
        "actor_target",
        "next_state",
        "wei",
        "noise",
        "clamp",
        "max_action",
        "critic_target",
        "next_state",
        "next_action",
        "torch",
        "min",
        "target_Q1",
        "target_Q2",
        "reward",
        "not_done",
        "discount",
        "target_Q",
        "critic",
        "state",
        "action",
        "F",
        "mse_loss",
        "current_Q1",
        "target_Q",
        "F",
        "mse_loss",
        "current_Q2",
        "target_Q",
        "critic_optimizer",
        "zero_grad",
        "critic_loss",
        "backward",
        "critic_optimizer",
        "step",
        "total_it",
        "policy_freq",
        "critic",
        "Q1",
        "state",
        "actor",
        "state",
        "wei",
        "mean",
        "actor_optimizer",
        "zero_grad",
        "actor_loss",
        "backward",
        "actor_optimizer",
        "step",
        "param",
        "target_param",
        "critic",
        "parameters",
        "critic_target",
        "parameters",
        "target_param",
        "data",
        "copy_",
        "tau",
        "param",
        "data",
        "tau",
        "target_param",
        "data",
        "param",
        "target_param",
        "actor",
        "parameters",
        "actor_target",
        "parameters",
        "target_param",
        "data",
        "copy_",
        "tau",
        "param",
        "data",
        "tau",
        "target_param",
        "data",
        "filename",
        "torch",
        "save",
        "critic",
        "state_dict",
        "filename",
        "torch",
        "save",
        "critic_optimizer",
        "state_dict",
        "filename",
        "torch",
        "save",
        "actor",
        "state_dict",
        "filename",
        "torch",
        "save",
        "actor_optimizer",
        "state_dict",
        "filename",
        "filename",
        "critic",
        "load_state_dict",
        "torch",
        "load",
        "filename",
        "critic_optimizer",
        "load_state_dict",
        "torch",
        "load",
        "filename",
        "copy",
        "deepcopy",
        "critic",
        "actor",
        "load_state_dict",
        "torch",
        "load",
        "filename",
        "actor_optimizer",
        "load_state_dict",
        "torch",
        "load",
        "filename",
        "copy",
        "deepcopy",
        "actor"
    ],
    "literals": [
        "\"cuda\"",
        "\"cpu\"",
        "\"_critic\"",
        "\"_critic_optimizer\"",
        "\"_actor\"",
        "\"_actor_optimizer\"",
        "\"_critic\"",
        "\"_critic_optimizer\"",
        "\"_actor\"",
        "\"_actor_optimizer\""
    ],
    "variables": [
        "device",
        "action_dim",
        "l1",
        "bn1",
        "l2",
        "bn2",
        "l3",
        "max_action",
        "a",
        "a",
        "a",
        "ll",
        "a",
        "j",
        "a",
        "a",
        "a",
        "l1",
        "l2",
        "l3",
        "l4",
        "l5",
        "l6",
        "sa",
        "q1",
        "q1",
        "q1",
        "q2",
        "q2",
        "q2",
        "sa",
        "q1",
        "q1",
        "q1",
        "actor",
        "actor_target",
        "actor_optimizer",
        "critic",
        "critic_target",
        "critic_optimizer",
        "max_action",
        "discount",
        "tau",
        "policy_noise",
        "noise_clip",
        "policy_freq",
        "total_it",
        "state",
        "ans",
        "state",
        "action",
        "next_state",
        "reward",
        "not_done",
        "noise",
        "next_action",
        "target_Q1",
        "target_Q2",
        "target_Q",
        "target_Q",
        "current_Q1",
        "current_Q2",
        "critic_loss",
        "actor_loss",
        "critic_target",
        "actor_target"
    ],
    "comments": [
        "The DPFed DRL code references the implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)(Paper: https://arxiv.org/abs/1802.09477)",
        "Q1 architecture",
        "self.l1 = nn.Linear(state_dim + action_dim, 256)",
        "self.l2 = nn.Linear(256, 256)",
        "self.l3 = nn.Linear(256, 1)",
        "Q2 architecture",
        "self.l4 = nn.Linear(state_dim + action_dim, 256)",
        "self.l5 = nn.Linear(256, 256)",
        "self.l6 = nn.Linear(256, 1)",
        "Sample replay buffer",
        "Select action according to policy and add clipped noise",
        "Compute the target Q value",
        "Get current Q estimates",
        "Compute critic loss",
        "Optimize the critic",
        "Delayed policy updates",
        "Compute actor losse",
        "Optimize the actor",
        "Update the frozen target models"
    ],
    "docstrings": [],
    "functions": [
        "forward",
        "forward",
        "Q1",
        "select_action",
        "train",
        "save",
        "load"
    ],
    "classes": [
        "Actor",
        "Critic",
        "TD3"
    ]
}