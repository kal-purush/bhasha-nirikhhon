{
    "identifiers": [
        "torch",
        "os",
        "pickle",
        "collections",
        "defaultdict",
        "transformers",
        "AutoTokenizer",
        "utils",
        "misc",
        "invert_dict",
        "re",
        "batch",
        "batch",
        "batch",
        "torch",
        "stack",
        "topic_entity",
        "k",
        "torch",
        "cat",
        "q",
        "k",
        "q",
        "question",
        "dim",
        "k",
        "question",
        "torch",
        "stack",
        "answer",
        "torch",
        "stack",
        "entity_range",
        "relation_embedd",
        "topic_entity",
        "question",
        "answer",
        "entity_range",
        "relation_embedd",
        "torch",
        "utils",
        "data",
        "Dataset",
        "questions",
        "ent2id",
        "relation_embedd",
        "questions",
        "ent2id",
        "relation_embedd",
        "index",
        "questions",
        "index",
        "toOneHot",
        "topic_entity",
        "toOneHot",
        "answer",
        "toOneHot",
        "entity_range",
        "relation_embedd",
        "topic_entity",
        "question",
        "answer",
        "entity_range",
        "relation_embedd",
        "len",
        "questions",
        "indices",
        "torch",
        "LongTensor",
        "indices",
        "len",
        "ent2id",
        "torch",
        "FloatTensor",
        "vec_len",
        "one_hot",
        "zero_",
        "one_hot",
        "scatter_",
        "indices",
        "one_hot",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "input_dir",
        "fn",
        "bert_name",
        "ent2id",
        "rel2id",
        "batch_size",
        "relation_embedd",
        "training",
        "format",
        "fn",
        "AutoTokenizer",
        "from_pretrained",
        "bert_name",
        "ent2id",
        "rel2id",
        "invert_dict",
        "ent2id",
        "invert_dict",
        "rel2id",
        "defaultdict",
        "defaultdict",
        "line",
        "open",
        "os",
        "path",
        "join",
        "input_dir",
        "line",
        "strip",
        "split",
        "l",
        "l",
        "strip",
        "l",
        "sub_map",
        "s",
        "append",
        "p",
        "o",
        "so_map",
        "s",
        "o",
        "append",
        "p",
        "line",
        "open",
        "fn",
        "line",
        "strip",
        "line",
        "line",
        "split",
        "line",
        "re",
        "sub",
        "question",
        "line",
        "line",
        "p",
        "o",
        "sub_map",
        "head",
        "entity_range",
        "add",
        "o",
        "list1",
        "append",
        "p",
        "p2",
        "o2",
        "sub_map",
        "o",
        "entity_range",
        "add",
        "o2",
        "list2",
        "append",
        "p2",
        "ent2id",
        "o",
        "o",
        "entity_range",
        "ent2id",
        "head",
        "tokenizer",
        "question",
        "strip",
        "max_length",
        "padding",
        "return_tensors",
        "ent2id",
        "ans",
        "data",
        "append",
        "head",
        "question",
        "ans",
        "entity_range",
        "format",
        "len",
        "data",
        "Dataset",
        "data",
        "ent2id",
        "relation_embedd",
        "dataset",
        "batch_size",
        "batch_size",
        "shuffle",
        "training",
        "collate_fn",
        "collate",
        "input_dir",
        "bert_name",
        "batch_size",
        "os",
        "path",
        "join",
        "input_dir",
        "os",
        "path",
        "exists",
        "cache_fn",
        "format",
        "cache_fn",
        "open",
        "cache_fn",
        "fp",
        "pickle",
        "load",
        "fp",
        "format",
        "len",
        "train_data",
        "dataset",
        "len",
        "test_data",
        "dataset",
        "line",
        "open",
        "os",
        "path",
        "join",
        "input_dir",
        "line",
        "strip",
        "len",
        "ent2id",
        "line",
        "open",
        "os",
        "path",
        "join",
        "input_dir",
        "line",
        "strip",
        "len",
        "rel2id",
        "line",
        "open",
        "os",
        "path",
        "join",
        "input_dir",
        "line",
        "strip",
        "split",
        "ent2id",
        "l",
        "rel2id",
        "l",
        "strip",
        "ent2id",
        "l",
        "triples",
        "append",
        "s",
        "p",
        "o",
        "torch",
        "LongTensor",
        "triples",
        "AutoTokenizer",
        "from_pretrained",
        "bert_name",
        "line",
        "open",
        "os",
        "path",
        "join",
        "input_dir",
        "line",
        "strip",
        "re",
        "sub",
        "l",
        "tokenizer",
        "relation",
        "max_length",
        "padding",
        "return_tensors",
        "relation_embedd",
        "append",
        "relation_e",
        "relation_embedd",
        "k",
        "torch",
        "cat",
        "q",
        "k",
        "q",
        "relation_embedd",
        "dim",
        "k",
        "relation_embedd",
        "DataLoader",
        "input_dir",
        "os",
        "path",
        "join",
        "input_dir",
        "bert_name",
        "ent2id",
        "rel2id",
        "batch_size",
        "relation_embedd",
        "training",
        "DataLoader",
        "input_dir",
        "os",
        "path",
        "join",
        "input_dir",
        "bert_name",
        "ent2id",
        "rel2id",
        "batch_size",
        "relation_embedd",
        "open",
        "cache_fn",
        "fp",
        "pickle",
        "dump",
        "ent2id",
        "rel2id",
        "triples",
        "train_data",
        "test_data",
        "fp",
        "ent2id",
        "rel2id",
        "triples",
        "train_data",
        "test_data"
    ],
    "literals": [
        "'Reading questions from {}'",
        "'kb2_r.txt'",
        "'\\t'",
        "''",
        "'\\t'",
        "'\\.|_'",
        "' '",
        "'max_length'",
        "\"pt\"",
        "'data number: {}'",
        "'processed.pt'",
        "'Read from cache file: {} (NOTE: delete it if you modified data loading process)'",
        "'rb'",
        "'Train number: {}, test number: {}'",
        "'Read data...'",
        "'entity_r2.txt'",
        "'relation2_r.txt'",
        "'kb2_r.txt'",
        "'\\t'",
        "'relation2_r.txt'",
        "'\\.|_'",
        "' '",
        "'max_length'",
        "\"pt\"",
        "'QA/train_asr.txt'",
        "'QA/test_asr3.txt'",
        "'wb'"
    ],
    "variables": [
        "batch",
        "topic_entity",
        "question",
        "answer",
        "entity_range",
        "relation_embedd",
        "topic_entity",
        "question",
        "answer",
        "entity_range",
        "relation_embedd",
        "questions",
        "ent2id",
        "relation_embedd",
        "topic_entity",
        "question",
        "answer",
        "entity_range",
        "topic_entity",
        "answer",
        "entity_range",
        "relation_embedd",
        "indices",
        "vec_len",
        "one_hot",
        "tokenizer",
        "ent2id",
        "rel2id",
        "id2ent",
        "id2rel",
        "sub_map",
        "so_map",
        "l",
        "s",
        "p",
        "o",
        "data",
        "line",
        "line",
        "question",
        "question",
        "head",
        "ans",
        "entity_range",
        "list1",
        "list2",
        "entity_range",
        "head",
        "question",
        "ans",
        "dataset",
        "cache_fn",
        "ent2id",
        "rel2id",
        "triples",
        "train_data",
        "test_data",
        "ent2id",
        "l",
        "ent2id",
        "l",
        "rel2id",
        "l",
        "rel2id",
        "l",
        "triples",
        "l",
        "s",
        "p",
        "o",
        "triples",
        "tokenizer",
        "relation_embedd",
        "l",
        "relation",
        "relation_e",
        "relation_embedd",
        "relation_embedd",
        "train_data",
        "test_data"
    ],
    "comments": [
        "relation_embedd = {k:torch.cat([q[k] for q in relation_embedd[0]], dim=0) for k in relation_embedd[0][0]}",
        "relation_embedd = {k: torch.cat([q[k] for q in relation_embedd], dim=0) for k in relation_embedd[0]}",
        "relation_embedd = {k: torch.cat([q[k] for q in relation_embedd], dim=0) for k in relation_embedd[0]}",
        "if no answer",
        "if len(line) != 2:",
        "continue",
        "question_1 = question[0]",
        "question_2 = question[1].split(']')",
        "question_2 = question_2[1]",
        "question = question_1 + 'NE' + question_2",
        "question = question_1.strip()",
        "if (head, ans[0]) not in so_map:",
        "continue",
        "list3=[]",
        "for p3, o3 in sub_map[o2]:",
        "entity_range.add(o3)",
        "list3.append(p3)",
        "relation_range = (list1,list2,list3)",
        "relation_embedd={k: torch.cat([q[k] for q in relation_embedd], dim=0) for k in relation_embedd[0]}",
        "ent2id, rel2id, triples, train_data,val_data,test_data = pickle.load(fp)",
        "l = line.replace('<', '').replace('>', '').replace('\"', '')",
        "print(len(ent2id))",
        "print(max(ent2id.values()))",
        "l = line.replace('.', '').replace('<', '').replace('>', '').replace('\"', '')",
        "rel2id[l + '_reverse'] = len(rel2id)",
        "f = open(\"2_relation.txt\",'a+')",
        "f.write(l)",
        "f.write('\\n')",
        "f.write(l + '_reverse')",
        "f.write('\\n')",
        "p_rev = rel2id[l[1].strip()+'_reverse']",
        "triples.append((o, p_rev, s))",
        "sourse_level=l[0].strip()",
        "relation_level = sourse_level.split('.')[-1]",
        "relation = l",
        "token = tokenizer.tokenize(relation)",
        "relation.append(sourse_level)",
        "relation.append(relation_level)",
        "relation.append(type_level)",
        "i=0",
        "for k in relation_embedd[0]:",
        "# for q in relation_embedd:",
        "l=torch.cat([q[k] for q in relation_embedd], dim=0)",
        "relation_embedd={k:l}",
        "val_data = DataLoader(input_dir, os.path.join(input_dir, 'QA/2q_val.txt'), bert_name, ent2id, rel2id, batch_size, relation_embedd)",
        "pickle.dump((ent2id, rel2id, triples, train_data,val_data,test_data), fp)",
        "return ent2id, rel2id, triples, train_data,val_data,test_data"
    ],
    "docstrings": [],
    "functions": [
        "collate",
        "__getitem__",
        "__len__",
        "toOneHot",
        "load_data"
    ],
    "classes": [
        "Dataset",
        "DataLoader"
    ]
}