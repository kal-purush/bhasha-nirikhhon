{
    "identifiers": [
        "numpy",
        "np",
        "torch",
        "torch",
        "optim",
        "torch",
        "nn",
        "utils",
        "clip_grad_norm_",
        "opt",
        "game",
        "model",
        "target",
        "index",
        "opt",
        "game",
        "model",
        "target",
        "p",
        "model_target",
        "parameters",
        "index",
        "optim",
        "RMSprop",
        "model",
        "get_params",
        "lr",
        "opt",
        "momentum",
        "opt",
        "model",
        "reset_parameters",
        "model_target",
        "reset_parameters",
        "eps",
        "np",
        "random",
        "rand",
        "eps",
        "items",
        "torch",
        "from_numpy",
        "np",
        "random",
        "choice",
        "items",
        "item",
        "step",
        "q",
        "eps",
        "target",
        "train_mode",
        "train_mode",
        "opt",
        "game",
        "get_action_range",
        "step",
        "id",
        "torch",
        "zeros",
        "opt",
        "_eps_flip",
        "eps",
        "_eps_flip",
        "eps",
        "action_range",
        "action_range",
        "should_select_random_a",
        "_random_choice",
        "a_range",
        "q",
        "action",
        "q",
        "a_range",
        "max",
        "action",
        "comm_range",
        "comm_range",
        "comm_range",
        "should_select_random_comm",
        "_random_choice",
        "c_range",
        "q",
        "comm_action",
        "comm_action",
        "opt",
        "q",
        "c_range",
        "max",
        "comm_action",
        "opt",
        "q",
        "q_a_range",
        "max",
        "action",
        "action_value",
        "comm_vector",
        "comm_action",
        "comm_value",
        "episode",
        "opt",
        "episode",
        "step",
        "steps",
        "episode",
        "step",
        "i",
        "opt",
        "record",
        "record",
        "i",
        "record",
        "i",
        "record",
        "r_t",
        "q_a_t",
        "episode",
        "step",
        "next_record",
        "i",
        "q_next_max",
        "next_record",
        "i",
        "r_t",
        "opt",
        "q_next_max",
        "q_a_t",
        "record",
        "i",
        "record",
        "i",
        "record",
        "r_t",
        "q_comm_t",
        "episode",
        "step",
        "next_record",
        "i",
        "q_next_max",
        "next_record",
        "i",
        "r_t",
        "opt",
        "q_next_max",
        "q_comm_t",
        "td_action",
        "td_comm",
        "total_loss",
        "loss_t",
        "total_loss",
        "sum",
        "opt",
        "loss",
        "episode",
        "optimizer",
        "zero_grad",
        "episode_loss",
        "episode",
        "loss",
        "backward",
        "retain_graph",
        "opt",
        "clip_grad_norm_",
        "parameters",
        "model",
        "get_params",
        "max_norm",
        "optimizer",
        "step",
        "episodes_seen",
        "episodes_seen",
        "opt",
        "model_target",
        "load_state_dict",
        "model",
        "state_dict"
    ],
    "literals": [
        "\"learningrate\"",
        "\"momentum\"",
        "\"game_comm_bits\"",
        "\"game_action_space\"",
        "\"game_action_space\"",
        "\"steps\"",
        "\"step_records\"",
        "\"game_nagents\"",
        "\"r_t\"",
        "\"q_a_t\"",
        "\"a_t\"",
        "\"terminal\"",
        "\"step_records\"",
        "\"q_a_max_t\"",
        "\"q_comm_max_t\"",
        "\"gamma\"",
        "\"a_comm_t\"",
        "\"q_comm_t\"",
        "\"terminal\"",
        "\"step_records\"",
        "\"q_comm_max_t\"",
        "\"q_a_max_t\"",
        "\"gamma\"",
        "\"game_nagents\"",
        "\"model_know_share\"",
        "\"step_target\""
    ],
    "variables": [
        "opt",
        "game",
        "model",
        "model_target",
        "p",
        "requires_grad",
        "episodes_seen",
        "id",
        "optimizer",
        "episodes_seen",
        "eps",
        "opt",
        "action_range",
        "comm_range",
        "comm_action",
        "comm_vector",
        "comm_value",
        "should_select_random_a",
        "should_select_random_comm",
        "a_range",
        "action",
        "action_value",
        "action_value",
        "action",
        "action",
        "c_range",
        "comm_action",
        "comm_value",
        "comm_action",
        "comm_value",
        "comm_action",
        "comm_vector",
        "comm_action",
        "comm_action",
        "q_a_range",
        "comm_value",
        "_",
        "opt",
        "total_loss",
        "steps",
        "record",
        "td_action",
        "td_comm",
        "r_t",
        "q_a_t",
        "q_comm_t",
        "td_action",
        "next_record",
        "q_next_max",
        "q_next_max",
        "td_action",
        "q_comm_t",
        "td_comm",
        "next_record",
        "q_next_max",
        "q_next_max",
        "td_comm",
        "loss_t",
        "total_loss",
        "loss",
        "loss",
        "episodes_seen"
    ],
    "comments": [
        "print(\"Printing q : \", q, a_range, q[a_range])",
        "print(\"Comm vector : \", comm_vector)",
        "print(\"Comm action : \", comm_action)",
        "print(record[\"r_t\"])",
        "print(\"Loss\")",
        "print(loss)"
    ],
    "docstrings": [],
    "functions": [
        "reset",
        "_eps_flip",
        "_random_choice",
        "select_action_and_comm",
        "episode_loss",
        "learn_from_episode"
    ],
    "classes": [
        "CNetAgentMy"
    ]
}