{
    "identifiers": [
        "builtins",
        "numpy",
        "np",
        "cs231n",
        "layers",
        "cs231n",
        "fast_layers",
        "cs231n",
        "layer_utils",
        "input_dim",
        "num_filters",
        "filter_size",
        "hidden_dim",
        "num_classes",
        "weight_scale",
        "reg",
        "dtype",
        "np",
        "float32",
        "reg",
        "dtype",
        "input_dim",
        "weight_scale",
        "np",
        "random",
        "randn",
        "num_filters",
        "C",
        "filter_size",
        "filter_size",
        "np",
        "zeros",
        "num_filters",
        "weight_scale",
        "np",
        "random",
        "randn",
        "H",
        "W",
        "num_filters",
        "hidden_dim",
        "np",
        "zeros",
        "hidden_dim",
        "weight_scale",
        "np",
        "random",
        "randn",
        "hidden_dim",
        "num_classes",
        "np",
        "zeros",
        "num_classes",
        "k",
        "v",
        "items",
        "v",
        "astype",
        "dtype",
        "X",
        "y",
        "W1",
        "shape",
        "filter_size",
        "conv_relu_pool_forward",
        "X",
        "conv_param",
        "pool_param",
        "affine_forward",
        "conv_forward_out_1",
        "relu_forward",
        "affine_forward_out_2",
        "affine_forward",
        "affine_relu_2",
        "y",
        "scores",
        "softmax_loss",
        "scores",
        "y",
        "loss",
        "reg",
        "np",
        "sum",
        "np",
        "sum",
        "np",
        "sum",
        "grads",
        "grads",
        "affine_backward",
        "dout",
        "cache_forward_3",
        "relu_backward",
        "dX3",
        "cache_relu_2",
        "grads",
        "grads",
        "affine_backward",
        "dX2",
        "cache_forward_2",
        "grads",
        "grads",
        "conv_relu_pool_backward",
        "dX2",
        "cache_forward_1",
        "grads",
        "reg",
        "grads",
        "reg",
        "grads",
        "reg",
        "loss",
        "grads"
    ],
    "literals": [
        "'W1'",
        "'b1'",
        "'W2'",
        "'b2'",
        "'W3'",
        "'b3'",
        "'W1'",
        "'b1'",
        "'W2'",
        "'b2'",
        "'W3'",
        "'b3'",
        "'stride'",
        "'pad'",
        "'pool_height'",
        "'pool_width'",
        "'stride'",
        "'W1'",
        "'b1'",
        "'W2'",
        "'b2'",
        "'W3'",
        "'b3'",
        "'W1'",
        "'W2'",
        "'W3'",
        "'W3'",
        "'b3'",
        "'W2'",
        "'b2'",
        "'W1'",
        "'b1'",
        "'W3'",
        "'W3'",
        "'W3'",
        "'W2'",
        "'W2'",
        "'W2'",
        "'W1'",
        "'W1'",
        "'W1'"
    ],
    "variables": [
        "reg",
        "dtype",
        "C",
        "H",
        "W",
        "k",
        "W1",
        "b1",
        "W2",
        "b2",
        "W3",
        "b3",
        "filter_size",
        "conv_param",
        "pool_param",
        "scores",
        "conv_forward_out_1",
        "cache_forward_1",
        "affine_forward_out_2",
        "cache_forward_2",
        "affine_relu_2",
        "cache_relu_2",
        "scores",
        "cache_forward_3",
        "loss",
        "grads",
        "loss",
        "dout",
        "dX3",
        "dX2",
        "dX2",
        "dX1",
        "grads",
        "grads",
        "grads"
    ],
    "comments": [
        "",
        "TODO: Initialize weights and biases for the three-layer convolutional    #",
        "network. Weights should be initialized from a Gaussian centered at 0.0   #",
        "with standard deviation equal to weight_scale; biases should be          #",
        "initialized to zero. All weights and biases should be stored in the      #",
        "dictionary self.params. Store weights and biases for the convolutional  #",
        "layer using the keys 'W1' and 'b1'; use keys 'W2' and 'b2' for the       #",
        "weights and biases of the hidden affine layer, and keys 'W3' and 'b3'    #",
        "for the weights and biases of the output affine layer.                   #",
        "#",
        "IMPORTANT: For this assignment, you can assume that the padding          #",
        "and stride of the first convolutional layer are chosen so that           #",
        "the start of the loss() function to see how that happens.                #",
        "",
        "self.filter_size = filter_size",
        "self.num_filters = num_filters",
        "self.input_dim = input_dim",
        "self.hidden_dim = hidden_dim",
        "self.num_classes = num_classes",
        "C, H, W = self.input_dim",
        "F = self.num_filters",
        "HH, WW = self.filter_size, self.filter_size",
        "self.params['W1'] = weight_scale * np.random.randn(F, C, HH, WW)",
        "self.params['b1'] = np.zeros(F, )",
        "self.params['W2'] = weight_scale * np.random.randn(np.prod((F, H//2, W//2)), self.hidden_dim)",
        "self.params['b2'] = np.zeros(self.hidden_dim, )",
        "self.params['W3'] = weight_scale * np.random.randn(self.hidden_dim, self.num_classes)",
        "self.params['b3'] = np.zeros(self.num_classes, )",
        "",
        "END OF YOUR CODE                             #",
        "",
        "pass conv_param to the forward pass for the convolutional layer",
        "Padding and stride chosen to preserve the input spatial size",
        "pass pool_param to the forward pass for the max-pooling layer",
        "",
        "TODO: Implement the forward pass for the three-layer convolutional net,  #",
        "computing the class scores for X and storing them in the scores          #",
        "variable.                                                                #",
        "#",
        "Remember you can use the functions defined in cs231n/fast_layers.py and  #",
        "cs231n/layer_utils.py in your implementation (already imported).         #",
        "",
        "C, H, W = self.input_dim",
        "layer1_out, layer1_cache = conv_relu_pool_forward(X, W1, b1, conv_param, pool_param)",
        "layer2_out, layer2_cache = affine_relu_forward(layer1_out, W2, b2)",
        "layer3_out, layer3_cache = affine_forward(layer2_out, W3, b3)",
        "",
        "END OF YOUR CODE                             #",
        "",
        "",
        "TODO: Implement the backward pass for the three-layer convolutional net, #",
        "storing the loss and gradients in the loss and grads variables. Compute  #",
        "data loss using softmax, and make sure that grads[k] holds the gradients #",
        "for self.params[k]. Don't forget to add L2 regularization!               #",
        "#",
        "NOTE: To ensure that your implementation matches ours and you pass the   #",
        "automated tests, make sure that your L2 regularization includes a factor #",
        "of 0.5 to simplify the expression for the gradient.                      #",
        "",
        "loss, dout = softmax_loss(layer3_out, y)",
        "loss += 0.5 * (self.reg * np.sum(W1 * W1) + self.reg * np.sum(W2 * W2) + self.reg * np.sum(W3 * W3))",
        "dout, dW3, db3 = affine_backward(dout, layer3_cache)",
        "dout, dW2, db2 = affine_relu_backward(dout, layer2_cache)",
        "dx, dW1, db1 = conv_relu_pool_backward(dout, layer1_cache)",
        "grads['W1'] = dW1 + self.reg * W1",
        "grads['b1'] = db1",
        "grads['W2'] = dW2 + self.reg * W2",
        "grads['b2'] = db2",
        "grads['W3'] = dW3 + self.reg * W3",
        "grads['b3'] = db3",
        "Add regularization",
        "",
        "END OF YOUR CODE                             #",
        ""
    ],
    "docstrings": [
        "\"\"\"\n    A three-layer convolutional network with the following architecture:\n\n    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n\n    The network operates on minibatches of data that have shape (N, C, H, W)\n    consisting of N images, each with height H and width W and with C input\n    channels.\n    \"\"\"",
        "\"\"\"\n        Initialize a new network.\n\n        Inputs:\n        - input_dim: Tuple (C, H, W) giving size of input data\n        - num_filters: Number of filters to use in the convolutional layer\n        - filter_size: Width/height of filters to use in the convolutional layer\n        - hidden_dim: Number of units to use in the fully-connected hidden layer\n        - num_classes: Number of scores to produce from the final affine layer.\n        - weight_scale: Scalar giving standard deviation for random initialization\n          of weights.\n        - reg: Scalar giving L2 regularization strength\n        - dtype: numpy datatype to use for computation.\n        \"\"\"",
        "**the width and height of the input are preserved**. Take a look at      #",
        "*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****",
        "*****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****",
        "\"\"\"\n        Evaluate loss and gradient for the three-layer convolutional network.\n\n        Input / output: Same API as TwoLayerNet in fc_net.py.\n        \"\"\"",
        "*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****",
        "*****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****",
        "*****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****",
        "*****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
    ],
    "functions": [
        "loss"
    ],
    "classes": [
        "ThreeLayerConvNet"
    ]
}