{
    "identifiers": [
        "builtwith",
        "whois",
        "urllib2",
        "re",
        "itertools",
        "urlparse",
        "builtwith",
        "parse",
        "whois",
        "whois",
        "url",
        "user_agent",
        "num_retries",
        "url",
        "user_agent",
        "urllib2",
        "Request",
        "url",
        "headers",
        "headers",
        "urllib2",
        "urlopen",
        "request",
        "read",
        "urllib2",
        "URLError",
        "e",
        "e",
        "reason",
        "num_retries",
        "hasattr",
        "e",
        "e",
        "code",
        "download",
        "url",
        "num_retries",
        "html",
        "url",
        "download",
        "url",
        "re",
        "findall",
        "sitemap",
        "link",
        "links",
        "link",
        "download",
        "link",
        "html",
        "html",
        "link_dict",
        "url",
        "start",
        "end",
        "max_errors",
        "user_agent",
        "num_retries",
        "page",
        "start",
        "end",
        "url",
        "page",
        "download",
        "url_",
        "user_agent",
        "num_retries",
        "html",
        "num_errors",
        "num_errors",
        "max_errors",
        "html",
        "pages",
        "seed_url",
        "link_regex",
        "seed_url",
        "craw_queue",
        "craw_queue",
        "craw_queue",
        "pop",
        "download",
        "url",
        "get_links",
        "html",
        "link",
        "get_links",
        "html",
        "re",
        "match",
        "link_regex",
        "link",
        "urlparse",
        "urljoin",
        "seed_url",
        "link",
        "link",
        "seen",
        "seen",
        "add",
        "link",
        "craw_queue",
        "append",
        "link",
        "html",
        "re",
        "compile",
        "re",
        "IGNORECASE",
        "webpage_regex",
        "findall",
        "html",
        "link_crawler"
    ],
    "literals": [
        "'http://example.webscraping.com'",
        "'baidu.com'",
        "'wswp'",
        "'Downloading:'",
        "'User-Agent'",
        "'Download error:'",
        "'code'",
        "'<loc>(.*?)</loc>'",
        "\"link:\"",
        "'wswp'",
        "'<a[^>]+href=[\"\\'](.*?)[\"\\']'",
        "'http://example.webscraping.com'",
        "'/places/default/(index|view)'"
    ],
    "variables": [
        "res",
        "who",
        "headers",
        "request",
        "html",
        "html",
        "link_dict",
        "sitemap",
        "links",
        "html",
        "link_dict",
        "link",
        "num_errors",
        "pages",
        "url_",
        "html",
        "num_errors",
        "pages",
        "page",
        "craw_queue",
        "seen",
        "url",
        "html",
        "link",
        "webpage_regex"
    ],
    "comments": [
        "! /usr/bin/env python",
        "-*- coding:utf-8 -*-",
        "pip install builtwith  识别网站所用技术",
        "pip install python-whois 寻找网站所有者",
        "示例网站  http://example.webscraping.com",
        "print type(res),res  #<type 'dict'>  {u'javascript-frameworks': [u'jQuery', u'Modernizr', u'jQuery UI'],}",
        "print type(who),who  #<class 'whois.parser.WhoisCom'>",
        "下载网页并返回其HTML, 用户代理",
        "recursively retry Sxx HTTP errors",
        "download('http://httpstat.us/500')",
        "res =  download('http://www.baidu.com')",
        "print type(res) #<type 'str'>",
        "抓取网站地图",
        "download the s itemap file",
        "extract the sitemap links",
        "download each link",
        "res = craw_sitemap('http://example.webscraping.com/sitemap.xml')",
        "print res",
        "忽略页面别名， 只遍历ID来下载",
        "max_errors: maximum number of consecutive download errors allowed",
        "current number of consecutive(连续的) download errors",
        "received an error trying to down load this webpage",
        "reached maximum number of",
        "consecutive errors so exit",
        "success - can scrape the result",
        "res = download_by_id('http://example.webscraping.com/view/-%d',1,2)",
        "print res",
        "keep track which URL’s have seen before",
        "默认移除并返回最后一个元素",
        "filter for links matching our regular expression",
        "check if link matches expected regex",
        "form absolute link",
        "check if have a l ready seen thi s link",
        "return relative link list",
        "a regular expression to extract all links from the webpage",
        "list of all links from the webpage"
    ],
    "docstrings": [
        "\"\"\"Crawl from the given seed URL following links matched by link regex\"\"\"",
        "\"\"\"Return a list of links from html\"\"\""
    ],
    "functions": [
        "download",
        "craw_sitemap",
        "download_by_id",
        "link_crawler",
        "get_links"
    ],
    "classes": []
}