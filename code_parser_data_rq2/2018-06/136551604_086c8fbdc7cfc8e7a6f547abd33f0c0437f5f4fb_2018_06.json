{
    "identifiers": [
        "time",
        "numpy",
        "np",
        "tensorflow",
        "tf",
        "tf",
        "contrib",
        "rnn",
        "GRUCell",
        "tf",
        "nn",
        "bidirectional_dynamic_rnn",
        "tf",
        "nn",
        "sigmoid_cross_entropy_with_logits",
        "tf",
        "train",
        "AdamOptimizer",
        "_MODEL_PATH",
        "seq_lens",
        "vocab_size",
        "num_units",
        "_NUM_UNITS",
        "embedding_dim",
        "_EMBEDDING_DIM",
        "attention_dim",
        "_ATTENTION_DIM",
        "time_major",
        "return_alpha",
        "batch_size",
        "_BATCH_SIZE",
        "learning_rate",
        "_LEARNING_RATE",
        "time",
        "time",
        "batch_size",
        "tf",
        "name_scope",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "seq_lens",
        "name",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "name",
        "tf",
        "placeholder",
        "tf",
        "int32",
        "name",
        "tf",
        "placeholder",
        "tf",
        "float32",
        "name",
        "tf",
        "name_scope",
        "tf",
        "Variable",
        "tf",
        "random_uniform",
        "vocab_size",
        "embedding_dim",
        "tf",
        "nn",
        "embedding_lookup",
        "embed_var",
        "inputs",
        "tf",
        "name_scope",
        "_GRUCell",
        "num_units",
        "_GRUCell",
        "num_units",
        "_BiDRNN",
        "fw",
        "bw",
        "inputs",
        "embeded",
        "sequence_length",
        "seq_len",
        "dtype",
        "tf",
        "float32",
        "tf",
        "name_scope",
        "attention",
        "opts",
        "attention_dim",
        "time_major",
        "return_alpha",
        "tf",
        "name_scope",
        "tf",
        "Variable",
        "tf",
        "truncated_normal",
        "_NUM_UNITS",
        "stddev",
        "tf",
        "Variable",
        "tf",
        "constant",
        "shape",
        "tf",
        "nn",
        "dropout",
        "attention_opt",
        "keep_prob",
        "tf",
        "squeeze",
        "tf",
        "nn",
        "xw_plus_b",
        "drop",
        "W",
        "b",
        "tf",
        "name_scope",
        "tf",
        "reduce_mean",
        "_Loss",
        "logits",
        "y_hat",
        "labels",
        "target",
        "_Optz",
        "learning_rate",
        "learning_rate",
        "minimize",
        "loss",
        "tf",
        "equal",
        "tf",
        "round",
        "tf",
        "sigmoid",
        "y_hat",
        "target",
        "tf",
        "reduce_mean",
        "tf",
        "cast",
        "predict",
        "tf",
        "float32",
        "tf",
        "train",
        "Saver",
        "format",
        "time",
        "time",
        "start",
        "inputs",
        "attention_size",
        "time_major",
        "return_alpha",
        "isinstance",
        "inputs",
        "tf",
        "concat",
        "inputs",
        "time_major",
        "tf",
        "array_ops",
        "transpose",
        "inputs",
        "inputs",
        "shape",
        "value",
        "tf",
        "Variable",
        "tf",
        "random_normal",
        "hidden_size",
        "attention_size",
        "stddev",
        "tf",
        "Variable",
        "tf",
        "random_normal",
        "attention_size",
        "stddev",
        "tf",
        "Variable",
        "tf",
        "random_normal",
        "attention_size",
        "stddev",
        "tf",
        "name_scope",
        "tf",
        "tanh",
        "tf",
        "tensordot",
        "inputs",
        "w",
        "axes",
        "b",
        "tf",
        "nn",
        "softmax",
        "tf",
        "tensordot",
        "score",
        "u",
        "axes",
        "name",
        "name",
        "tf",
        "reduce_sum",
        "inputs",
        "tf",
        "expand_dims",
        "alpha",
        "axis",
        "axis",
        "return_alpha",
        "context",
        "alpha",
        "context",
        "allow_growth",
        "gpu_memory_frac",
        "tf",
        "GPUOptions",
        "allow_growth",
        "allow_growth",
        "per_process_gpu_memory_fraction",
        "gpu_memory_frac",
        "tf",
        "ConfigProto",
        "gpu_options",
        "options",
        "x",
        "y",
        "max_iteration",
        "_MAX_ITERATION",
        "x",
        "y",
        "x",
        "shape",
        "np",
        "arange",
        "size",
        "np",
        "random",
        "shuffle",
        "indices",
        "x",
        "copy",
        "indices",
        "y",
        "copy",
        "indices",
        "x_copy",
        "y_copy",
        "_shuffle",
        "x",
        "y",
        "x",
        "shape",
        "max_iteration",
        "max_iteration",
        "max_iteration",
        "minus",
        "i",
        "batch_size",
        "size",
        "xc",
        "i",
        "i",
        "batch_size",
        "yc",
        "i",
        "i",
        "batch_size",
        "i",
        "batch_size",
        "_shuffle",
        "x",
        "y",
        "sess",
        "tf",
        "train",
        "get_checkpoint_state",
        "_MODEL_PATH",
        "checkpoint",
        "checkpoint",
        "model_checkpoint_path",
        "saver",
        "restore",
        "sess",
        "checkpoint",
        "model_checkpoint_path",
        "checkpoint",
        "model_checkpoint_path",
        "x_tr",
        "y_tr",
        "epochs",
        "_EPOCHS",
        "keep_prob",
        "_KEEP_PROB",
        "delta",
        "_DELTA",
        "sess_conf",
        "get_batch",
        "x_tr",
        "y_tr",
        "max_iteration",
        "sess_conf",
        "init_sess_config",
        "tf",
        "Session",
        "config",
        "sess_conf",
        "sess",
        "reload_model",
        "sess",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "epoch_i",
        "epochs",
        "x_tr",
        "shape",
        "batch_size",
        "b",
        "num_batches",
        "time",
        "time",
        "next",
        "tr_batch_gen",
        "np",
        "array",
        "len",
        "x",
        "x",
        "x_batch",
        "sess",
        "run",
        "loss",
        "accuracy",
        "optimizer",
        "feed_dict",
        "inputs",
        "x_batch",
        "target",
        "y_batch",
        "seq_len",
        "seq_len",
        "keep_prob",
        "keep_prob",
        "loss",
        "delta",
        "loss_tr",
        "delta",
        "time",
        "time",
        "start",
        "start",
        "period",
        "format",
        "epoch_i",
        "b",
        "num_batches",
        "period",
        "format",
        "loss_tr",
        "acc",
        "format",
        "epoch_i",
        "saver",
        "save",
        "sess",
        "_MODEL_NAME",
        "global_step",
        "epoch_i",
        "num_batches",
        "saver",
        "save",
        "sess",
        "_MODEL_NAME",
        "global_step",
        "epochs",
        "num_batches",
        "format",
        "epochs",
        "x_ts",
        "y_ts",
        "epochs",
        "keep_prob",
        "_KEEP_PROB",
        "delta",
        "_DELTA",
        "sess_conf",
        "get_batch",
        "x_ts",
        "y_ts",
        "max_iteration",
        "sess_conf",
        "init_sess_config",
        "tf",
        "Session",
        "config",
        "sess_conf",
        "sess",
        "sess",
        "run",
        "tf",
        "global_variables_initializer",
        "epoch_i",
        "epochs",
        "x_ts",
        "shape",
        "batch_size",
        "b",
        "num_batches",
        "time",
        "time",
        "next",
        "ts_batch_gen",
        "np",
        "array",
        "len",
        "x",
        "x",
        "x_batch",
        "sess",
        "run",
        "loss",
        "accuracy",
        "feed_dict",
        "inputs",
        "x_batch",
        "target",
        "y_batch",
        "seq_len",
        "seq_len",
        "keep_prob",
        "keep_prob",
        "loss",
        "delta",
        "loss_ts",
        "delta",
        "time",
        "time",
        "start",
        "start",
        "period",
        "format",
        "epoch_i",
        "b",
        "num_batches",
        "period",
        "format",
        "loss_ts",
        "acc"
    ],
    "literals": [
        "'./models/'",
        "\"model-han\"",
        "'init'",
        "'inputs'",
        "'target'",
        "'seq_len'",
        "'keep_prob'",
        "'build model start ....'",
        "'embedding_layer'",
        "'Bi_RNN'",
        "'attention_layer'",
        "'full_connect_layer'",
        "'train_op'",
        "'build model finished, cost: {}'",
        "'v'",
        "'vu'",
        "'alpha'",
        "\"Successfully loaded:\"",
        "\"no old network weights can be loaded\"",
        "'Training start ....'",
        "'Training: Epoch {} Batch {}/{} - Period: {}'",
        "'Loss: {} - Accuracy: {} '",
        "'save sub_epoch_{} model'",
        "'Model Trained and Saved'",
        "'Training {} epoachs finished'",
        "'Testing start ....'",
        "'Testing: Epoch {} Batch {}/{} - Period: {}'",
        "'Loss: {} - Accuracy: {} '",
        "'Testing end ....'"
    ],
    "variables": [
        "_GRUCell",
        "_BiDRNN",
        "_Loss",
        "_Optz",
        "_EMBEDDING_DIM",
        "_NUM_UNITS",
        "_ATTENTION_DIM",
        "_LEARNING_RATE",
        "_BATCH_SIZE",
        "_EPOCHS",
        "_MAX_ITERATION",
        "_KEEP_PROB",
        "_DELTA",
        "_MODEL_PATH",
        "_MODEL_NAME",
        "start",
        "batch_size",
        "inputs",
        "target",
        "seq_len",
        "keep_prob",
        "embed_var",
        "embeded",
        "fw",
        "bw",
        "opts",
        "_",
        "attention_opt",
        "W",
        "b",
        "drop",
        "y_hat",
        "loss",
        "optimizer",
        "predict",
        "accuracy",
        "saver",
        "inputs",
        "inputs",
        "hidden_size",
        "w",
        "b",
        "u",
        "score",
        "alpha",
        "context",
        "options",
        "size",
        "indices",
        "x_copy",
        "y_copy",
        "xc",
        "yc",
        "size",
        "i",
        "minus",
        "i",
        "xc",
        "yc",
        "checkpoint",
        "tr_batch_gen",
        "sess_conf",
        "loss_tr",
        "num_batches",
        "start",
        "x_batch",
        "y_batch",
        "seq_len",
        "loss",
        "acc",
        "_",
        "loss_tr",
        "period",
        "ts_batch_gen",
        "sess_conf",
        "loss_ts",
        "num_batches",
        "start",
        "x_batch",
        "y_batch",
        "seq_len",
        "loss",
        "acc",
        "loss_ts",
        "period"
    ],
    "comments": [
        "dim for embedding layer",
        "size for BiRNN Cell",
        "dim of attention weight",
        "learning_rate",
        "for word embed pre-train",
        "Hierarchical Attention Networks",
        "build model",
        "notice W for two gru cell forward and backand",
        "shape (T, B, Dim) = (B, T, Dim)",
        "RNN hidden size (Dim)",
        "trainable variable",
        "w, b : parameter for calculate score by tanh(w*h+b)",
        "u : parameter for calculate alpha by softmax(u * score)",
        "output context : reduce_sum(inputs * alpha)",
        "(B, T, Adim)",
        "(B, T)",
        "(B, Dim)",
        "re-shuffle data x and y, then re-generate batches",
        "load checkpoint including all structure",
        "load trained network",
        "seq_len = np.array([list(x).index(0) + 1 for x in x_batch])",
        "save epoch i",
        "Save final Model",
        "seq_len = np.array([list(x).index(0) + 1 for x in x_batch])"
    ],
    "docstrings": [
        "'''\n        The idea was proposed in the article by Z. Yang et al., \n        \"Hierarchical Attention Networks for Document Classification\", 2016: \n        http://www.aclweb.org/anthology/N16-1174.\n    '''",
        "'''\n            inputs: attention layer input from output state of Bi-RNN,\n                    Bi-RNN's output state should be a tuple : (forward, backword)\n            attention_size: attention parameters weight size\n            time_major: The shape format of the inputs Tensors.\n                        If true, [max_time, batch_size, depth].\n                        If false, [batch_size, max_time, depth]\n                Using `time_major = True` is a bit more efficient because it \n                avoids transposes at the beginning and end of the RNN calculation.  \n                However, most TensorFlow data is batch-major, so by default \n                this function accepts input and emits output in batch-major form.\n            return_alpha: whether to return weight [alpha] of output state of Bi-RNN\n        '''",
        "'''\n            allow_growth: if True, gpu memory will increase by requirement \n            gpu_memory_frac: percentage of gpu memory use\n        '''",
        "'''\n            max_iteration: control loop times, if not 0, else loop always\n        '''"
    ],
    "functions": [
        "attention",
        "init_sess_config",
        "get_batch",
        "_shuffle",
        "reload_model",
        "training",
        "test"
    ],
    "classes": [
        "HAN"
    ]
}