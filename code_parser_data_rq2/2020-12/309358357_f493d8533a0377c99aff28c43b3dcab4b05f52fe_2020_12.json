{
    "identifiers": [
        "torch",
        "torch",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "torch",
        "distributions",
        "dist",
        "numpy",
        "np",
        "nn",
        "Module",
        "kwargs",
        "kwargs",
        "kwargs",
        "kwargs",
        "kwargs",
        "kwargs",
        "nn",
        "Embedding",
        "embedding_dim",
        "ed",
        "num_embeddings",
        "nt",
        "nn",
        "Embedding",
        "embedding_dim",
        "ed",
        "num_embeddings",
        "sl",
        "emb_dim",
        "kwargs",
        "kwargs",
        "TransformerBlock",
        "trans_args",
        "_",
        "nr",
        "nn",
        "Sequential",
        "trans_blocks",
        "nn",
        "Linear",
        "ed",
        "nt",
        "x",
        "x",
        "shape",
        "token_embedding",
        "x",
        "torch",
        "arange",
        "seq_length",
        "device",
        "device",
        "pos_embedding",
        "pos_range",
        "pos",
        "unsqueeze",
        "pos",
        "expand",
        "batch",
        "seq_length",
        "emb_dim",
        "x",
        "pos",
        "tblocks",
        "x",
        "x",
        "mean",
        "dim",
        "x",
        "max",
        "dim",
        "x",
        "view",
        "batch",
        "seq_length",
        "emb_dim",
        "toprobs",
        "x",
        "x",
        "view",
        "batch",
        "num_tokens",
        "seq_length",
        "F",
        "log_softmax",
        "x",
        "dim",
        "emb_mean",
        "emb_max",
        "nn",
        "Module",
        "emb",
        "heads",
        "mask",
        "ff_hidden_mult",
        "dropout",
        "wide",
        "SelfAttentionWide",
        "emb",
        "heads",
        "heads",
        "mask",
        "mask",
        "wide",
        "SelfAttentionNarrow",
        "emb",
        "heads",
        "heads",
        "mask",
        "mask",
        "mask",
        "nn",
        "LayerNorm",
        "emb",
        "nn",
        "LayerNorm",
        "emb",
        "nn",
        "Sequential",
        "nn",
        "Linear",
        "emb",
        "ff_hidden_mult",
        "emb",
        "nn",
        "ReLU",
        "nn",
        "Linear",
        "ff_hidden_mult",
        "emb",
        "emb",
        "nn",
        "Dropout",
        "dropout",
        "x",
        "attention",
        "x",
        "norm1",
        "attended",
        "x",
        "x",
        "ff",
        "x",
        "norm2",
        "fedforward",
        "x",
        "x",
        "x",
        "matrices",
        "maskval",
        "mask_diagonal",
        "matrices",
        "size",
        "torch",
        "triu_indices",
        "h",
        "w",
        "offset",
        "mask_diagonal",
        "indices",
        "indices",
        "maskval",
        "nn",
        "Module",
        "emb",
        "heads",
        "mask",
        "emb",
        "heads",
        "mask",
        "nn",
        "Linear",
        "emb",
        "emb",
        "heads",
        "bias",
        "nn",
        "Linear",
        "emb",
        "emb",
        "heads",
        "bias",
        "nn",
        "Linear",
        "emb",
        "emb",
        "heads",
        "bias",
        "nn",
        "Linear",
        "heads",
        "emb",
        "emb",
        "x",
        "x",
        "size",
        "heads",
        "e",
        "emb",
        "e",
        "emb",
        "tokeys",
        "x",
        "view",
        "b",
        "t",
        "h",
        "e",
        "toqueries",
        "x",
        "view",
        "b",
        "t",
        "h",
        "e",
        "tovalues",
        "x",
        "view",
        "b",
        "t",
        "h",
        "e",
        "keys",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "e",
        "queries",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "e",
        "values",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "e",
        "queries",
        "e",
        "keys",
        "e",
        "torch",
        "bmm",
        "queries",
        "keys",
        "transpose",
        "dot",
        "size",
        "b",
        "h",
        "t",
        "t",
        "mask",
        "mask_",
        "dot",
        "maskval",
        "mask_diagonal",
        "F",
        "softmax",
        "dot",
        "dim",
        "torch",
        "bmm",
        "dot",
        "values",
        "view",
        "b",
        "h",
        "t",
        "e",
        "transpose",
        "contiguous",
        "view",
        "b",
        "t",
        "h",
        "e",
        "unifyheads",
        "nn",
        "Module",
        "emb",
        "heads",
        "mask",
        "emb",
        "heads",
        "emb",
        "heads",
        "emb",
        "heads",
        "mask",
        "emb",
        "heads",
        "nn",
        "Linear",
        "s",
        "s",
        "bias",
        "nn",
        "Linear",
        "s",
        "s",
        "bias",
        "nn",
        "Linear",
        "s",
        "s",
        "bias",
        "nn",
        "Linear",
        "heads",
        "s",
        "emb",
        "x",
        "x",
        "size",
        "heads",
        "e",
        "emb",
        "e",
        "emb",
        "e",
        "h",
        "x",
        "view",
        "b",
        "t",
        "h",
        "s",
        "tokeys",
        "x",
        "toqueries",
        "x",
        "tovalues",
        "x",
        "keys",
        "size",
        "b",
        "t",
        "h",
        "s",
        "queries",
        "size",
        "b",
        "t",
        "h",
        "s",
        "values",
        "size",
        "b",
        "t",
        "h",
        "s",
        "keys",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "s",
        "queries",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "s",
        "values",
        "transpose",
        "contiguous",
        "view",
        "b",
        "h",
        "t",
        "s",
        "queries",
        "e",
        "keys",
        "e",
        "torch",
        "bmm",
        "queries",
        "keys",
        "transpose",
        "dot",
        "size",
        "b",
        "h",
        "t",
        "t",
        "mask",
        "mask_",
        "dot",
        "maskval",
        "mask_diagonal",
        "F",
        "softmax",
        "dot",
        "dim",
        "torch",
        "bmm",
        "dot",
        "values",
        "view",
        "b",
        "h",
        "t",
        "s",
        "transpose",
        "contiguous",
        "view",
        "b",
        "t",
        "s",
        "h",
        "unifyheads"
    ],
    "literals": [
        "'device'",
        "'num_tokens'",
        "'seq_length'",
        "'emb_dim'",
        "'n_transformers'",
        "'emb'",
        "'heads'",
        "'n_att_heads'",
        "'mask'",
        "'wide'",
        "'wide'",
        "f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'",
        "'-inf'",
        "f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'",
        "f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'",
        "'-inf'"
    ],
    "variables": [
        "device",
        "num_tokens",
        "nt",
        "seq_length",
        "sl",
        "emb_dim",
        "ed",
        "n_transformers",
        "nr",
        "token_embedding",
        "pos_embedding",
        "trans_args",
        "trans_blocks",
        "tblocks",
        "toprobs",
        "batch",
        "x",
        "pos_range",
        "pos",
        "pos",
        "pos",
        "x",
        "x",
        "emb_mean",
        "emb_max",
        "x",
        "x",
        "x",
        "attention",
        "mask",
        "norm1",
        "norm2",
        "ff",
        "attended",
        "x",
        "x",
        "fedforward",
        "x",
        "x",
        "b",
        "h",
        "w",
        "indices",
        "matrices",
        "emb",
        "heads",
        "mask",
        "tokeys",
        "toqueries",
        "tovalues",
        "unifyheads",
        "b",
        "t",
        "e",
        "h",
        "keys",
        "queries",
        "values",
        "keys",
        "queries",
        "values",
        "queries",
        "keys",
        "dot",
        "dot",
        "emb",
        "heads",
        "mask",
        "s",
        "tokeys",
        "toqueries",
        "tovalues",
        "unifyheads",
        "b",
        "t",
        "e",
        "h",
        "s",
        "x",
        "keys",
        "queries",
        "values",
        "keys",
        "queries",
        "values",
        "queries",
        "keys",
        "dot",
        "dot"
    ],
    "comments": [
        "Based on TRANSFORMERS FROM SCRATCH",
        "http://peterbloem.nl/blog/transformers",
        "batch size",
        "Encode ASCII indexes to embedding vectors of dimension `emb_dim`",
        "[batch x seq_length x emb_dim]",
        "Create a range of possible positions 0..seq_length-1",
        "Encode the range of positions to vectors",
        "[ seq_len x emb_dim ]",
        "[ 1 x seq_len x emb_dim ]",
        "[ batch x seq_len x emb_dim ]",
        "[ batch x seq_len x emb_dim ]",
        "Output of the last transformer block",
        "[batch x seq_len x emb_dim]",
        "Extract embeddings by collapsing `seq_len` dimension",
        "see https://stackoverflow.com/questions/59030907",
        "[ batch*seq_len,  emb_dim ]",
        "[ batch*seq_len,  num_tokens ]",
        "[ batch, num_tokens, seq_len ]",
        "compute scaled dot-product self-attention",
        "- fold heads into the batch dimension",
        "- Instead of dividing the dot products by sqrt(e), we scale the keys and values.",
        "This should be more memory efficient",
        "- get dot product of queries and keys, and scale",
        "mask out the upper half of the dot matrix, excluding the diagonal",
        "- dot now has row-wise self-attention probabilities",
        "apply the self attention to the values",
        "swap h, t back, unify heads",
        "- We will break the embedding into `heads` chunks and feed each to a different attention head",
        "Compute scaled dot-product self-attention",
        "- fold heads into the batch dimension",
        "- Instead of dividing the dot products by sqrt(e), we scale the keys and values.",
        "This should be more memory efficient",
        "- get dot product of queries and keys, and scale",
        "mask out the upper half of the dot matrix, excluding the diagonal",
        "- dot now has row-wise self-attention probabilities",
        "apply the self attention to the values",
        "swap h, t back, unify heads"
    ],
    "docstrings": [
        "\"\"\"\n        :param x: A (batch, sequence length) integer tensor of token indices.\n        :return: predicted log-probability vectors for each token based on the preceding tokens.\n        \"\"\"",
        "\"\"\"\n    Masks out all values in the given batch of matrices where i <= j holds,\n    i < j if mask_diagonal is false\n    In place operation\n    :param tns:\n    :return:\n    \"\"\"",
        "\"\"\"\n        :param emb:\n        :param heads:\n        :param mask:\n        \"\"\"",
        "\"\"\"\n        :param emb:\n        :param heads:\n        :param mask:\n        \"\"\""
    ],
    "functions": [
        "forward",
        "forward",
        "mask_",
        "forward",
        "forward"
    ],
    "classes": [
        "Transformer",
        "TransformerBlock",
        "SelfAttentionWide",
        "SelfAttentionNarrow"
    ]
}