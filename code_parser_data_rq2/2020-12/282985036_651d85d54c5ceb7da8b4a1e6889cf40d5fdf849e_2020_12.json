{
    "identifiers": [
        "sys",
        "torch",
        "pickle",
        "transformers",
        "GPT2Tokenizer",
        "GPT2LMHeadModel",
        "get_linear_schedule_with_warmup",
        "AdamW",
        "sklearn",
        "metrics",
        "classification_report",
        "gpt2_coarse_finetune",
        "gpt2_tokenize",
        "test_generate",
        "create_data_loaders",
        "format_time",
        "basic_gpt2_tokenize",
        "torch",
        "utils",
        "data",
        "DataLoader",
        "SequentialSampler",
        "RandomSampler",
        "torch",
        "utils",
        "data",
        "TensorDataset",
        "torch",
        "nn",
        "CrossEntropyLoss",
        "numpy",
        "np",
        "random",
        "time",
        "os",
        "copy",
        "pytorch_memlab",
        "MemReporter",
        "tokenizer",
        "df",
        "index_to_label",
        "pad_token_dict",
        "max_length",
        "df",
        "text",
        "values",
        "len",
        "sentences",
        "i",
        "sent",
        "sentences",
        "ind_l",
        "index_to_label",
        "index_to_label",
        "ind_l",
        "join",
        "label",
        "split",
        "pad_token_dict",
        "label",
        "len",
        "temp_list",
        "processed_label_str",
        "join",
        "temp_list",
        "processed_label_str",
        "tokenizer",
        "encode_plus",
        "label_str",
        "sent",
        "truncation",
        "max_length",
        "max_length",
        "pad_to_max_length",
        "return_attention_mask",
        "return_tensors",
        "torch",
        "tensor",
        "tokenizer",
        "bos_token_id",
        "encoded_dict",
        "data",
        "tolist",
        "torch",
        "tensor",
        "encoded_dict",
        "data",
        "tolist",
        "sibling_input_ids",
        "append",
        "encoded_dict",
        "sibling_attn_masks",
        "append",
        "encoded_dict",
        "input_ids",
        "append",
        "torch",
        "cat",
        "sibling_input_ids",
        "dim",
        "attention_masks",
        "append",
        "torch",
        "cat",
        "sibling_attn_masks",
        "dim",
        "torch",
        "cat",
        "input_ids",
        "dim",
        "view",
        "num_sentences",
        "max_length",
        "torch",
        "cat",
        "attention_masks",
        "dim",
        "view",
        "num_sentences",
        "max_length",
        "input_ids",
        "attention_masks",
        "coarse_model",
        "fine_model",
        "coarse_tokenizer",
        "fine_tokenizer",
        "train_dataloader",
        "validation_dataloader",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "index_to_label",
        "device",
        "secondary_device",
        "batch_fine_probs",
        "batch_coarse_probs",
        "batch_fine_input_masks",
        "batch_coarse_input_masks",
        "batch_fine_input_ids",
        "batch_coarse_input_ids",
        "coarse_tokenizer",
        "fine_tokenizer",
        "doc_start_ind",
        "torch",
        "nn",
        "KLDivLoss",
        "reduction",
        "batch_fine_probs",
        "shape",
        "b",
        "batch_size",
        "batch_fine_probs",
        "b",
        "batch_coarse_probs",
        "b",
        "batch_fine_input_masks",
        "b",
        "batch_coarse_input_masks",
        "b",
        "torch",
        "all",
        "fine_mask",
        "eq",
        "coarse_mask",
        "fine_tokenizer",
        "decode",
        "batch_fine_input_ids",
        "b",
        "coarse_tokenizer",
        "decode",
        "batch_coarse_input_ids",
        "b",
        "fine_tokenizer",
        "decode",
        "batch_fine_input_ids",
        "b",
        "doc_start_ind",
        "coarse_tokenizer",
        "decode",
        "batch_coarse_input_ids",
        "b",
        "doc_start_ind",
        "fine_dec_sent",
        "coarse_dec_sent",
        "fine_tokenizer",
        "decode",
        "batch_fine_input_ids",
        "b",
        "doc_start_ind",
        "coarse_tokenizer",
        "decode",
        "batch_coarse_input_ids",
        "b",
        "doc_start_ind",
        "fine_mask",
        "unsqueeze",
        "expand_as",
        "fine_logits_ind",
        "coarse_mask",
        "unsqueeze",
        "expand_as",
        "coarse_logits_ind",
        "torch",
        "masked_select",
        "fine_logits_ind",
        "fine_maski",
        "view",
        "fine_logits_ind",
        "size",
        "torch",
        "masked_select",
        "coarse_logits_ind",
        "coarse_maski",
        "view",
        "coarse_logits_ind",
        "size",
        "fine_logits_pad_removed",
        "doc_start_ind",
        "contiguous",
        "coarse_logits_pad_removed",
        "doc_start_ind",
        "contiguous",
        "loss_fct",
        "shift_fine_logits",
        "shift_coarse_logits",
        "unsqueeze",
        "losses",
        "append",
        "loss",
        "torch",
        "cat",
        "losses",
        "dim",
        "losses",
        "mean",
        "fine_model",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "device",
        "CrossEntropyLoss",
        "random",
        "sample",
        "label_to_exclusive_dataloader",
        "keys",
        "selected_labs",
        "l",
        "selected_labs",
        "label_to_exclusive_dataloader",
        "l",
        "step",
        "batch",
        "dataloader",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "fine_model",
        "b_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_input_mask",
        "labels",
        "b_labels",
        "b_labels_list",
        "append",
        "b_labels",
        "b_input_ids_list",
        "append",
        "b_input_ids",
        "b_input_mask_list",
        "append",
        "b_input_mask",
        "scores_list",
        "append",
        "outputs",
        "it",
        "it",
        "torch",
        "cat",
        "b_labels_list",
        "dim",
        "torch",
        "cat",
        "b_input_ids_list",
        "dim",
        "torch",
        "cat",
        "b_input_mask_list",
        "dim",
        "torch",
        "cat",
        "scores_list",
        "dim",
        "b_labels_tensor",
        "shape",
        "b_input_ids_tensor",
        "shape",
        "b_input_mask_tensor",
        "shape",
        "scores_tensor",
        "shape",
        "scores_tensor",
        "shape",
        "b",
        "batch_size",
        "scores_tensor",
        "b",
        "b_labels_tensor",
        "b",
        "b_input_mask_tensor",
        "b",
        "mask",
        "unsqueeze",
        "expand_as",
        "logits_ind",
        "torch",
        "masked_select",
        "logits_ind",
        "maski",
        "view",
        "logits_ind",
        "size",
        "torch",
        "masked_select",
        "labels_ind",
        "mask",
        "logits_pad_removed",
        "doc_start_ind",
        "contiguous",
        "labels_pad_removed",
        "doc_start_ind",
        "contiguous",
        "logits_collected",
        "append",
        "shift_logits",
        "view",
        "shift_logits",
        "size",
        "labels_collected",
        "append",
        "shift_labels",
        "view",
        "torch",
        "cat",
        "logits_collected",
        "dim",
        "torch",
        "cat",
        "labels_collected",
        "dim",
        "loss_function",
        "logits_collected",
        "labels_collected",
        "to",
        "device",
        "loss",
        "batch_fine_probs",
        "batch_coarse_probs",
        "batch_fine_input_masks",
        "batch_coarse_input_masks",
        "batch_fine_input_ids",
        "batch_coarse_input_ids",
        "coarse_tokenizer",
        "fine_tokenizer",
        "fine_model",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "device",
        "lambda_1",
        "is_val",
        "calculate_kl_div_loss",
        "batch_fine_probs",
        "batch_coarse_probs",
        "batch_fine_input_masks",
        "batch_coarse_input_masks",
        "batch_fine_input_ids",
        "batch_coarse_input_ids",
        "coarse_tokenizer",
        "fine_tokenizer",
        "doc_start_ind",
        "is_val",
        "calculate_cross_entropy_loss",
        "fine_model",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "device",
        "kl_div_loss",
        "item",
        "cross_ent_loss",
        "item",
        "kl_div_loss",
        "item",
        "cross_ent_loss",
        "lambda_1",
        "kl_div_loss",
        "lambda_1",
        "cross_ent_loss",
        "step",
        "max_steps",
        "step",
        "max_steps",
        "temp",
        "temp",
        "torch",
        "nn",
        "Parameter",
        "torch",
        "ones",
        "len",
        "index_to_label",
        "to",
        "device",
        "AdamW",
        "fine_model",
        "parameters",
        "fine_posterior",
        "lr",
        "eps",
        "len",
        "train_dataloader",
        "epochs",
        "get_linear_schedule_with_warmup",
        "optimizer",
        "num_warmup_steps",
        "warmup_steps",
        "num_training_steps",
        "total_steps",
        "random",
        "seed",
        "seed_val",
        "np",
        "random",
        "seed",
        "seed_val",
        "torch",
        "manual_seed",
        "seed_val",
        "torch",
        "cuda",
        "manual_seed_all",
        "seed_val",
        "time",
        "time",
        "coarse_model",
        "eval",
        "epoch_i",
        "epochs",
        "flush",
        "format",
        "epoch_i",
        "epochs",
        "flush",
        "flush",
        "time",
        "time",
        "fine_model",
        "train",
        "step",
        "batch",
        "train_dataloader",
        "step",
        "sample_every",
        "step",
        "format_time",
        "time",
        "time",
        "t0",
        "format",
        "step",
        "len",
        "train_dataloader",
        "elapsed",
        "flush",
        "fine_model",
        "eval",
        "random",
        "choice",
        "index_to_label",
        "values",
        "pad_token_dict",
        "lbl",
        "len",
        "temp_list",
        "join",
        "lbl",
        "split",
        "join",
        "temp_list",
        "join",
        "lbl",
        "split",
        "fine_tokenizer",
        "bos_token",
        "label_str",
        "fine_model",
        "generate",
        "input_ids",
        "fine_tokenizer",
        "encode",
        "text",
        "return_tensors",
        "to",
        "device",
        "do_sample",
        "top_k",
        "max_length",
        "top_p",
        "num_return_sequences",
        "i",
        "sample_output",
        "sample_outputs",
        "format",
        "i",
        "fine_tokenizer",
        "decode",
        "sample_output",
        "flush",
        "fine_model",
        "train",
        "torch",
        "log_softmax",
        "fine_posterior",
        "dim",
        "torch",
        "softmax",
        "fine_posterior",
        "dim",
        "flush",
        "batch",
        "to",
        "secondary_device",
        "batch",
        "to",
        "secondary_device",
        "batch",
        "to",
        "secondary_device",
        "b_coarse_input_ids",
        "shape",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "coarse_model",
        "zero_grad",
        "optimizer",
        "zero_grad",
        "coarse_model",
        "b_coarse_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_coarse_input_mask",
        "labels",
        "b_coarse_labels",
        "torch",
        "softmax",
        "outputs",
        "dim",
        "to",
        "device",
        "b_coarse_input_ids",
        "to",
        "device",
        "b_coarse_input_mask",
        "to",
        "device",
        "b_ind",
        "b_size",
        "l_ind",
        "index_to_label",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_mask_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "fine_model",
        "b_fine_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_fine_input_mask",
        "labels",
        "b_fine_labels",
        "b_fine_labels",
        "to",
        "secondary_device",
        "torch",
        "log_softmax",
        "outputs",
        "dim",
        "fine_label_sum_log_probs",
        "append",
        "fine_log_probs",
        "fine_posterior_log_probs",
        "l_ind",
        "torch",
        "cat",
        "fine_label_sum_log_probs",
        "dim",
        "batch_fine_probs",
        "append",
        "fine_label_sum_log_probs",
        "unsqueeze",
        "batch_fine_input_ids",
        "append",
        "b_fine_input_ids",
        "batch_fine_input_masks",
        "append",
        "b_fine_input_mask",
        "torch",
        "cat",
        "batch_fine_probs",
        "dim",
        "torch",
        "cat",
        "batch_fine_input_masks",
        "dim",
        "torch",
        "cat",
        "batch_fine_input_ids",
        "dim",
        "torch",
        "logsumexp",
        "batch_fine_probs",
        "dim",
        "calculate_loss",
        "batch_fine_log_probs",
        "batch_coarse_probs",
        "batch_fine_input_masks",
        "b_coarse_input_mask",
        "batch_fine_input_ids",
        "b_coarse_input_ids",
        "coarse_tokenizer",
        "fine_tokenizer",
        "fine_model",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "device",
        "lambda_1",
        "compute_lambda",
        "global_step",
        "max_steps",
        "len",
        "train_dataloader",
        "epochs",
        "total_train_loss",
        "loss",
        "item",
        "loss",
        "item",
        "flush",
        "loss",
        "backward",
        "optimizer",
        "step",
        "scheduler",
        "step",
        "global_step",
        "total_train_loss",
        "len",
        "train_dataloader",
        "format_time",
        "time",
        "time",
        "t0",
        "flush",
        "format",
        "avg_train_loss",
        "flush",
        "format",
        "training_time",
        "flush",
        "flush",
        "flush",
        "time",
        "time",
        "fine_model",
        "eval",
        "batch",
        "validation_dataloader",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "b_coarse_input_ids",
        "shape",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "torch",
        "no_grad",
        "torch",
        "log_softmax",
        "fine_posterior",
        "dim",
        "coarse_model",
        "b_coarse_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_coarse_input_mask",
        "labels",
        "b_coarse_labels",
        "torch",
        "softmax",
        "outputs",
        "dim",
        "b_ind",
        "b_size",
        "l_ind",
        "index_to_label",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_mask_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "fine_model",
        "b_fine_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_fine_input_mask",
        "labels",
        "b_fine_labels",
        "torch",
        "log_softmax",
        "outputs",
        "dim",
        "fine_label_sum_log_probs",
        "append",
        "fine_log_probs",
        "fine_posterior_log_probs",
        "l_ind",
        "torch",
        "cat",
        "fine_label_sum_log_probs",
        "dim",
        "batch_fine_probs",
        "append",
        "fine_label_sum_log_probs",
        "unsqueeze",
        "batch_fine_input_ids",
        "append",
        "b_fine_input_ids",
        "batch_fine_input_masks",
        "append",
        "b_fine_input_mask",
        "torch",
        "cat",
        "batch_fine_probs",
        "dim",
        "torch",
        "cat",
        "batch_fine_input_masks",
        "dim",
        "torch",
        "cat",
        "batch_fine_input_ids",
        "dim",
        "torch",
        "logsumexp",
        "batch_fine_probs",
        "dim",
        "calculate_loss",
        "batch_fine_log_probs",
        "batch_coarse_probs",
        "batch_fine_input_masks",
        "b_coarse_input_mask",
        "batch_fine_input_ids",
        "b_coarse_input_ids",
        "coarse_tokenizer",
        "fine_tokenizer",
        "fine_model",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "device",
        "is_val",
        "lambda_1",
        "compute_lambda",
        "global_step",
        "max_steps",
        "len",
        "train_dataloader",
        "epochs",
        "total_eval_loss",
        "loss",
        "item",
        "total_eval_loss",
        "len",
        "validation_dataloader",
        "format_time",
        "time",
        "time",
        "t0",
        "format",
        "avg_val_loss",
        "flush",
        "format",
        "validation_time",
        "flush",
        "training_stats",
        "append",
        "epoch_i",
        "avg_train_loss",
        "avg_val_loss",
        "training_time",
        "validation_time",
        "flush",
        "flush",
        "format",
        "format_time",
        "time",
        "time",
        "total_t0",
        "flush",
        "fine_posterior",
        "fine_model",
        "p",
        "parent_to_child",
        "coarse_tokenizer",
        "fine_tokenizer",
        "parent_to_child",
        "p",
        "coarse_tokenizer",
        "tokenize",
        "p",
        "len",
        "parent_tokens",
        "ch",
        "children",
        "max",
        "len",
        "fine_tokenizer",
        "tokenize",
        "join",
        "ch",
        "split",
        "max_num",
        "max_num",
        "len",
        "parent_tokens",
        "ch",
        "children",
        "len",
        "fine_tokenizer",
        "tokenize",
        "join",
        "ch",
        "split",
        "max_num",
        "ch_tokens",
        "max_num",
        "doc_start_ind",
        "pad_token_dict",
        "fine_model",
        "fine_posterior",
        "fine_input_ids",
        "fine_attention_masks",
        "doc_start_ind",
        "index_to_label",
        "label_to_index",
        "true_labels",
        "device",
        "copy",
        "deepcopy",
        "true_labels",
        "i",
        "l",
        "labels",
        "label_to_index",
        "l",
        "np",
        "array",
        "labels",
        "dtype",
        "torch",
        "LongTensor",
        "labels",
        "TensorDataset",
        "fine_input_ids",
        "fine_attention_masks",
        "labels",
        "SequentialSampler",
        "prediction_data",
        "DataLoader",
        "prediction_data",
        "sampler",
        "prediction_sampler",
        "batch_size",
        "batch_size",
        "fine_model",
        "eval",
        "batch",
        "prediction_dataloader",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "batch",
        "to",
        "device",
        "b_fine_input_ids_minibatch",
        "shape",
        "torch",
        "no_grad",
        "torch",
        "log_softmax",
        "fine_posterior",
        "dim",
        "b_ind",
        "b_size",
        "l_ind",
        "index_to_label",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_ids_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "b_fine_input_mask_minibatch",
        "b_ind",
        "l_ind",
        "unsqueeze",
        "to",
        "device",
        "fine_model",
        "b_fine_input_ids",
        "token_type_ids",
        "attention_mask",
        "b_fine_input_mask",
        "labels",
        "b_fine_labels",
        "b_fine_input_mask",
        "torch",
        "log_softmax",
        "outputs",
        "dim",
        "mask",
        "unsqueeze",
        "expand_as",
        "fine_logits",
        "torch",
        "masked_select",
        "fine_logits",
        "maski",
        "view",
        "fine_logits",
        "size",
        "unsqueeze",
        "fine_logits_pad_removed",
        "doc_start_ind",
        "torch",
        "masked_select",
        "b_fine_labels",
        "mask",
        "unsqueeze",
        "b_fine_labels_pad_removed",
        "doc_start_ind",
        "fine_logits_pad_removed",
        "gather",
        "b_fine_labels_pad_removed",
        "unsqueeze",
        "dim",
        "squeeze",
        "dim",
        "squeeze",
        "dim",
        "label_log_probs",
        "append",
        "fine_posterior_log_probs",
        "l_ind",
        "fine_log_probs",
        "sum",
        "torch",
        "tensor",
        "label_log_probs",
        "unsqueeze",
        "batch_fine_logits",
        "append",
        "label_log_probs",
        "torch",
        "cat",
        "batch_fine_logits",
        "dim",
        "logits",
        "append",
        "batch_fine_logits",
        "predictions",
        "append",
        "batch_fine_logits",
        "detach",
        "cpu",
        "numpy",
        "b_cls_labels",
        "to",
        "numpy",
        "true_labels",
        "append",
        "label_ids",
        "torch",
        "cat",
        "logits",
        "dim",
        "pred",
        "predictions",
        "preds",
        "pred",
        "argmax",
        "axis",
        "t",
        "true_labels",
        "t",
        "i",
        "t",
        "index_to_label",
        "t",
        "index_to_label",
        "preds",
        "i",
        "classification_report",
        "preds",
        "flush",
        "preds",
        "logits",
        "dataloader",
        "step",
        "batch",
        "dataloader",
        "step",
        "batch",
        "basepath",
        "dataset",
        "pkl_dump_dir",
        "pkl_dump_dir",
        "pkl_dump_dir",
        "sys",
        "argv",
        "sys",
        "argv",
        "sys",
        "argv",
        "sys",
        "argv",
        "sys",
        "argv",
        "sys",
        "argv",
        "iteration",
        "pkl_dump_dir",
        "pkl_dump_dir",
        "iteration",
        "use_gpu",
        "torch",
        "device",
        "gpu_id",
        "torch",
        "device",
        "secondary_gpu_id",
        "torch",
        "device",
        "torch",
        "device",
        "pickle",
        "load",
        "open",
        "pkl_dump_dir",
        "pickle",
        "load",
        "open",
        "pkl_dump_dir",
        "df",
        "label",
        "values",
        "GPT2Tokenizer",
        "from_pretrained",
        "coarse_tok_path",
        "do_lower_case",
        "torch",
        "load",
        "model_path",
        "model_name",
        "map_location",
        "device",
        "coarse_model",
        "to",
        "secondary_device",
        "random",
        "seed",
        "seed_val",
        "np",
        "random",
        "seed",
        "seed_val",
        "torch",
        "manual_seed",
        "seed_val",
        "torch",
        "cuda",
        "manual_seed_all",
        "seed_val",
        "torch",
        "backends",
        "cudnn",
        "torch",
        "backends",
        "cudnn",
        "p",
        "parent_label",
        "p",
        "base_fine_path",
        "p",
        "os",
        "makedirs",
        "fine_label_path",
        "exist_ok",
        "fine_label_path",
        "fine_label_path",
        "os",
        "makedirs",
        "fine_tok_path",
        "exist_ok",
        "os",
        "makedirs",
        "fine_model_path",
        "exist_ok",
        "GPT2Tokenizer",
        "from_pretrained",
        "bos_token",
        "pad_token",
        "additional_special_tokens",
        "GPT2LMHeadModel",
        "from_pretrained",
        "fine_model",
        "resize_token_embeddings",
        "len",
        "fine_tokenizer",
        "fine_model",
        "to",
        "device",
        "parent_to_child",
        "p",
        "i",
        "l",
        "children",
        "i",
        "l",
        "create_pad_token_dict",
        "p",
        "parent_to_child",
        "coarse_tokenizer",
        "fine_tokenizer",
        "pad_token_dict",
        "doc_start_ind",
        "pickle",
        "dump",
        "pad_token_dict",
        "open",
        "fine_label_path",
        "df",
        "df",
        "label",
        "isin",
        "children",
        "reset_index",
        "drop",
        "p",
        "len",
        "temp_df",
        "text",
        "values",
        "p",
        "gpt2_tokenize",
        "coarse_tokenizer",
        "temp_df",
        "text",
        "values",
        "temp_coarse_lbls",
        "pad_token_dict",
        "temp_coarse_label_to_index",
        "gpt2_fine_tokenize",
        "fine_tokenizer",
        "temp_df",
        "index_to_label",
        "pad_token_dict",
        "TensorDataset",
        "coarse_input_ids",
        "coarse_attention_masks",
        "fine_input_ids",
        "fine_attention_masks",
        "create_data_loaders",
        "dataset",
        "batch_size",
        "ch",
        "children",
        "pickle",
        "load",
        "open",
        "exclusive_df_dir",
        "ch",
        "iteration",
        "len",
        "child_df",
        "n",
        "child_df",
        "sample",
        "n",
        "n",
        "random_state",
        "reset_index",
        "drop",
        "ch",
        "len",
        "child_df",
        "text",
        "values",
        "basic_gpt2_tokenize",
        "fine_tokenizer",
        "child_df",
        "text",
        "values",
        "temp_child_lbls",
        "pad_token_dict",
        "TensorDataset",
        "child_exc_input_ids",
        "child_exc_attention_masks",
        "DataLoader",
        "child_exc_dataset",
        "sampler",
        "SequentialSampler",
        "child_exc_dataset",
        "batch_size",
        "func",
        "dataloader",
        "train",
        "coarse_model",
        "fine_model",
        "coarse_tokenizer",
        "fine_tokenizer",
        "train_dataloader",
        "validation_dataloader",
        "label_to_exclusive_dataloader",
        "doc_start_ind",
        "index_to_label",
        "device",
        "secondary_device",
        "test_generate",
        "fine_model",
        "fine_tokenizer",
        "children",
        "pad_token_dict",
        "device",
        "fine_tokenizer",
        "save_pretrained",
        "fine_tok_path",
        "torch",
        "save",
        "fine_model",
        "fine_model_path",
        "p",
        "torch",
        "save",
        "fine_posterior",
        "fine_label_path",
        "pickle",
        "dump",
        "index_to_label",
        "open",
        "fine_label_path",
        "pickle",
        "dump",
        "label_to_index",
        "open",
        "fine_label_path",
        "flush"
    ],
    "literals": [
        "\" \"",
        "\"_\"",
        "\"<|labelpad|>\"",
        "\" \"",
        "\" \"",
        "\" <|labelsep|> \"",
        "'pt'",
        "'input_ids'",
        "'input_ids'",
        "'attention_mask'",
        "'attention_mask'",
        "'input_ids'",
        "'attention_mask'",
        "\"batchmean\"",
        "\"Fine sentence\"",
        "\"Coarse sentence\"",
        "\"Fine and Coarse mask is not same\"",
        "\"Fine sentence \"",
        "\"Coarse sentence \"",
        "\"Fine and Coarse decoded sentence is not same\"",
        "\"KL-loss\"",
        "\"CE-loss\"",
        "\"KL-loss\"",
        "\"CE-loss\"",
        "\"\"",
        "'======== Epoch {:} / {:} ========'",
        "'Training...'",
        "'  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'",
        "\"<|labelpad|>\"",
        "\" \"",
        "\"_\"",
        "\" \"",
        "\" \"",
        "\" \"",
        "\"_\"",
        "\" \"",
        "\" <|labelsep|> \"",
        "'pt'",
        "\"{}: {}\"",
        "\"Loss:\"",
        "\"\"",
        "\"  Average training loss: {0:.2f}\"",
        "\"  Training epoch took: {:}\"",
        "\"\"",
        "\"Running Validation...\"",
        "\"  Validation Loss: {0:.2f}\"",
        "\"  Validation took: {:}\"",
        "'epoch'",
        "'Training Loss'",
        "'Valid. Loss'",
        "'Training Time'",
        "'Validation Time'",
        "\"\"",
        "\"Training complete!\"",
        "\"Total training took {:} (h:mm:ss)\"",
        "\" \"",
        "\"_\"",
        "\" \"",
        "\"_\"",
        "'int32'",
        "'cpu'",
        "\"__main__\"",
        "\"/data4/dheeraj/coarse2fine/\"",
        "\"nyt/\"",
        "\"gpt2/fine/\"",
        "\"gpt2/tokenizer_coarse\"",
        "\"gpt2/model/\"",
        "\"coarse.pt\"",
        "\"exclusive/\"",
        "\"exclusive_\"",
        "\"it/\"",
        "'cuda:'",
        "'cuda:'",
        "\"cpu\"",
        "\"cpu\"",
        "\"df_fine.pkl\"",
        "\"rb\"",
        "\"parent_to_child.pkl\"",
        "\"rb\"",
        "\"Training coarse label:\"",
        "\"/tokenizer\"",
        "\"/model/\"",
        "'gpt2'",
        "'<|startoftext|>'",
        "'<|pad|>'",
        "'<|labelsep|>'",
        "'<|labelpad|>'",
        "'gpt2'",
        "\"/pad_token_dict.pkl\"",
        "\"wb\"",
        "\"Pad token dict Dumped\"",
        "\".pkl\"",
        "\"rb\"",
        "\".pt\"",
        "\"/fine_posterior.pt\"",
        "\"/index_to_label.pkl\"",
        "\"wb\"",
        "\"/label_to_index.pkl\"",
        "\"wb\"",
        "\"*\"",
        "\"*\""
    ],
    "variables": [
        "input_ids",
        "attention_masks",
        "sentences",
        "num_sentences",
        "sibling_input_ids",
        "sibling_attn_masks",
        "label",
        "processed_label_str",
        "temp_list",
        "label_str",
        "label_str",
        "encoded_dict",
        "encoded_dict",
        "encoded_dict",
        "input_ids",
        "attention_masks",
        "loss_fct",
        "batch_size",
        "losses",
        "fine_logits_ind",
        "coarse_logits_ind",
        "fine_mask",
        "coarse_mask",
        "fine_dec_sent",
        "coarse_dec_sent",
        "fine_maski",
        "coarse_maski",
        "fine_logits_pad_removed",
        "coarse_logits_pad_removed",
        "shift_fine_logits",
        "shift_coarse_logits",
        "loss",
        "losses",
        "loss_function",
        "b_labels_list",
        "b_input_ids_list",
        "b_input_mask_list",
        "scores_list",
        "selected_labs",
        "dataloader",
        "it",
        "b_input_ids",
        "b_labels",
        "b_input_mask",
        "outputs",
        "b_labels_tensor",
        "b_input_ids_tensor",
        "b_input_mask_tensor",
        "scores_tensor",
        "batch_size",
        "logits_collected",
        "labels_collected",
        "logits_ind",
        "labels_ind",
        "mask",
        "maski",
        "logits_pad_removed",
        "labels_pad_removed",
        "shift_logits",
        "shift_labels",
        "logits_collected",
        "labels_collected",
        "loss",
        "kl_div_loss",
        "cross_ent_loss",
        "cross_ent_loss",
        "temp",
        "fine_posterior",
        "optimizer",
        "sample_every",
        "warmup_steps",
        "epochs",
        "total_steps",
        "scheduler",
        "seed_val",
        "training_stats",
        "total_t0",
        "global_step",
        "t0",
        "total_train_loss",
        "elapsed",
        "lbl",
        "temp_list",
        "label_str",
        "label_str",
        "text",
        "sample_outputs",
        "fine_posterior_log_probs",
        "b_coarse_input_ids",
        "b_coarse_labels",
        "b_coarse_input_mask",
        "b_size",
        "b_fine_input_ids_minibatch",
        "b_fine_input_mask_minibatch",
        "outputs",
        "batch_coarse_probs",
        "b_coarse_input_ids",
        "b_coarse_input_mask",
        "batch_fine_probs",
        "batch_fine_input_masks",
        "batch_fine_input_ids",
        "fine_label_sum_log_probs",
        "b_fine_input_ids",
        "b_fine_labels",
        "b_fine_input_mask",
        "outputs",
        "b_fine_labels",
        "fine_log_probs",
        "fine_label_sum_log_probs",
        "batch_fine_probs",
        "batch_fine_input_masks",
        "batch_fine_input_ids",
        "batch_fine_log_probs",
        "loss",
        "avg_train_loss",
        "training_time",
        "t0",
        "total_eval_loss",
        "nb_eval_steps",
        "b_coarse_input_ids",
        "b_coarse_labels",
        "b_coarse_input_mask",
        "b_size",
        "b_fine_input_ids_minibatch",
        "b_fine_input_mask_minibatch",
        "fine_posterior_log_probs",
        "outputs",
        "batch_coarse_probs",
        "batch_fine_probs",
        "batch_fine_input_masks",
        "batch_fine_input_ids",
        "fine_label_sum_log_probs",
        "b_fine_input_ids",
        "b_fine_labels",
        "b_fine_input_mask",
        "outputs",
        "fine_log_probs",
        "fine_label_sum_log_probs",
        "batch_fine_probs",
        "batch_fine_input_masks",
        "batch_fine_input_ids",
        "batch_fine_log_probs",
        "loss",
        "avg_val_loss",
        "validation_time",
        "pad_token_dict",
        "children",
        "parent_tokens",
        "max_num",
        "max_num",
        "pad_token_dict",
        "p",
        "ch_tokens",
        "pad_token_dict",
        "ch",
        "doc_start_ind",
        "batch_size",
        "labels",
        "labels",
        "i",
        "labels",
        "labels",
        "prediction_data",
        "prediction_sampler",
        "prediction_dataloader",
        "predictions",
        "true_labels",
        "logits",
        "b_fine_input_ids_minibatch",
        "b_fine_input_mask_minibatch",
        "b_cls_labels",
        "b_size",
        "fine_posterior_log_probs",
        "batch_fine_logits",
        "label_log_probs",
        "b_fine_input_ids",
        "b_fine_labels",
        "b_fine_input_mask",
        "outputs",
        "mask",
        "fine_logits",
        "maski",
        "fine_logits_pad_removed",
        "fine_logits_pad_removed",
        "b_fine_labels_pad_removed",
        "b_fine_labels_pad_removed",
        "fine_log_probs",
        "label_log_probs",
        "batch_fine_logits",
        "label_ids",
        "logits",
        "preds",
        "preds",
        "i",
        "preds",
        "i",
        "basepath",
        "dataset",
        "pkl_dump_dir",
        "base_fine_path",
        "coarse_tok_path",
        "model_path",
        "model_name",
        "use_gpu",
        "gpu_id",
        "secondary_gpu_id",
        "parent_label",
        "iteration",
        "n",
        "exclusive_df_dir",
        "exclusive_df_dir",
        "device",
        "secondary_device",
        "device",
        "secondary_device",
        "df",
        "parent_to_child",
        "fine_labels",
        "coarse_tokenizer",
        "coarse_model",
        "seed_val",
        "deterministic",
        "benchmark",
        "all_true",
        "all_preds",
        "fine_label_path",
        "fine_tok_path",
        "fine_model_path",
        "fine_tokenizer",
        "fine_model",
        "children",
        "label_to_index",
        "index_to_label",
        "label_to_index",
        "l",
        "index_to_label",
        "i",
        "doc_start_ind",
        "pad_token_dict",
        "temp_df",
        "temp_coarse_lbls",
        "temp_coarse_label_to_index",
        "coarse_input_ids",
        "coarse_attention_masks",
        "_",
        "fine_input_ids",
        "fine_attention_masks",
        "dataset",
        "train_dataloader",
        "validation_dataloader",
        "label_to_exclusive_dataloader",
        "child_df",
        "child_df",
        "temp_child_lbls",
        "child_exc_input_ids",
        "child_exc_attention_masks",
        "child_exc_dataset",
        "dataloader",
        "label_to_exclusive_dataloader",
        "ch",
        "fine_posterior",
        "fine_model"
    ],
    "comments": [
        "For every sentence...",
        "Sentence to encode.",
        "Pad & truncate all sentences.",
        "Construct attn. masks.",
        "Return pytorch tensors.",
        "Add the encoded sentence to the list.",
        "And its attention mask (simply differentiates padding from non-padding).",
        "Convert the lists into tensors.",
        "Remove pad tokens",
        "consider from doc_start_ind - 1",
        "seq_len x |V|",
        "seq_len x |V|",
        "unpad_seq_len x |V|",
        "Compute loss here of shift_fine_logits and shift_coarse_logits append to losses",
        "Return mean of losses here",
        "print(\"Label\", l)",
        "print(\"Step for exc\", step, it)",
        "reporter = MemReporter()",
        "reporter.report()",
        "seq_len x |V|",
        "seq_len",
        "unpad_seq_len x |V|",
        "unpad_seq_len",
        "Flatten the tokens",
        "del batch_fine_probs",
        "del batch_coarse_probs",
        "del batch_fine_input_masks",
        "del batch_coarse_input_masks",
        "del batch_fine_input_ids",
        "del batch_coarse_input_ids",
        "torch.cuda.empty_cache()",
        "epsilon = 1e-20  # Defined to avoid log probability getting undefined.",
        "args.learning_rate - default is 5e-5, our notebook had 2e-5",
        "args.adam_epsilon  - default is 1e-8.",
        "batch contains -> coarse_input_ids, coarse_attention_masks, fine_input_ids, fine_attention_masks",
        "fine_model.zero_grad()",
        "(b_size, seq_len, |V|)",
        "(|F|, seq_len, |V|)",
        "(b_size, |F|, seq_len, |V|)",
        "(b_size, seq_len)",
        "(b_size, seq_len)",
        "This computes logsum_i P(f_i|c) P(D|f_i)",
        "loss = criterion(batch_fine_probs.log(), batch_coarse_probs.detach()).sum(dim=-1).mean(dim=-1).mean(dim=-1)",
        "Calculate the average loss over all of the batches.",
        "Measure how long this epoch took.",
        "========================================",
        "Validation",
        "========================================",
        "After the completion of each training epoch, measure our performance on",
        "our validation set.",
        "Evaluate data for one epoch",
        "batch contains -> coarse_input_ids, coarse_attention_masks, fine_input_ids, fine_attention_masks",
        "(b_size, seq_len, |V|)",
        "(|F|, seq_len, |V|)",
        "(b_size, |F|, seq_len, |V|)",
        "(b_size, seq_len)",
        "(b_size, seq_len)",
        "This computes logsum_i P(f_i|c) P(D|f_i)",
        "Accumulate the validation loss.",
        "Calculate the average loss over all of the batches.",
        "Measure how long the validation run took.",
        "Record all statistics from this epoch.",
        "todo make temp_df, fine_input_ids, fine_attention_masks class variables.",
        "true, preds, _ = test(fine_model, fine_posterior, fine_input_ids, fine_attention_masks, doc_start_ind,",
        "index_to_label, label_to_index, list(temp_df.label.values), device)",
        "this gives the token from which the document starts in the inputids, 1 for the starttoken, max_num for label infor, 1 for label_sup",
        "Set the batch size.",
        "Create the DataLoader.",
        "Tracking variables",
        "batch contains -> fine_input_ids, fine_attention_masks, fine_grained_labels",
        "basepath = \"/Users/dheerajmekala/Work/Coarse2Fine/data/\"",
        "use_gpu = False",
        "Tell pytorch to run this model on the GPU.",
        "fine_model = torch.nn.DataParallel(fine_model, device_ids=[1, 2])",
        "The training samples.",
        "Select batches randomly",
        "Trains with this batch size.",
        "true, preds, _ = test(fine_model, fine_posterior, fine_input_ids, fine_attention_masks, doc_start_ind,",
        "index_to_label, label_to_index, list(temp_df.label.values), device)",
        "all_true += true",
        "all_preds += preds",
        "print(classification_report(all_true, all_preds), flush=True)"
    ],
    "docstrings": [],
    "functions": [
        "gpt2_fine_tokenize",
        "train",
        "calculate_kl_div_loss",
        "calculate_cross_entropy_loss",
        "calculate_loss",
        "compute_lambda",
        "create_pad_token_dict",
        "test",
        "func"
    ],
    "classes": []
}