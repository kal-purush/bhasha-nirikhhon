{
    "identifiers": [
        "numpy",
        "postingList",
        "classVec",
        "dataSet",
        "dataSet",
        "vocabSet",
        "vocabSet",
        "vocabList",
        "inputSet",
        "len",
        "vocabList",
        "word",
        "inputSet",
        "word",
        "vocabList",
        "vocabList",
        "index",
        "word",
        "word",
        "returnVec",
        "vocabList",
        "inputSet",
        "len",
        "vocabList",
        "word",
        "inputSet",
        "word",
        "vocabList",
        "returnVec",
        "vocabList",
        "index",
        "word",
        "returnVec",
        "trainMatrix",
        "trainCategory",
        "len",
        "trainMatrix",
        "len",
        "trainMatrix",
        "sum",
        "trainCategory",
        "numTrainDocs",
        "ones",
        "numWords",
        "ones",
        "numWords",
        "i",
        "numTrainDocs",
        "trainCategory",
        "i",
        "p1Num",
        "trainMatrix",
        "i",
        "p1Denom",
        "sum",
        "trainMatrix",
        "i",
        "p0Num",
        "trainMatrix",
        "i",
        "p0Denom",
        "sum",
        "trainMatrix",
        "i",
        "log",
        "p1Num",
        "p1Denom",
        "log",
        "p0Num",
        "p0Denom",
        "p0Vect",
        "p1Vect",
        "pAbusive",
        "vec2Classify",
        "p0Vec",
        "p1Vec",
        "pClass1",
        "sum",
        "vec2Classify",
        "p1Vec",
        "log",
        "pClass1",
        "sum",
        "vec2Classify",
        "p0Vec",
        "log",
        "pClass1",
        "p1",
        "p0",
        "bigString",
        "re",
        "re",
        "split",
        "bigString",
        "tok",
        "lower",
        "tok",
        "listOfTokens",
        "len",
        "tok",
        "i",
        "textParse",
        "open",
        "i",
        "read",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "textParse",
        "open",
        "i",
        "read",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "createVocabList",
        "docList",
        "i",
        "random",
        "uniform",
        "len",
        "trainingSet",
        "testSet",
        "append",
        "trainingSet",
        "randIndex",
        "trainingSet",
        "randIndex",
        "docIndex",
        "trainingSet",
        "trainMat",
        "append",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "trainClasses",
        "append",
        "classList",
        "docIndex",
        "trainNB0",
        "array",
        "trainMat",
        "array",
        "trainClasses",
        "docIndex",
        "testSet",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "classifyNB",
        "array",
        "wordVector",
        "p0V",
        "p1V",
        "pSpam",
        "classList",
        "docIndex",
        "errorCount",
        "docList",
        "docIndex",
        "errorCount",
        "len",
        "testSet",
        "bigString",
        "re",
        "re",
        "split",
        "bigString",
        "tok",
        "lower",
        "tok",
        "listOfTokens",
        "len",
        "tok",
        "i",
        "textParse",
        "open",
        "i",
        "read",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "textParse",
        "open",
        "i",
        "read",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "createVocabList",
        "docList",
        "i",
        "random",
        "uniform",
        "len",
        "trainingSet",
        "testSet",
        "append",
        "trainingSet",
        "randIndex",
        "trainingSet",
        "randIndex",
        "docIndex",
        "trainingSet",
        "trainMat",
        "append",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "trainClasses",
        "append",
        "classList",
        "docIndex",
        "trainNB0",
        "array",
        "trainMat",
        "array",
        "trainClasses",
        "docIndex",
        "testSet",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "classifyNB",
        "array",
        "wordVector",
        "p0V",
        "p1V",
        "pSpam",
        "classList",
        "docIndex",
        "errorCount",
        "docList",
        "docIndex",
        "errorCount",
        "len",
        "testSet",
        "loadDataSet",
        "createVocabList",
        "listOPosts",
        "postinDoc",
        "listOPosts",
        "trainMat",
        "append",
        "setOfWords2Vec",
        "myVocabList",
        "postinDoc",
        "trainNB0",
        "array",
        "trainMat",
        "array",
        "listClasses",
        "array",
        "setOfWords2Vec",
        "myVocabList",
        "testEntry",
        "testEntry",
        "classifyNB",
        "thisDoc",
        "p0V",
        "p1V",
        "pAb",
        "array",
        "setOfWords2Vec",
        "myVocabList",
        "testEntry",
        "testEntry",
        "classifyNB",
        "thisDoc",
        "p0V",
        "p1V",
        "pAb",
        "vocabList",
        "fullText",
        "token",
        "vocabList",
        "fullText",
        "count",
        "token",
        "sorted",
        "freqDict",
        "iteritems",
        "key",
        "itemgetter",
        "reverse",
        "sortedFreq",
        "feed1",
        "feed0",
        "feedparser",
        "min",
        "len",
        "feed1",
        "len",
        "feed0",
        "i",
        "minLen",
        "textParse",
        "feed1",
        "i",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "textParse",
        "feed0",
        "i",
        "docList",
        "append",
        "wordList",
        "fullText",
        "extend",
        "wordList",
        "classList",
        "append",
        "createVocabList",
        "docList",
        "calcMostFreq",
        "vocabList",
        "fullText",
        "pairW",
        "top30Words",
        "pairW",
        "vocabList",
        "vocabList",
        "remove",
        "pairW",
        "minLen",
        "i",
        "random",
        "uniform",
        "len",
        "trainingSet",
        "testSet",
        "append",
        "trainingSet",
        "randIndex",
        "trainingSet",
        "randIndex",
        "docIndex",
        "trainingSet",
        "trainMat",
        "append",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "trainClasses",
        "append",
        "classList",
        "docIndex",
        "trainNB0",
        "array",
        "trainMat",
        "array",
        "trainClasses",
        "docIndex",
        "testSet",
        "bagOfWords2VecMN",
        "vocabList",
        "docList",
        "docIndex",
        "classifyNB",
        "array",
        "wordVector",
        "p0V",
        "p1V",
        "pSpam",
        "classList",
        "docIndex",
        "errorCount",
        "errorCount",
        "len",
        "testSet",
        "vocabList",
        "p0V",
        "p1V",
        "ny",
        "sf",
        "localWords",
        "ny",
        "sf",
        "i",
        "len",
        "p0V",
        "p0V",
        "i",
        "topSF",
        "append",
        "vocabList",
        "i",
        "p0V",
        "i",
        "p1V",
        "i",
        "topNY",
        "append",
        "vocabList",
        "i",
        "p1V",
        "i",
        "sorted",
        "topSF",
        "key",
        "pair",
        "pair",
        "reverse",
        "item",
        "sortedSF",
        "item",
        "sorted",
        "topNY",
        "key",
        "pair",
        "pair",
        "reverse",
        "item",
        "sortedNY",
        "item",
        "spamTest",
        "feedparser",
        "feedparser",
        "parse",
        "feedparser",
        "parse",
        "len",
        "ny",
        "getTopWords",
        "ny",
        "sf",
        "cctest"
    ],
    "literals": [
        "'my'",
        "'dog'",
        "'has'",
        "'flea'",
        "'problems'",
        "'help'",
        "'please'",
        "'maybe'",
        "'not'",
        "'take'",
        "'him'",
        "'to'",
        "'dog'",
        "'park'",
        "'stupid'",
        "'my'",
        "'dalmation'",
        "'is'",
        "'so'",
        "'cute'",
        "'I'",
        "'love'",
        "'him'",
        "'stop'",
        "'posting'",
        "'stupid'",
        "'worthless'",
        "'garbage'",
        "'mr'",
        "'licks'",
        "'ate'",
        "'my'",
        "'steak'",
        "'how'",
        "'to'",
        "'stop'",
        "'him'",
        "'quit'",
        "'buying'",
        "'worthless'",
        "'dog'",
        "'food'",
        "'stupid'",
        "\"the word: %s is not in my Vocabulary!\"",
        "r'\\W*'",
        "'email/spam/%d.txt'",
        "'email/ham/%d.txt'",
        "\"classification error\"",
        "'the error rate is: '",
        "r'\\W*'",
        "'email/spam/%d.txt'",
        "'email/ham/%d.txt'",
        "\"classification error\"",
        "'the error rate is: '",
        "'love'",
        "'my'",
        "'dalmation'",
        "'classified as: '",
        "'stupid'",
        "'garbage'",
        "'classified as: '",
        "'entries'",
        "'entries'",
        "'entries'",
        "'summary'",
        "'entries'",
        "'summary'",
        "'the error rate is: '",
        "\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"",
        "\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"",
        "'http://newyork.craigslist.org/stp/index.rss'",
        "'http://sfbay.craigslist.org/stp/index.rss'",
        "'entries'",
        "'__main__'"
    ],
    "variables": [
        "postingList",
        "classVec",
        "vocabSet",
        "vocabSet",
        "returnVec",
        "returnVec",
        "returnVec",
        "numTrainDocs",
        "numWords",
        "pAbusive",
        "p0Num",
        "p1Num",
        "p0Denom",
        "p1Denom",
        "p1Vect",
        "p0Vect",
        "p1",
        "p0",
        "listOfTokens",
        "docList",
        "classList",
        "fullText",
        "wordList",
        "wordList",
        "vocabList",
        "trainingSet",
        "testSet",
        "randIndex",
        "trainMat",
        "trainClasses",
        "p0V",
        "p1V",
        "pSpam",
        "errorCount",
        "wordVector",
        "listOfTokens",
        "docList",
        "classList",
        "fullText",
        "wordList",
        "wordList",
        "vocabList",
        "trainingSet",
        "testSet",
        "randIndex",
        "trainMat",
        "trainClasses",
        "p0V",
        "p1V",
        "pSpam",
        "errorCount",
        "wordVector",
        "listOPosts",
        "listClasses",
        "myVocabList",
        "trainMat",
        "p0V",
        "p1V",
        "pAb",
        "testEntry",
        "thisDoc",
        "testEntry",
        "thisDoc",
        "freqDict",
        "freqDict",
        "token",
        "sortedFreq",
        "docList",
        "classList",
        "fullText",
        "minLen",
        "wordList",
        "wordList",
        "vocabList",
        "top30Words",
        "trainingSet",
        "testSet",
        "randIndex",
        "trainMat",
        "trainClasses",
        "p0V",
        "p1V",
        "pSpam",
        "errorCount",
        "wordVector",
        "vocabList",
        "p0V",
        "p1V",
        "topNY",
        "topSF",
        "sortedSF",
        "sortedNY",
        "ny",
        "sf"
    ],
    "comments": [
        "coding=utf-8",
        "1 is abusive, 0 not",
        "create empty set",
        "print vocabSet",
        "union of the two sets",
        "print vocabSet",
        "print numTrainDocs",
        "print trainCategory",
        "print trainMatrix[0]",
        "print trainMatrix",
        "print numWords",
        "change to ones()",
        "print p0Num",
        "print p1Num",
        "change to 2.0",
        "print trainMatrix[i]",
        "print p1Num",
        "print \"*&****************\"",
        "print sum(trainMatrix[i])",
        "print p1Denom",
        "change to log()",
        "change to log()",
        "element-wise mult",
        "input is big string, #output is word list",
        "create vocabulary",
        "create test set",
        "train the classifier (get probs) trainNB0",
        "classify the remaining items",
        "return vocabList,fullText",
        "input is big string, #output is word list",
        "create vocabulary",
        "create test set",
        "train the classifier (get probs) trainNB0",
        "classify the remaining items",
        "return vocabList,fullText",
        "def test():",
        "listOposts,listClasses = loadDataSet()",
        "print listOposts",
        "print listClasses",
        "myVocabList = createVocabList(listOposts)",
        "print myVocabList",
        "testRes1 = setOfWords2Vec(myVocabList,listOposts[0])",
        "print testRes1",
        "# testRes2 = setOfWords2Vec(myVocabList, listOposts[3])",
        "# print testRes2",
        "",
        "trainMat=[]",
        "for postinDoc in listOposts:",
        "trainMat.append(setOfWords2Vec(myVocabList,postinDoc))",
        "print \"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\"",
        "print trainMat",
        "p0v,p1v,pAb=trainNB0(trainMat,listClasses)",
        "print pAb",
        "print p0v",
        "print p1v",
        "NY is class 1",
        "create vocabulary",
        "remove top 30 words",
        "把top30Words保存在txt中（unfineshed）",
        "print 'Removed: ', pairW[0]",
        "create test set",
        "train the classifier (get probs) trainNB0",
        "classify the remaining items",
        "ny = feedparser.parse('http://newyork.craigslist.org/search/stp?format=rss')",
        "sf = feedparser.parse('http://sfbay.craigslist.org/search/stp?format=rss')",
        "vocabList,pSF,pNF=localWords(ny,sf)",
        "sptest()",
        "testingNB()"
    ],
    "docstrings": [],
    "functions": [
        "loadDataSet",
        "createVocabList",
        "setOfWords2Vec",
        "bagOfWords2VecMN",
        "trainNB0",
        "classifyNB",
        "textParse",
        "spamTest",
        "textParse",
        "spamTest",
        "testingNB",
        "calcMostFreq",
        "localWords",
        "getTopWords",
        "sptest",
        "cctest"
    ],
    "classes": []
}