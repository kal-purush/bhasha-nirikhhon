{
    "identifiers": [
        "lab6",
        "lab6",
        "exceptions",
        "LexerException",
        "java",
        "util",
        "java",
        "util",
        "java",
        "util",
        "java",
        "util",
        "input",
        "input",
        "tokenize",
        "OPERATOR_CHARS",
        "OPERATORS",
        "OPERATORS",
        "OPERATORS",
        "put",
        "TokenType",
        "SEMI_COLON",
        "OPERATORS",
        "put",
        "TokenType",
        "COLON",
        "OPERATORS",
        "put",
        "TokenType",
        "PLUS",
        "OPERATORS",
        "put",
        "TokenType",
        "MINUS",
        "OPERATORS",
        "put",
        "TokenType",
        "LEFT_ROUND_BRACKET",
        "OPERATORS",
        "put",
        "TokenType",
        "RIGHT_ROUND_BRACKET",
        "OPERATORS",
        "put",
        "TokenType",
        "ASSIGN",
        "OPERATORS",
        "put",
        "TokenType",
        "LESS",
        "OPERATORS",
        "put",
        "TokenType",
        "MORE",
        "OPERATORS",
        "put",
        "TokenType",
        "EQUALS",
        "OPERATORS",
        "put",
        "TokenType",
        "NOT_EQUALS",
        "OPERATORS",
        "put",
        "TokenType",
        "LESS_EQUALS",
        "OPERATORS",
        "put",
        "TokenType",
        "MORE_EQUALS",
        "OPERATORS",
        "put",
        "TokenType",
        "LEFT_SQUARE_BRACKET",
        "OPERATORS",
        "put",
        "TokenType",
        "RIGHT_SQUARE_BRACKET",
        "OPERATORS",
        "put",
        "TokenType",
        "POINT_POINT",
        "KEYWORDS",
        "KEYWORDS",
        "KEYWORDS",
        "put",
        "TokenType",
        "REAL",
        "KEYWORDS",
        "put",
        "TokenType",
        "LONGINT",
        "KEYWORDS",
        "put",
        "TokenType",
        "BOOLEAN",
        "KEYWORDS",
        "put",
        "TokenType",
        "ARRAY",
        "KEYWORDS",
        "put",
        "TokenType",
        "OF",
        "KEYWORDS",
        "put",
        "TokenType",
        "TRUE",
        "KEYWORDS",
        "put",
        "TokenType",
        "FALSE",
        "input",
        "length",
        "tokens",
        "buffer",
        "pos",
        "row",
        "col",
        "input",
        "input",
        "input",
        "length",
        "input",
        "length",
        "tokens",
        "buffer",
        "row",
        "col",
        "pos",
        "length",
        "current",
        "peek",
        "isDigit",
        "current",
        "tokenizeNumber",
        "isIdentifierStart",
        "current",
        "tokenizeWord",
        "OPERATOR_CHARS",
        "indexOf",
        "current",
        "tokenizeOperator",
        "next",
        "tokens",
        "clearBuffer",
        "current",
        "peek",
        "isLetter",
        "current",
        "error",
        "isDigit",
        "current",
        "buffer",
        "append",
        "current",
        "current",
        "next",
        "addToken",
        "TokenType",
        "LONGINT",
        "buffer",
        "toString",
        "current",
        "peek",
        "current",
        "peek",
        "next",
        "next",
        "tokenizeOneLineComment",
        "peek",
        "next",
        "next",
        "tokenizeMultilineComment",
        "clearBuffer",
        "text",
        "buffer",
        "toString",
        "text",
        "isEmpty",
        "OPERATORS",
        "containsKey",
        "text",
        "current",
        "addToken",
        "OPERATORS",
        "get",
        "text",
        "text",
        "buffer",
        "append",
        "current",
        "current",
        "next",
        "clearBuffer",
        "buffer",
        "append",
        "peek",
        "current",
        "next",
        "isIdentifierPart",
        "current",
        "buffer",
        "append",
        "current",
        "current",
        "next",
        "word",
        "buffer",
        "toString",
        "addToken",
        "KEYWORDS",
        "getOrDefault",
        "word",
        "toLowerCase",
        "TokenType",
        "VAR",
        "word",
        "toLowerCase",
        "current",
        "peek",
        "indexOf",
        "current",
        "current",
        "next",
        "current",
        "peek",
        "current",
        "peek",
        "current",
        "error",
        "current",
        "next",
        "next",
        "next",
        "current",
        "isLetter",
        "current",
        "current",
        "isLetterOrDigit",
        "current",
        "buffer",
        "setLength",
        "pos",
        "result",
        "peek",
        "result",
        "row",
        "col",
        "col",
        "result",
        "relativePosition",
        "position",
        "pos",
        "relativePosition",
        "position",
        "length",
        "input",
        "charAt",
        "position",
        "text",
        "tokens",
        "add",
        "text",
        "row",
        "col",
        "text",
        "length",
        "text",
        "row",
        "col",
        "text"
    ],
    "literals": [
        "\"+-()=<>:;.[]\"",
        "\";\"",
        "\":\"",
        "\"+\"",
        "\"-\"",
        "\"(\"",
        "\")\"",
        "\":=\"",
        "\"<\"",
        "\">\"",
        "\"=\"",
        "\"<>\"",
        "\"<=\"",
        "\">=\"",
        "\"[\"",
        "\"]\"",
        "\"..\"",
        "\"real\"",
        "\"longint\"",
        "\"boolean\"",
        "\"array\"",
        "\"of\"",
        "\"true\"",
        "\"false\"",
        "\"Variable starts with number\"",
        "\"\\r\\n\\0\"",
        "\"Reached end of file while parsing multiline comment\""
    ],
    "variables": [
        "OPERATORS",
        "KEYWORDS",
        "input",
        "length",
        "tokens",
        "buffer",
        "pos",
        "row",
        "col"
    ],
    "comments": [
        "whitespaces",
        "/"
    ],
    "docstrings": [
        "*"
    ],
    "functions": [
        "tokenize",
        "Lexer",
        "tokenize",
        "tokenizeNumber",
        "tokenizeOperator",
        "tokenizeWord",
        "tokenizeOneLineComment",
        "tokenizeMultilineComment",
        "isIdentifierStart",
        "isIdentifierPart",
        "clearBuffer",
        "next",
        "peek",
        "addToken",
        "LexerException",
        "error"
    ],
    "classes": [
        "Lexer"
    ]
}