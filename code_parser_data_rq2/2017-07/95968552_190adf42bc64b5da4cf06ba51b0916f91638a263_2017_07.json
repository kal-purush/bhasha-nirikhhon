{
    "identifiers": [
        "langdetect",
        "detect_langs",
        "nltk",
        "twingly_search",
        "Client",
        "tokens",
        "x",
        "x",
        "lower",
        "stop_words",
        "tokens",
        "text",
        "nltk",
        "sent_tokenize",
        "text",
        "sentence",
        "sent_text",
        "nltk",
        "word_tokenize",
        "sentence",
        "nltk",
        "pos_tag",
        "tokenized_text",
        "nouns",
        "w",
        "w",
        "tagged",
        "w",
        "nouns",
        "text",
        "lang",
        "lang",
        "tokenize_english",
        "text",
        "join",
        "strip_stop_words",
        "tokens",
        "raw_input",
        "detect_langs",
        "user_input",
        "lang",
        "user_input",
        "lang",
        "find_articles",
        "user_input",
        "lang",
        "main"
    ],
    "literals": [
        "'i'",
        "'you'",
        "'blog'",
        "'article'",
        "u'私'",
        "u'俺'",
        "u'記事'",
        "u'ブロク'",
        "'NN'",
        "'en'",
        "'LEYS TOKENIZE JAPANESE!'",
        "' '",
        "\">>>\"",
        "\"lets find a blog!\"",
        "'en'",
        "'ja'",
        "'not supported :('",
        "'__main__'"
    ],
    "variables": [
        "stop_words",
        "sent_text",
        "nouns",
        "tokenized_text",
        "tagged",
        "tokens",
        "tokens",
        "user_input",
        "lang"
    ],
    "comments": [
        "-*- coding: utf-8 -*-",
        "english and japanese stop words are requires",
        "英語でも日本語でもいらない言葉を消す"
    ],
    "docstrings": [],
    "functions": [
        "strip_stop_words",
        "tokenize_english",
        "find_articles",
        "main"
    ],
    "classes": []
}