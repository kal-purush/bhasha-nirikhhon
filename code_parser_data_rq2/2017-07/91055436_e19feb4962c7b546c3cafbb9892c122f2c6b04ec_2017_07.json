{
    "identifiers": [
        "argparse",
        "csv",
        "re",
        "os",
        "random",
        "uniform",
        "collections",
        "defaultdict",
        "re",
        "compile",
        "END_OF_SENT_SYM",
        "model",
        "DEBUG_PRINT",
        "ORDER",
        "prev_words",
        "next_words_info",
        "model",
        "iteritems",
        "ORDER",
        "join",
        "next_word",
        "probality",
        "next_word",
        "probality",
        "next_words_info",
        "join",
        "prev_words",
        "str_next_word_info",
        "words_count",
        "i",
        "words_count",
        "i",
        "corpus",
        "open",
        "corpus",
        "line",
        "data",
        "line",
        "decode",
        "lower",
        "lines",
        "line",
        "lines",
        "token",
        "r_alphabet",
        "findall",
        "line",
        "token",
        "encode",
        "tokens",
        "order",
        "EMPTY_WORD",
        "x",
        "order",
        "token",
        "tokens",
        "tn",
        "token",
        "token",
        "END_OF_SENT_SYM",
        "tn",
        "append",
        "token",
        "i",
        "order",
        "tn",
        "append",
        "EMPTY_WORD",
        "tn",
        "pop",
        "tn",
        "tn",
        "pop",
        "tn",
        "append",
        "token",
        "tn",
        "pop",
        "corpus",
        "model",
        "gen_lines",
        "corpus",
        "gen_tokens",
        "lines",
        "gen_n_grams",
        "tokens",
        "ORDER",
        "defaultdict",
        "defaultdict",
        "DEBUG_PRINT",
        "ORDER",
        "get_table_header",
        "ORDER",
        "n_gram",
        "n_grams",
        "DEBUG_PRINT",
        "join",
        "n_gram",
        "n_min_stat",
        "n_gram",
        "n_stat",
        "n_gram",
        "DEBUG_PRINT",
        "get_table_header",
        "ORDER",
        "words",
        "freq",
        "n_min_stat",
        "iteritems",
        "join",
        "words",
        "freq",
        "get_table_header",
        "ORDER",
        "words",
        "freq",
        "n_stat",
        "iteritems",
        "join",
        "words",
        "freq",
        "words",
        "freq",
        "n_stat",
        "iteritems",
        "words",
        "words",
        "prev_words",
        "model",
        "model",
        "prev_words",
        "append",
        "next_word",
        "freq",
        "n_min_stat",
        "prev_words",
        "next_word",
        "freq",
        "n_min_stat",
        "prev_words",
        "print_model",
        "model",
        "model",
        "model",
        "DEBUG_PRINT",
        "EMPTY_WORD",
        "ORDER",
        "DEBUG_PRINT",
        "join",
        "prev_words",
        "model",
        "prev_words",
        "prev_words",
        "append",
        "unirand",
        "model",
        "prev_words",
        "prev_words",
        "pop",
        "prev_words",
        "DEBUG_PRINT",
        "next_word",
        "next_word",
        "EMPTY_WORD",
        "next_word",
        "END_OF_SENT_SYM",
        "prev_words",
        "EMPTY_WORD",
        "phrase",
        "next_word",
        "phrase",
        "next_word",
        "phrase",
        "capitalize",
        "seq",
        "item",
        "freq",
        "seq",
        "sum_",
        "freq",
        "uniform",
        "sum_",
        "token",
        "freq",
        "seq",
        "freq_",
        "freq",
        "rnd",
        "freq_",
        "token",
        "model",
        "file_name",
        "open",
        "file_name",
        "csv_file",
        "csv",
        "writer",
        "csv_file",
        "dialect",
        "prev_words",
        "next_words_info",
        "model",
        "iteritems",
        "next_word",
        "probality",
        "next_words_info",
        "prev_words",
        "csv_row",
        "extend",
        "next_word",
        "probality",
        "writer",
        "writerow",
        "csv_row",
        "file_name",
        "os",
        "path",
        "exists",
        "file_name",
        "model",
        "open",
        "file_name",
        "csv_file",
        "csv",
        "reader",
        "csv_file",
        "dialect",
        "row",
        "reader",
        "row",
        "row",
        "row",
        "prev_words",
        "model",
        "model",
        "prev_words",
        "append",
        "next_words_info",
        "next_words_info",
        "print_model",
        "model",
        "model",
        "argparse",
        "ArgumentParser",
        "description",
        "ORDER",
        "parser",
        "add_argument",
        "metavar",
        "required",
        "help",
        "parser",
        "add_argument",
        "metavar",
        "required",
        "help",
        "parser",
        "add_argument",
        "metavar",
        "required",
        "help",
        "parser",
        "add_argument",
        "action",
        "parser",
        "parse_args",
        "args",
        "logs",
        "args",
        "model_file",
        "load_model",
        "args",
        "model_file",
        "args",
        "train_text_file",
        "model",
        "train",
        "args",
        "train_text_file",
        "model",
        "model",
        "exit",
        "args",
        "model_file",
        "save_model",
        "model",
        "args",
        "model_file",
        "args",
        "num_of_sent",
        "args",
        "num_of_sent",
        "i",
        "sentences_count",
        "result",
        "generate_sentence",
        "model",
        "result"
    ],
    "literals": [
        "\".,:;?!\"",
        "\"$\"",
        "u'[ёa-zA-Zа-яА-Я0-9-]+|['",
        "'\\n]+'",
        "\"\\n-------- model ---------------------------------------\"",
        "\"previous words\"",
        "'\\t'",
        "\"|  next word\\t: next word probality\"",
        "\"------------------------------------------------------\"",
        "'\\n'",
        "'\\t'",
        "\"\\t: \"",
        "\"\\t\"",
        "\"\\t|\\t\"",
        "\"\"",
        "\"wrd %s\\t\"",
        "'utf-8'",
        "'utf-8'",
        "\"GENERATING \"",
        "\"-gram\"",
        "\"\\n-------- n_grams --------\"",
        "\"--------------------------\"",
        "\"\\t\"",
        "\"\\nCONVERTING TO DICT WITH WEIGS\"",
        "\"\\n-------- n_min_stat with weigs ---------\"",
        "\": weigh\\t\"",
        "\"----------------------------------\"",
        "\"%s\\t: %s\"",
        "\"\\t\"",
        "\"\\n-------- n_stat with weigs --------\"",
        "\": weigh\\t\"",
        "\"----------------------------------\"",
        "\"%s\\t: %s\"",
        "\"\\t\"",
        "\"\\nGENERATING SENTENCE\"",
        "''",
        "\"PREV WORD:\\t\"",
        "\"\\t\"",
        "\"\\tAVAILABLE NEXT WORDS: %s\"",
        "\"SELECTED WORD:\\t\"",
        "' '",
        "\"w\"",
        "'excel'",
        "\"rb\"",
        "'excel'",
        "'__main__'",
        "\"Markov model order: \"",
        "\" Sentences generator\"",
        "'--train_text_file'",
        "'path'",
        "'the path to text file with train data'",
        "'--model_file'",
        "'path'",
        "'the path to csv file with pretrained model'",
        "'--num_of_sent'",
        "'N'",
        "'number of sentences'",
        "'--logs'",
        "'store_true'",
        "\"model is empty \\nusage: \\npython model.py --model_file dump_model.csv --train_text_file input.txt --num_of_sent 3 --logs\"",
        "\"\""
    ],
    "variables": [
        "DEBUG_PRINT",
        "ORDER",
        "END_OF_SENT_SYM",
        "EMPTY_WORD",
        "r_alphabet",
        "str_next_word_info",
        "data",
        "tn",
        "lines",
        "tokens",
        "n_grams",
        "n_min_stat",
        "n_stat",
        "prev_words",
        "next_word",
        "model",
        "prev_words",
        "phrase",
        "prev_words",
        "next_word",
        "sum_",
        "freq_",
        "rnd",
        "writer",
        "csv_row",
        "model",
        "reader",
        "prev_words",
        "next_words_info",
        "model",
        "prev_words",
        "parser",
        "args",
        "DEBUG_PRINT",
        "model",
        "model",
        "model",
        "model",
        "sentences_count",
        "result"
    ],
    "comments": [
        "!/usr/bin/env python",
        "-*- coding: utf-8 -*-",
        "number of previous word chains N-1",
        "number of previous word chains N",
        "calculate freq for word chain",
        "calculate freq for word chain",
        "возвращает случайное слово с вероятностью, равной вероятности данного слова в зависимости от двух предыдущих"
    ],
    "docstrings": [],
    "functions": [
        "print_model",
        "get_table_header",
        "gen_lines",
        "gen_tokens",
        "gen_n_grams",
        "train",
        "generate_sentence",
        "unirand",
        "save_model",
        "load_model"
    ],
    "classes": []
}