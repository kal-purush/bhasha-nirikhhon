{
    "identifiers": [
        "android",
        "content",
        "Context",
        "org",
        "tensorflow",
        "lite",
        "Interpreter",
        "java",
        "io",
        "FileInputStream",
        "java",
        "io",
        "java",
        "nio",
        "MappedByteBuffer",
        "java",
        "nio",
        "channels",
        "FileChannel",
        "tfliteInterpreter",
        "OUTPUT_SIZE",
        "context",
        "OUTPUT_SIZE",
        "tfliteModel",
        "loadModelFile",
        "context",
        "options",
        "tfliteInterpreter",
        "tfliteModel",
        "options",
        "e",
        "e",
        "printStackTrace",
        "context",
        "modelFilename",
        "fileInputStream",
        "modelFilename",
        "fileChannel",
        "fileInputStream",
        "getChannel",
        "startOffset",
        "fileChannel",
        "size",
        "fileChannel",
        "FileChannel",
        "MapMode",
        "READ_ONLY",
        "startOffset",
        "inputData",
        "output",
        "OUTPUT_SIZE",
        "tfliteInterpreter",
        "run",
        "inputData",
        "output",
        "output"
    ],
    "literals": [
        "\"model.tflite\"",
        "\"assets/\""
    ],
    "variables": [
        "tfliteInterpreter",
        "OUTPUT_SIZE"
    ],
    "comments": [
        "Adjust outputSize based on your model",
        "Assuming 10 classes",
        "Load the TensorFlow Lite model from the assets folder",
        "Helper method to load the TensorFlow Lite model from the assets folder",
        "Perform inference with input data",
        "Adjust outputSize based on your model"
    ],
    "docstrings": [],
    "functions": [
        "TFLiteInference",
        "MappedByteBuffer",
        "loadModelFile",
        "predict"
    ],
    "classes": [
        "TFLiteInference"
    ]
}