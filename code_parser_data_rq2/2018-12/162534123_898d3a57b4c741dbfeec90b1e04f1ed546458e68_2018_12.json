{
    "identifiers": [
        "torch",
        "nn",
        "nn",
        "torch",
        "nn",
        "functional",
        "F",
        "nn",
        "Module",
        "hidden_dim",
        "attn_dim",
        "attn_method",
        "Attn",
        "attn_method",
        "attn_method",
        "nn",
        "Linear",
        "hidden_dim",
        "attn_method",
        "nn",
        "Linear",
        "hidden_dim",
        "attn_dim",
        "nn",
        "Parameter",
        "torch",
        "randn",
        "attn_dim",
        "hiddens",
        "F",
        "tanh",
        "projection",
        "hiddens",
        "attn_method",
        "energy",
        "matmul",
        "u",
        "F",
        "softmax",
        "energy",
        "squeeze",
        "dim",
        "unsqueeze",
        "attn_weights",
        "nn",
        "Module",
        "V",
        "D",
        "hidden_dim",
        "num_layers",
        "attn_method",
        "attn_dim",
        "BiLSTMwithAttn",
        "nn",
        "Embedding",
        "V",
        "D",
        "nn",
        "Dropout",
        "p",
        "inplace",
        "nn",
        "LSTM",
        "D",
        "hidden_dim",
        "num_layers",
        "num_layers",
        "bidirectional",
        "dropout",
        "Attn",
        "hidden_dim",
        "attn_dim",
        "attn_method",
        "nn",
        "Linear",
        "hidden_dim",
        "seq",
        "embedding",
        "seq",
        "emb_dropout",
        "seq",
        "encoder",
        "seq",
        "attn",
        "hiddens",
        "transpose",
        "attn_weigths",
        "bmm",
        "hiddens",
        "transpose",
        "predictor",
        "contexts",
        "squeeze",
        "F",
        "log_softmax",
        "preds"
    ],
    "literals": [
        "'simple'",
        "'complex'",
        "'complex'",
        "'basic'"
    ],
    "variables": [
        "attn_method",
        "projection",
        "projection",
        "u",
        "energy",
        "energy",
        "attn_weights",
        "embedding",
        "emb_dropout",
        "encoder",
        "attn",
        "predictor",
        "seq",
        "hiddens",
        "_",
        "attn_weigths",
        "contexts",
        "preds"
    ],
    "comments": [
        "输入的shape: (batch,len,hidden_dim)",
        "shape: (batch,1,len)",
        "shape: (batch,1,len)",
        "shape: (batch,1,hidden_dim)"
    ],
    "docstrings": [
        "'''\n    @model: BiLSTMwithAttn\n    @params:\n        V: (int)Vocab_size\n        D: (int) embedding_dim\n        hidden_dim: (int) hidden_dim\n        num_layers: (int) lstm stack的层数\n        attn_method: (str)注意力机制的方法\n                     两种：\n                     ‘simple': 直接将各个hidden转换为一个scalar\n                     ‘complex': 先将hidden映射到另一个向量空间，然后计算它与一个单词上下文向量u的相似度,最后得到这个scalar\n        attn_dim: (int) 注意力空间的dim\n'''"
    ],
    "functions": [
        "forward",
        "forward"
    ],
    "classes": [
        "Attn",
        "BiLSTMwithAttn"
    ]
}