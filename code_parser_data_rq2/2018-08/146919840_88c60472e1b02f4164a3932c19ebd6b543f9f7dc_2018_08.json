{
    "identifiers": [
        "exports",
        "callback",
        "require",
        "require",
        "file",
        "file",
        "file",
        "file",
        "filename",
        "filename",
        "dashOffset",
        "filename",
        "tableId",
        "BigQuery",
        "projectId",
        "Storage",
        "projectId",
        "bigquery",
        "datasetId",
        "tableId",
        "storage",
        "bucketName",
        "filename",
        "results",
        "job",
        "results",
        "job",
        "job",
        "metadata",
        "metadata",
        "errors",
        "errors",
        "errors",
        "job",
        "err",
        "err",
        "callback"
    ],
    "literals": [
        "'@google-cloud/bigquery'",
        "'@google-cloud/storage'",
        "\"mikekahn-sandbox\"",
        "\"avroimport\"",
        "gs://${file.bucket}/${file.name}",
        "'-'",
        "\"_STAGE\"",
        "Load ${filename} into ${tableId}.",
        "Job ${job.id} started.",
        "Job ${job.id} completed.",
        "'ERROR:'"
    ],
    "variables": [
        "file",
        "context",
        "BigQuery",
        "Storage",
        "projectId",
        "datasetId",
        "bucketName",
        "filename",
        "gcsFile",
        "dashOffset",
        "tableId",
        "bigquery",
        "storage",
        "job",
        "errors"
    ],
    "comments": [
        "function code taken from https://stackoverflow.com/questions/49111829/using-cloud-function-to-load-data-into-big-query-table-it-is-appending-to-the-t",
        "fixed by tfrantzen@google.com",
        "specify projectID and bigquery datasetID below",
        "Do not use the ftp_files Bucket to ensure that the bucket does not get crowded.",
        "Change bucket to gas_ddr_files_staging",
        "Set the table name (TableId) to the full file name including date,",
        "this will give each table a new distinct name and we can keep a record of all of the files recieved.",
        "This may not be the best way to do this... at some point we will need to archive and delete prior records.",
        "Instantiates clients",
        "const metadata = {\n\t\tallowJaggedRows: true,\n\t\tskipLeadingRows: 1\n\n\t};",
        "Loads data from a Google Cloud Storage file into the table",
        "Wait for the job to finish",
        "Check the job's status for errors"
    ],
    "docstrings": [],
    "functions": [],
    "classes": []
}