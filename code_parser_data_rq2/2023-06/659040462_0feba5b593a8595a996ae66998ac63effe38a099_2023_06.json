{
    "identifiers": [
        "json",
        "torch",
        "uvicorn",
        "datetime",
        "fastapi",
        "FastAPI",
        "Request",
        "utils",
        "ModelArguments",
        "FinetuningArguments",
        "auto_configure_device_map",
        "load_pretrained",
        "transformers",
        "HfArgumentParser",
        "torch",
        "cuda",
        "is_available",
        "torch",
        "cuda",
        "device_count",
        "device_id",
        "num_gpus",
        "torch",
        "cuda",
        "device",
        "device_id",
        "torch",
        "cuda",
        "empty_cache",
        "torch",
        "cuda",
        "ipc_collect",
        "FastAPI",
        "app",
        "post",
        "request",
        "Request",
        "model",
        "tokenizer",
        "request",
        "json",
        "json",
        "dumps",
        "json_post_raw",
        "json",
        "loads",
        "json_post",
        "json_post_list",
        "get",
        "json_post_list",
        "get",
        "model",
        "chat",
        "tokenizer",
        "prompt",
        "history",
        "history",
        "datetime",
        "datetime",
        "now",
        "now",
        "strftime",
        "repr",
        "response",
        "repr",
        "history",
        "time",
        "time",
        "prompt",
        "repr",
        "response",
        "log",
        "torch_gc",
        "answer",
        "HfArgumentParser",
        "ModelArguments",
        "FinetuningArguments",
        "parser",
        "parse_args_into_dataclasses",
        "load_pretrained",
        "model_args",
        "finetuning_args",
        "torch",
        "cuda",
        "device_count",
        "accelerate",
        "dispatch_model",
        "auto_configure_device_map",
        "torch",
        "cuda",
        "device_count",
        "dispatch_model",
        "model",
        "device_map",
        "model",
        "cuda",
        "model",
        "eval",
        "uvicorn",
        "run",
        "app",
        "host",
        "port",
        "workers"
    ],
    "literals": [
        "\"/\"",
        "\"prompt\"",
        "\"history\"",
        "\"%Y-%m-%d %H:%M:%S\"",
        "\"response\"",
        "\"history\"",
        "\"status\"",
        "\"time\"",
        "\"[\"",
        "\"] \"",
        "\"\\\", prompt:\\\"\"",
        "\"\\\", response:\\\"\"",
        "\"\\\"\"",
        "\"__main__\"",
        "'0.0.0.0'"
    ],
    "variables": [
        "num_gpus",
        "app",
        "json_post_raw",
        "json_post",
        "json_post_list",
        "prompt",
        "history",
        "response",
        "history",
        "now",
        "time",
        "answer",
        "log",
        "parser",
        "model_args",
        "finetuning_args",
        "model",
        "tokenizer",
        "device_map",
        "model",
        "model"
    ],
    "comments": [
        "coding=utf-8",
        "Implements API for ChatGLM fine-tuned with PEFT.",
        "This code is largely borrowed from https://github.com/THUDM/ChatGLM-6B/blob/main/api.py",
        "Usage: python api_demo.py --checkpoint_dir path_to_checkpoint [--quantization_bit 4]",
        "Request:",
        "curl http://127.0.0.1:8000 --header 'Content-Type: application/json' --data '{\"prompt\": \"Hello there!\", \"history\": []}'",
        "Response:",
        "{",
        "\"response\": \"'Hi there!'\",",
        "\"history\": \"[('Hello there!', 'Hi there!')]\",",
        "\"status\": 200,",
        "\"time\": \"2000-00-00 00:00:00\"",
        "}",
        "Parse the request JSON",
        "Generate response",
        "Prepare response",
        "Log and clean up"
    ],
    "docstrings": [],
    "functions": [
        "torch_gc",
        "create_item"
    ],
    "classes": []
}