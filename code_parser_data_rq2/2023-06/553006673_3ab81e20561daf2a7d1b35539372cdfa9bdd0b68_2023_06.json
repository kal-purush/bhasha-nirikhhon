{
    "identifiers": [
        "sys",
        "os",
        "path",
        "typing",
        "Dict",
        "Optional",
        "Union",
        "numpy",
        "np",
        "gymnasium",
        "spaces",
        "gymnasium",
        "envs",
        "mujoco",
        "ant_v4",
        "AntEnv",
        "gymnasium",
        "utils",
        "ezpickle",
        "EzPickle",
        "gymnasium_robotics",
        "envs",
        "maze",
        "maps",
        "U_MAZE",
        "gymnasium_robotics",
        "envs",
        "maze",
        "maze_v4",
        "MazeEnv",
        "gymnasium_robotics",
        "utils",
        "mujoco_utils",
        "MujocoModelNames",
        "MazeEnv",
        "EzPickle",
        "render_mode",
        "Optional",
        "maze_map",
        "Union",
        "U_MAZE",
        "reward_type",
        "continuing_task",
        "kwargs",
        "path",
        "join",
        "path",
        "dirname",
        "sys",
        "modules",
        "AntEnv",
        "__module__",
        "agent_xml_path",
        "ant_xml_file_path",
        "maze_map",
        "maze_map",
        "maze_size_scaling",
        "maze_height",
        "reward_type",
        "reward_type",
        "continuing_task",
        "continuing_task",
        "kwargs",
        "AntEnv",
        "xml_file",
        "tmp_xml_file_path",
        "exclude_current_positions_from_observation",
        "render_mode",
        "render_mode",
        "reset_noise_scale",
        "kwargs",
        "MujocoModelNames",
        "ant_env",
        "model",
        "_model_names",
        "site_name2id",
        "ant_env",
        "action_space",
        "ant_env",
        "observation_space",
        "shape",
        "spaces",
        "Dict",
        "observation",
        "spaces",
        "Box",
        "np",
        "inf",
        "np",
        "inf",
        "shape",
        "obs_shape",
        "dtype",
        "achieved_goal",
        "spaces",
        "Box",
        "np",
        "inf",
        "np",
        "inf",
        "shape",
        "dtype",
        "desired_goal",
        "spaces",
        "Box",
        "np",
        "inf",
        "np",
        "inf",
        "shape",
        "dtype",
        "render_mode",
        "EzPickle",
        "render_mode",
        "maze_map",
        "reward_type",
        "continuing_task",
        "kwargs",
        "seed",
        "Optional",
        "kwargs",
        "reset",
        "seed",
        "seed",
        "kwargs",
        "ant_env",
        "init_qpos",
        "reset_pos",
        "ant_env",
        "reset",
        "seed",
        "seed",
        "_get_obs",
        "obs",
        "np",
        "linalg",
        "norm",
        "obs_dict",
        "goal",
        "obs_dict",
        "info",
        "action",
        "ant_env",
        "step",
        "action",
        "_get_obs",
        "ant_obs",
        "compute_reward",
        "obs",
        "goal",
        "info",
        "compute_terminated",
        "obs",
        "goal",
        "info",
        "compute_truncated",
        "obs",
        "goal",
        "info",
        "np",
        "linalg",
        "norm",
        "obs",
        "goal",
        "render_mode",
        "render",
        "update_goal",
        "obs",
        "obs",
        "reward",
        "terminated",
        "truncated",
        "info",
        "ant_obs",
        "np",
        "ndarray",
        "Dict",
        "np",
        "ndarray",
        "ant_obs",
        "ant_obs",
        "observation",
        "copy",
        "achieved_goal",
        "copy",
        "goal",
        "copy",
        "ant_env",
        "model",
        "site_pos",
        "target_site_id",
        "np",
        "append",
        "goal",
        "maze",
        "maze_height",
        "maze",
        "maze_size_scaling",
        "ant_env",
        "render",
        "close",
        "ant_env",
        "close"
    ],
    "literals": [
        "\"render_modes\"",
        "\"human\"",
        "\"rgb_array\"",
        "\"depth_array\"",
        "\"render_fps\"",
        "\"sparse\"",
        "\"assets/ant.xml\"",
        "\"target\"",
        "\"float64\"",
        "\"float64\"",
        "\"float64\"",
        "\"success\"",
        "\"achieved_goal\"",
        "\"achieved_goal\"",
        "\"achieved_goal\"",
        "\"achieved_goal\"",
        "\"success\"",
        "\"achieved_goal\"",
        "\"human\"",
        "\"achieved_goal\"",
        "\"observation\"",
        "\"achieved_goal\"",
        "\"desired_goal\""
    ],
    "variables": [
        "metadata",
        "ant_xml_file_path",
        "ant_env",
        "_model_names",
        "target_site_id",
        "action_space",
        "obs_shape",
        "observation_space",
        "render_mode",
        "obs",
        "info",
        "obs_dict",
        "info",
        "ant_obs",
        "_",
        "_",
        "_",
        "info",
        "obs",
        "reward",
        "terminated",
        "truncated",
        "info",
        "achieved_goal",
        "observation"
    ],
    "comments": [
        "Get the ant.xml path from the Gymnasium package",
        "Create the MuJoCo environment, include position observation of the Ant for GoalEnv",
        "Update the goal position if necessary"
    ],
    "docstrings": [
        "\"\"\"A maze environment with the Gymnasium Ant agent (https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/ant_v4.py).\n\nThe code is inspired by the D4RL repository hosted on GitHub (https://github.com/Farama-Foundation/D4RL), published in the paper\n'D4RL: Datasets for Deep Data-Driven Reinforcement Learning' by Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine.\n\nOriginal Author of the code: Justin Fu\n\nThe modifications made involve reusing the code in Gymnasium for the Ant environment and in `point_maze/maze_env.py`.\nThe new code also follows the Gymnasium API and Multi-goal API\n\nThis project is covered by the Apache 2.0 License.\n\"\"\"",
        "\"\"\"\n    ### Description\n\n    This environment was refactored from the [D4RL](https://github.com/Farama-Foundation/D4RL) repository, introduced by Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine\n    in [\"D4RL: Datasets for Deep Data-Driven Reinforcement Learning\"](https://arxiv.org/abs/2004.07219).\n\n    The tasks found in the `AntMaze` environments are the same as the ones in the `PointMaze` environments. However, in this case the agent is the Ant quadruped from the main [Gymnaisum](https://gymnasium.farama.org/environments/mujoco/ant/) repository.\n    The control frequency of the ant is of `f = 20 Hz`. Each simulation timestep is of `dt=0.01` and the ant robot repeats the same action for 5 simulation steps.\n\n    ### Maze Variations\n\n    #### Maze size\n\n    The map variations for the mazes are the same as for `PointMaze`. The ant environments with fixed goal and reset locations are the following:\n\n    * `AntMaze_UMaze-v4`\n    * `AntMaze_BigMaze-v4`\n    * `AntMaze_HardestMaze-v4`\n\n    #### Diverse goal mazes\n\n    The environments with fixed reset position for the ant and randomly selected goals, also known as diverse goal, are:\n\n    * `AntMaze_BigMaze_DG-v4`\n    * `AntMaze_HardestMaze_DG-v4`\n\n    #### Diverse goal and reset mazes\n\n    Finally, the environments that select the reset and goal locations randomly are:\n\n    * `AntMaze_BigMaze_DGR-v4`\n    * `AntMaze_HardestMaze_DGR-v4`\n\n    #### Custom maze\n\n    Also, any of the `AntMaze` environments can be initialized with a custom maze map by setting the `maze_map` argument like follows:\n\n    ```python\n    import gymnasium as gym\n\n    example_map = [[1, 1, 1, 1, 1],\n           [1, C, 0, C, 1],\n           [1, 1, 1, 1, 1]]\n\n    env = gym.make('AntMaze_UMaze-v4', maze_map=example_map)\n    ```\n\n    ### Action Space\n\n    The action space is a `Box(-1, 1, (8,), float32)`. An action represents the torques applied at the hinge joints.\n\n    | Num | Action                                                            | Control Min | Control Max | Name (in corresponding XML file) | Joint | Unit         |\n    | --- | ----------------------------------------------------------------- | ----------- | ----------- | -------------------------------- | ----- | ------------ |\n    | 0   | Torque applied on the rotor between the torso and front left hip  | -1          | 1           | hip_1 (front_left_leg)           | hinge | torque (N m) |\n    | 1   | Torque applied on the rotor between the front left two links      | -1          | 1           | angle_1 (front_left_leg)         | hinge | torque (N m) |\n    | 2   | Torque applied on the rotor between the torso and front right hip | -1          | 1           | hip_2 (front_right_leg)          | hinge | torque (N m) |\n    | 3   | Torque applied on the rotor between the front right two links     | -1          | 1           | angle_2 (front_right_leg)        | hinge | torque (N m) |\n    | 4   | Torque applied on the rotor between the torso and back left hip   | -1          | 1           | hip_3 (back_leg)                 | hinge | torque (N m) |\n    | 5   | Torque applied on the rotor between the back left two links       | -1          | 1           | angle_3 (back_leg)               | hinge | torque (N m) |\n    | 6   | Torque applied on the rotor between the torso and back right hip  | -1          | 1           | hip_4 (right_back_leg)           | hinge | torque (N m) |\n    | 7   | Torque applied on the rotor between the back right two links      | -1          | 1           | angle_4 (right_back_leg)         | hinge | torque (N m) |\n\n    ### Observation Space\n\n    The observation is a `goal-aware observation space`. It consists of a dictionary with information about the robot's position and goal. The dictionary consists of the following 3 keys:\n\n    * `observation`: Observations consist of positional values of different body parts of the ant, followed by the velocities of those individual parts (their derivatives) with all\n        the positions ordered before all the velocities.\n\n        By default, observations do not include the x- and y-coordinates of the ant's torso. These values are included in the `achieved_goal` key of the observation.\n        However, by default, an observation is a `ndarray` with shape `(111,)` if the external contact forces are included with the `use_contact_forces` arguments. Otherwise, the shape will be `(27, )`\n        The elements of the array correspond to the following:\n\n        | Num | Observation                                                  | Min    | Max    | Name (in corresponding XML file)       | Joint | Unit                     |\n        |-----|--------------------------------------------------------------|--------|--------|----------------------------------------|-------|--------------------------|\n        | 0   | z-coordinate of the torso (centre)                           | -Inf   | Inf    | torso                                  | free  | position (m)             |\n        | 1   | x-orientation of the torso (centre)                          | -Inf   | Inf    | torso                                  | free  | angle (rad)              |\n        | 2   | y-orientation of the torso (centre)                          | -Inf   | Inf    | torso                                  | free  | angle (rad)              |\n        | 3   | z-orientation of the torso (centre)                          | -Inf   | Inf    | torso                                  | free  | angle (rad)              |\n        | 4   | w-orientation of the torso (centre)                          | -Inf   | Inf    | torso                                  | free  | angle (rad)              |\n        | 5   | angle between torso and first link on front left             | -Inf   | Inf    | hip_1 (front_left_leg)                 | hinge | angle (rad)              |\n        | 6   | angle between the two links on the front left                | -Inf   | Inf    | ankle_1 (front_left_leg)               | hinge | angle (rad)              |\n        | 7   | angle between torso and first link on front right            | -Inf   | Inf    | hip_2 (front_right_leg)                | hinge | angle (rad)              |\n        | 8   | angle between the two links on the front right               | -Inf   | Inf    | ankle_2 (front_right_leg)              | hinge | angle (rad)              |\n        | 9   | angle between torso and first link on back left              | -Inf   | Inf    | hip_3 (back_leg)                       | hinge | angle (rad)              |\n        | 10  | angle between the two links on the back left                 | -Inf   | Inf    | ankle_3 (back_leg)                     | hinge | angle (rad)              |\n        | 11  | angle between torso and first link on back right             | -Inf   | Inf    | hip_4 (right_back_leg)                 | hinge | angle (rad)              |\n        | 12  | angle between the two links on the back right                | -Inf   | Inf    | ankle_4 (right_back_leg)               | hinge | angle (rad)              |\n        | 13  | x-coordinate velocity of the torso                           | -Inf   | Inf    | torso                                  | free  | velocity (m/s)           |\n        | 14  | y-coordinate velocity of the torso                           | -Inf   | Inf    | torso                                  | free  | velocity (m/s)           |\n        | 15  | z-coordinate velocity of the torso                           | -Inf   | Inf    | torso                                  | free  | velocity (m/s)           |\n        | 16  | x-coordinate angular velocity of the torso                   | -Inf   | Inf    | torso                                  | free  | angular velocity (rad/s) |\n        | 17  | y-coordinate angular velocity of the torso                   | -Inf   | Inf    | torso                                  | free  | angular velocity (rad/s) |\n        | 18  | z-coordinate angular velocity of the torso                   | -Inf   | Inf    | torso                                  | free  | angular velocity (rad/s) |\n        | 19  | angular velocity of angle between torso and front left link  | -Inf   | Inf    | hip_1 (front_left_leg)                 | hinge | angle (rad)              |\n        | 20  | angular velocity of the angle between front left links       | -Inf   | Inf    | ankle_1 (front_left_leg)               | hinge | angle (rad)              |\n        | 21  | angular velocity of angle between torso and front right link | -Inf   | Inf    | hip_2 (front_right_leg)                | hinge | angle (rad)              |\n        | 22  | angular velocity of the angle between front right links      | -Inf   | Inf    | ankle_2 (front_right_leg)              | hinge | angle (rad)              |\n        | 23  | angular velocity of angle between torso and back left link   | -Inf   | Inf    | hip_3 (back_leg)                       | hinge | angle (rad)              |\n        | 24  | angular velocity of the angle between back left links        | -Inf   | Inf    | ankle_3 (back_leg)                     | hinge | angle (rad)              |\n        | 25  | angular velocity of angle between torso and back right link  | -Inf   | Inf    | hip_4 (right_back_leg)                 | hinge | angle (rad)              |\n        | 26  |angular velocity of the angle between back right links        | -Inf   | Inf    | ankle_4 (right_back_leg)               | hinge | angle (rad)              |\n\n        The remaining 14*6 = 84 elements of the observation are contact forces (external forces - force x, y, z and torque x, y, z) applied to the center of mass of each of the links. The 14 links are: the ground link,\n        the torso link, and 3 links for each leg (1 + 1 + 12) with the 6 external forces. These elements are included only if at the environments initialization the argument `use_contact_forces` is set to `True`.\n\n    * `desired_goal`: this key represents the final goal to be achieved. In this environment it is a 2-dimensional `ndarray`, `(2,)`, that consists of the two cartesian coordinates of the desired final ant torso position `[x,y]`. The elements of the array are the following:\n\n        | Num | Observation             | Min    | Max    | Site Name (in corresponding XML file) |Unit          |\n        |-----|------------------------ |--------|--------|---------------------------------------|--------------|\n        | 0   | Final goal x coordinate | -Inf   | Inf    | target                                | position (m) |\n        | 1   | Final goal y coordinate | -Inf   | Inf    | target                                | position (m) |\n\n    * `achieved_goal`: this key represents the current state of the ant's torso, as if it would have achieved a goal. This is useful for goal orientated learning algorithms such as those that use [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (HER).\n        The value is an `ndarray` with shape `(2,)`. The elements of the array are the following:\n\n        | Num | Observation                                    | Min    | Max    | Site Name (in corresponding XML file) |Unit          |\n        |-----|------------------------------------------------|--------|--------|---------------------------------------|--------------|\n        | 0   | Current goal ant position in the x coordinate  | -Inf   | Inf    | torso                                 | position (m) |\n        | 1   | Current goal ant position in the y coordinate  | -Inf   | Inf    | torso                                 | position (m) |\n\n    ### Rewards\n\n    The reward can be initialized as `sparse` or `dense`:\n    - *sparse*: the returned reward can have two values: `0` if the ant hasn't reached its final target position, and `1` if the ant is in the final target position (the ant is considered to have reached the goal if the Euclidean distance between both is lower than 0.5 m).\n    - *dense*: the returned reward is the negative Euclidean distance between the achieved goal position and the desired goal.\n\n    To initialize this environment with one of the mentioned reward functions the type of reward must be specified in the id string when the environment is initialized. For `sparse` reward the id is the default of the environment, `AntMaze_UMaze-v4`. However, for `dense`\n    reward the id must be modified to `AntMaze_UMazeDense-v4` and initialized as follows:\n\n    ```python\n    import gymnasium as gym\n\n    env = gym.make('AntMaze_UMaze-v4')\n    ```\n\n    ### Starting State\n\n    The goal and initial placement of the ant in the maze follows the same structure for all environments. A discrete cell `(i,j)` is selected for the goal and agent's initial position as previously menitoned in the **Maze** section.\n    Then this cell index is converted to its cell center as an `(x,y)` continuous Cartesian coordinates in the MuJoCo simulation. Finally, a sampled noise from a uniform distribution with range `[-0.25,0.25]m` is added to the\n    cell's center x and y coordinates. This allows to create a richer goal distribution.\n\n    The goal and initial position of the agent can also be specified by the user when the episode is reset. This is done by passing the dictionary argument `options` to the gymnasium reset() function. This dictionary expects one or both of\n    the following keys:\n\n    * `goal_cell`: `numpy.ndarray, shape=(2,0), type=int` - Specifies the desired `(i,j)` cell location of the goal. A uniform sampled noise will be added to the continuous coordinates of the center of the cell.\n    * `reset_cell`: `numpy.ndarray, shape=(2,0), type=int` - Specifies the desired `(i,j)` cell location of the reset initial agent position. A uniform sampled noise will be added to the continuous coordinates of the center of the cell.\n\n    ### Episode End\n\n    * `truncated` - The episode will be `truncated` when the duration reaches a total of `max_episode_steps`.\n    * `terminated` - The task can be set to be continuing with the `continuing_task` argument. In this case the episode will never terminate, instead the goal location is randomly selected again. If the task is set not to be continuing the\n    episode will be terminated when the Euclidean distance to the goal is less or equal to 0.5.\n\n    ### Arguments\n\n    * `maze_map` - Optional argument to initialize the environment with a custom maze map.\n    * `continuing_task` - If set to `True` the episode won't be terminated when reaching the goal, instead a new goal location will be generated. If `False` the environment is terminated when the ant reaches the final goal.\n    * `use_contact_forces` - If `True` contact forces of the ant are included in the `observation`.\n\n    Note that, the maximum number of timesteps before the episode is `truncated` can be increased or decreased by specifying the `max_episode_steps` argument at initialization. For example,\n    to increase the total number of timesteps to 100 make the environment as follows:\n\n    ```python\n    import gymnasium as gym\n\n    env = gym.make('AntMaze_UMaze-v4', max_episode_steps=100)\n    ```\n\n    ### Version History\n    * v4: Update to maze_v4. Refactor compute_terminated in MazeEnv into a pure function compute_terminated and a new function update_goal which resets the goal position. Ant bug fix: Reward is now computed before reset (i.e. sparse reward is not always zero). Maze bug fix: Ant can no longer reset within the goal radius 0.45 due to maze_size_scaling factor missing in MazeEnv. info['success'] key added.\n    * v3: refactor version of the D4RL environment, also create dependency on newest [mujoco python bindings](https://mujoco.readthedocs.io/en/latest/python.html) maintained by the MuJoCo team in Deepmind.\n    * v2 & v1: legacy versions in the [D4RL](https://github.com/Farama-Foundation/D4RL).\n    \"\"\""
    ],
    "functions": [
        "reset",
        "step",
        "_get_obs",
        "update_target_site_pos",
        "render",
        "close"
    ],
    "classes": [
        "AntMazeEnv"
    ]
}